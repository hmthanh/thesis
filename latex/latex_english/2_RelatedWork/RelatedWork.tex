%\chapter{RELATED WORK}

%\chapter{Các công trình liên quan}
\chapter{RELATED WORK}
\label{chap:RelatedWork}

In this section, we present the basic definitions of knowledge graphs in order to understand the task of link prediction in knowledge graphs, as well as other related research directions.

\section{Definition of Knowledge Graphs}

The basic definitions of knowledge graphs are compiled and categorized by Cai, Hongyun \cite{cai2018comprehensive} and Goyal, Palash \cite{goyal2018graph} as follows:

\begin{figure}[htp]
	\centering
	\includegraphics[width=7 cm]{images/graph_emb_1.png}
	\caption{Example of an input graph}
	\label{fig:graphInput}
\end{figure}

\begin{itemize}
	\item \begin{definition}[Graph]\label{def:defGraph}
		\(\mathcal{G} = (V, E)\), where \(v \in V\) is a vertex and \(e \in E\) is an edge. \(\mathcal{G}\) is associated with a vertex-type mapping function \(f_v: V \to T^v\) and an edge-type mapping function \(f_e: E \to T^e\).
	\end{definition}
	
	Here, \(T^v\) and \(T^e\) are the sets of vertex types and edge types, respectively. Each vertex \(v_i \in V\) belongs to a specific type, i.e., \(f_v(v_i) \in T^v\). Similarly, for \(e_{ij} \in E, f_e (e_{ij}) \in T^e\).
	
	\item
	\begin{definition}[Homogeneous Graph]\label{def:homogeneous}
		Homogeneous graph: \textit{ $\mathcal{G}_{homo} = (V, E)$ is a graph where $\mid T^v \mid = \mid T^e \mid = 1$. All vertices in $\mathcal{G}$ belong to a single type, and all edges belong to a single type}.
	\end{definition}
	
	\item
	\begin{definition}[Heterogeneous Graph]\label{def:heterogeneous}
		Heterogeneous graph: \textit{$\mathcal{G}_{hete} = (V, E)$ is a graph where $\mid T^v \mid > 1$ or $\mid T^e \mid > 1$. That is, there is more than one type of vertex or more than one type of edge}.
	\end{definition}
	
	\item
	\begin{definition}[Knowledge Graph]\label{def:knowledgeGraph}
		Knowledge graph:
		$\mathcal{G}_{know} = (V, R, E)$ is a directed graph, where the vertex set represents entities, the relation set represents relationships between entities, and the edge set $E \subseteq V\times R \times V$ represents events in the form of subject-property-object triples. Each edge is a triple $(\text{entity}_{\text{head}}, \text{relation}, \text{entity}_{\text{tail}})$ (denoted as $\langle h, r, t \rangle$), expressing a relation $r$ from head entity $h$ to tail entity $t$.
	\end{definition}
	
	Here, $h, t \in V$ are entities and $r \in R$ is a relation. We refer to $\langle h, r, t \rangle$ as a knowledge graph triple.
	
	Example: in \autoref{fig:graphExample}, there are two triples: 
	$\langle \text{Tom Cruise, born\_in, New York} \rangle$
	and $\langle \text{New York, state\_of, U.S} \rangle$. Note that entities and relations in a knowledge graph often belong to different types. Therefore, a knowledge graph can be viewed as a specific case of a heterogeneous graph.
\end{itemize}

\section{Link Prediction in Knowledge Graphs}


Link prediction, also known as knowledge graph completion, is the task of exploiting known facts (events) in a knowledge graph to infer missing ones. This is equivalent to predicting the correct tail entity in a triple $\langle h, r, ? \rangle$ (tail prediction) or the correct head entity in $\langle ?, r, t \rangle$ (head prediction). For simplicity, instead of distinguishing between head and tail prediction, we generally refer to the known entity as the \textit{source entity} and the entity to be predicted as the \textit{target entity}.

Most current research on link prediction in knowledge graphs is related to approaches that focus on embedding a given graph into a low-dimensional vector space. In contrast to these approaches is a rule-based method explored in \cite{burl}. Its core algorithm samples arbitrary rules and generalizes them into Horn clauses \cite{wiki:Horn}, then uses statistics to compute the confidence of these generalized rules. When predicting a new link (edge) in the graph, the task is to infer whether an edge with a specific label exists between two given nodes. Many methods have been proposed to learn rules from graphs, such as in RuDiK \cite{ortona2018robust}, AMIE \cite{galarraga2015fast}, and RuleN \cite{meilicke2018fine}.

As mentioned earlier, there are two main approaches to this problem: one is optimizing an objective function to find a small set of rules that cover the majority of correct examples with minimal error, as explored in RuDiK \cite{ortona2018robust}. The other approach, which we adopt in this thesis, aims to explore all possible rules and then generate a top-\(k\) ranking of candidate triples, each associated with a confidence score measured on the training set.

Our rule-based method is largely based on the \textit{Anytime Bottom-Up Rule Learning for Knowledge Graph Completion} method \cite{meilicke2019anytime}, hereafter referred to as \textbf{AnyBURL}. As its name suggests, this method primarily focuses on completing the graph by filling in missing parts. A key limitation of this model is that when a new edge or fact is added to the graph, the entire model must be retrained. We address this issue using two strategies: the \textit{offline-to-online} strategy, which retrains a portion of the graph only after a batch of new edges is added; and the \textit{online-to-online} strategy, which immediately retrains the affected portion of the graph whenever a new edge is added.

% GAT
In the deep learning branch of approaches, many successful techniques from image processing and natural language processing have been applied to knowledge graphs, such as Convolutional Neural Networks (CNNs \cite{lecun1999object}), Recurrent Neural Networks (RNNs \cite{hopfield2007hopfield}), and more recently, Transformers \cite{yang2019xlnet} and Capsule Neural Networks (CapsNets \cite{sabour2017dynamic}). In addition, other techniques such as random walks and hierarchical structure-based models have also been explored. The common advantage of these deep learning methods on knowledge graphs is their ability to automatically extract features and generalize complex graph structures based on large amounts of training data. However, some methods focus mainly on grid-like structures and fail to preserve the spatial characteristics of knowledge graphs.

The attention mechanism, particularly the multi-head attention layer, has been applied to graphs through the Graph Attention Network (GAT \cite{velivckovic2017graph}) model, which aggregates information about an entity based on attention weights from its neighboring entities. However, GAT lacks integration of relation embeddings and the embeddings of an entity's neighbors—components that are crucial for capturing the role of each entity. This limitation has been addressed in the work \textit{Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs} (\textbf{KBAT} \cite{nathani2019learning}), which we adopt as the foundation for our study.


The attention mechanism is currently one of the most effective (state-of-the-art) deep learning structures, as it has been proven to substitute any convolution operation \cite{cordonnier2019relationship}. Moreover, it serves as a core component in leading models for natural language processing, such as Megatron-LM \cite{shoeybi2019megatron}, and image segmentation, such as HRNet-OCR (Hierarchical Multi-Scale Attention \cite{tao2020hierarchical}). Some recent works \cite{cordonnier2020multi} have proposed interesting improvements based on the attention mechanism. However, these advancements have not yet been applied to knowledge graphs, which motivates us to adopt this family of methods to integrate the latest innovations into knowledge graph modeling.

\section{Research Areas in Knowledge Graphs}

\begin{figure}[htp]
	\centering
	\tikzset{
		category/.style  = {draw, font=\sffamily, thin, align=center},
		subcat/.style={rectangle, rounded corners=6pt},
		center/.style = {category, ellipse, fill=blue!60, text width=4em},
		group/.style = {category, subcat, fill=blue!30, rounded corners=6pt, text width=6em},
		yellowbox/.style = {category, subcat, fill=yellow!30},
		greenbox/.style = {category, subcat, fill=green!30},
		redbox/.style = {category, subcat, fill=red!30},
		bluebox/.style = {category, subcat, fill=cyan!30},
		leafbox/.style = {category, subcat, fill=black!10, rounded corners=0mm}
	}
	\resizebox{\textwidth}{!}{%
		\begin{tikzpicture}
			\node[center] (root) {Knowledge Graph};
			\node[group][above left=1cm of root] (c1) {Knowledge Representation Learning};
			\node[group][above right=8mm of root] (c2) {Knowledge Inference};
			\node[group][below left=5mm of root] (c3) {Knowledge Acquisition};
			\node[group][below right=20mm of root, xshift=-2cm] (c4) {Temporal Knowledge Graph};
			
			\begin{scope}[every node/.style={yellowbox}]
				\node[above=8mm of c1, xshift=-1cm] (c11) {Embedding Space};
				\node[left=of c1, yshift=9mm, xshift=-10mm] (c12) {Scoring Function};
				\node[left=15mm of c1, yshift=-5mm] (c13) {Model Encoding};
				\node[left=of c1, yshift=-20mm] (c14) {Auxiliary Information};
			\end{scope}
			
			\begin{scope}[every node/.style={redbox}]
				\node[above left=of c2, text width=35mm, yshift=1mm, xshift=18mm] (c21) {Natural Language Understanding};
				\node[above=1cm of c2, yshift=5mm, xshift=13mm] (c22) {Question Answering};
				\node[right=of c2, yshift=15mm] (c23) {Dialogue System};
				\node[right=of c2, yshift=-2mm] (c24) {Recommendation System};
				\node[below=5mm of c2, xshift=5mm] (c25) {Other Applications};
			\end{scope}
			
			\begin{scope}[every node/.style={greenbox}]
				\node[left=1cm of c3, yshift=5mm] (c31) {Entity Discovery};
				\node[below left=of c3, yshift=1cm, xshift=3mm] (c32) {Relation Extraction};
				\node[below right=5mm of c3, xshift=-4cm] (c33) {Graph Completion};
			\end{scope}
			
			\begin{scope}[every node/.style={bluebox}]
				\node[below=of c4, xshift=-15mm, yshift=-35mm] (c41) {Temporal Logic Reasoning};
				\node[below=of c4, xshift=2cm, yshift=-23mm] (c42) {Time-Independent Relations};
				\node[below right=of c4,xshift=-2mm, yshift=-10mm] (c43) {Dynamic Entities};
				\node[right=of c4, yshift=-2cm, xshift=15mm] (c44) {Temporal Embeddings};
			\end{scope}
			
			\begin{scope}[every node/.style={leafbox}]
				\node[above=5mm of c11] (c11x) {
					\begin{tabular}{@{}l@{}@{}l@{}}
						- Point-wise & - Manifold \\
						- Complex & - Gaussian \\
						- Discrete & \\
					\end{tabular}
				};
				\node[above=5mm of c12, xshift=-5mm] (c12x) {
					\begin{tabular}{@{}l@{}}
						- Distance-based \\
						- Semantic \\
						- Others \\
					\end{tabular}
				};
				\node[left=of c13, yshift=5mm] (c13x) {
					\begin{tabular}{@{}l@{}}
						- Linear / Bilinear \\
						- Matrix Factorization \\
						- Neural Nets \\
						- CNN \\
						- RNN \\
						- Transformers \\
						- GCN \\
					\end{tabular}
				};
				\node[below left=5mm of c14, xshift=6mm, yshift=-7mm] (c14x) {
					\begin{tabular}{@{}l@{}c@{}l@{}}
						- Textual & - Type & - Visual \\
					\end{tabular}
				};
				%%%%%%%%%%%% 
				\node[below left=of c31] (c31x) {
					\begin{tabular}{@{}l@{}}
						- Recognition \\
						- Typing \\
						- Disambiguation \\
						- Ranking \\
					\end{tabular}
				};
				\node[below=of c32, xshift=-5mm] (c32x) {
					\begin{tabular}{@{}l@{}}
						- Neural Nets \\
						- Attention \\
						- GCN \\
						- GAN \\
						- RL \\
						- Others \\
					\end{tabular}
				};
				\node[below=5mm of c33] (c33x) {
					\begin{tabular}{@{}l@{}}
						- Ranking-based Embeddings \\
						- Textual Reasoning \\
						- Rule-based Reasoning \\
						- Hyper-relational Learning \\
						- Triple Classification \\
					\end{tabular}
				};
				%%%%%%%%%%%%%%%%%
				\node[above=5mm of c22] (c22x) {
					\begin{tabular}{@{}l@{}}
						- Single-fact QA \\
						- Multi-step Reasoning \\
					\end{tabular}
				};
				\node[below right=5mm of c25, yshift=15mm] (c25x) {
					\begin{tabular}{@{}l@{}}
						- Question Generation \\
						- Search Engines \\
						- Medical Applications \\
						- Health Recovery \\
						- Zero-shot Image Classification \\
						- Text Generation \\
						- Semantic Analysis \\
					\end{tabular}
				};
			\end{scope}
			
			\foreach \value in {1,...,4}
			\draw[->, line width=0.8mm] (root) -> (c\value);
			
			\foreach \value in {1,...,4}
			\draw[->, ultra thick] (c1) -> (c1\value);
			\foreach \value in {1,...,4}
			\draw[->, thick] (c1\value) -> (c1\value x);
			
			\foreach \value in {1,...,5}
			\draw[->, ultra thick] (c2) -> (c2\value);
			\foreach \value in {2,5}
			\draw[->, thick] (c2\value) -> (c2\value x);
			
			\foreach \value in {1,...,3}
			\draw[->, ultra thick] (c3) -> (c3\value);
			\foreach \value in {1,...,3}
			\draw[->, ultra thick] (c3\value) -> (c3\value x);
			
			\foreach \value in {1,...,4}
			\draw[->, ultra thick] (c4) -> (c4\value);
			
	\end{tikzpicture}}
	\caption{
		A taxonomy of research areas in knowledge graphs}
	\label{fig:categoriesResearch}
\end{figure}


Knowledge representation has a long-standing history in logic and artificial intelligence. In the context of knowledge graphs, four major research areas have been categorized and summarized in the survey \cite{ji2020survey}, including: Knowledge Representation Learning, Knowledge Acquisition, Temporal Knowledge Graphs, and Knowledge-aware Applications. All research categories are illustrated in \autoref{fig:categoriesResearch}.

\textbf{Knowledge Representation Learning}

Knowledge representation learning is an essential research topic in knowledge graphs that enables a wide range of real-world applications. It is categorized into four subgroups:

\begin{itemize}
	\item \textit{Representation Space} focuses on how entities and relations are represented in vector space. This includes point-wise, manifold, complex vector space, Gaussian distribution, and discrete space embeddings.
	
	\item \textit{Scoring Function} studies how to measure the validity of a triple in practice. These scoring functions may be distance-based or similarity-based.
	
	\item \textit{Encoding Models} investigate how to represent and learn interactions among relations. This is currently the main research direction, including linear or non-linear models, matrix factorization, or neural network-based approaches.
	
	\item \textit{Auxiliary Information} explores how to incorporate additional information into embedding models, such as textual, visual, and type information.
\end{itemize}

\textbf{Knowledge Acquisition}

Knowledge acquisition focuses on how to extract or obtain knowledge based on knowledge graphs, including knowledge graph completion, relation extraction, and entity discovery. Relation extraction and entity discovery aim to extract new knowledge (relations or entities) into the graph from text. Knowledge graph completion refers to expanding an existing graph by inferring missing links. Research directions include embedding-based ranking, relation path reasoning, rule-based reasoning, and hyper-relational learning.

Entity discovery tasks include entity recognition, disambiguation, typing, and ranking. Relation extraction models often employ attention mechanisms, graph convolutional networks (GCNs), adversarial training, reinforcement learning (RL), deep learning, and transfer learning, which is the foundation of the method proposed in our work.

In addition, other major research directions in knowledge graphs include \textbf{temporal knowledge graphs} and \textbf{knowledge-aware applications}. Temporal knowledge graphs incorporate temporal information into the graph to learn temporal representations. Knowledge-aware applications include natural language understanding, question answering, recommendation systems, and many other real-world tasks where integrating knowledge improves representation learning.
