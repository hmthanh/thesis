\chapter{Kết luận}
\label{Chapter5}

Trong phần này chúng tôi sẽ trình bày các kết quả đạt được của mô hình chúng tôi, cũng như những phân tích của chúng tôi
trên các kết quả của các tập dữ liệu khác nhau để giải thích những điểm tốt và điểm cần cải thiện trên mô hình của chúng tôi trên tập dữ liệu đó. Từ đó chúng tôi xác định những hướng nghiên cứu để cải tiến trong tương lai.
% chúng tôi cố gắng tìm hiểu các đặc trưng của các bộ dữ liệu tương ứng để cố gắng lý  giải thích  tại sao mô hình của chúng tôi hoặc các công trình khác có được kết quả tốt trên tập dữ liệu tương ứng đó.
% Những kết quả của hai đề xuất của chúng tôi cũng như các dịnh hướng nghiên cứu của chúng tôi trong tương lai.

Mặc dù kết quả chúng tôi cho thấy phương pháp dựa trên luật của chúng tôi có hiệu suất tương đương với các mô hình học sâu hiện đại (state-of-art) và có ưu thế vượt trội trong thời gian đào tạo khoảng 17 phút so với thời gian hàng giờ của phương pháp học sâu khác nhưng không phải là các mô hình học sâu này không đáng nghiên cứu. Chúng tôi cũng nhận thấy, với tập dữ liệu có nhiều loại quan hệ khác nhau như FreeBase, mô hình GCAT nhờ sử dụng cơ chế chú ý đạt được kết quả tốt hơn so với tập WorldNet với số lượng các loại quan hệ ít hơn.
Điều này chứng tỏ cơ chế chú ý bổ sung thêm thông tin vector nhúng quan hệ giúp học được các cấu trúc của đồ thị tốt hơn trên các tập dữ liệu nhiều loại quan hệ.
Đối với tập dữ liệu có nhiều mẫu tương tự và nghịch đảo như FB15k và WN18RR, mô hình dựa trên luật AnyBURL đạt được kết quả vượt trội, trong khi với phương pháp học sâu mô hình chỉ đạt kết quả trung bình so với các phương pháp khác.
Mô hình dựa trên luật AnyBURL có ưu thế tốt hơn trên các tập dữ liệu FB15k và WN18RR, tuy nhiên với các tập dữ liệu đã loại bỏ các thông tin tương tự hoặc nghịch đảo như FB15-237 và WN18RR thì phương pháp dựa trên học sâu đạt kết quả tốt hơn nhờ việc khái quát hóa được các cấu trúc phức tạp của đồ thị bằng việc cập nhật thông tin của một lượng lớn dữ liệu.
Một trong những ưu điểm của phương pháp dựa trên luật đó là các luật được sinh ra có thể lý giải được trong quá trình học, và có thời gian học vượt trội so với các phương pháp khác. Tuy nhiên, sau quá trình học phương pháp dựa trên luật phải duyệt qua tất cả các luật đã học mới có thể đưa ra được dự đoán. Đây là một điểm mà các phương pháp học sâu thể hiện tốt hơn, vì thông qua các trọng số đã học, mô hình học sâu GCAT thông qua các lớp tính toán có thể biến đổi đầu vào thành các kết quả xác xuất dự đoán nhanh hơn. Nhược điểm với phương pháp học sâu là quá trình học không thể lý giải được, hơn nữa chí phí huấn luyện rất tốn kém. Đối với hai thuật toán mở rộng của chúng tôi trong việc thêm tri thức mới vào đồ thị chúng tôi nhận thấy rằng là vượt trội hoàn toàn so với các phương pháp học sâu.
% Ngược lại đối với các phương pháp dựa trên học sâu lại có ưu thế rất lớn trong các tập dữ liệu này do có thể dễ dàng tính toán độ gần của các luật mới cần đánh giá so với các luật đã học từ đó có một kết quả khá tốt.
% Do đó chúng tôi cũng sẽ tiếp tục nghiên cứu các phương pháp học sâu và sẽ dùng phương pháp này làm đường cơ sở (base line) để so sánh với các nghiên cứu của chúng tôi trong tương lai.
% Một điểm yếu nữa của mô hình đựa trên luật của chúng tôi là mặc dù thời gian học là vượt trội nhưng thời gian để tính toán đưa ra đự đoán khá lâu do phải duyệt qua tất cả các luật được sinh ra mới có thể đưa ra dự đoán.
% Không giống như các phương pháp nhúng đồ thị khác thao tác này có thể dễ dàng tính toán.


Trong thế giới thực thông tin nhúng có thể thay đổi hoàn toàn theo thời gian, ví dụ trong quan hệ $\langle e_{\text{head}}, \textbf{president_of}, U.S\rangle$, năm 2012, ta biết kết quả là Barract Obama. Đối với nhiệm vụ dự đoán thực thể đầu, thì kết quả của $e_{\text{head}}$ có thể thay đổi hoàn toàn. Vì vậy, bổ sung thêm yếu tố thời gian là một hướng nghiên cứu quan trọng để đảm bảo tính đúng đắn của đồ thị tri thức. Ngoài ra, đối với phương pháp học sâu GCAT, có rất nhiều cải tiến trên cơ chế chú ý mà chúng tôi muốn áp dụng vào đồ thị tri thức. Đối với phương pháp học trên luật AnyBURL, gần đây nhánh học tăng cường (reinforcement learning) khá phát triển và nhóm tác giả Meilicke, Christian and Chekol \cite{meilicke2020reinforced} gần đây cũng đã có 1 nghiên cứu để tối ưu hóa lại phương pháp AnyBURL này.
Chúng tôi cũng có ý định nghiên cứu về hướng này và cố gắng báo cáo lại trong một tương lai gần.