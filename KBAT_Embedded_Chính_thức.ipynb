{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVe3gXdlAKiX"
   },
   "source": [
    "# Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRm8BTekEnGi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import deepcopy\n",
    "from torchsummary import summary\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8t0zLh2cEjuy"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "     # network arguments\n",
    "    data = \"./data/WN18RR/\"\n",
    "    epochs_gat = 3600\n",
    "    epochs_conv = 200\n",
    "    weight_decay_gat = float(5e-6)\n",
    "    weight_decay_conv = float(1e-5)\n",
    "    pretrained_emb = True\n",
    "    embedding_size = 50\n",
    "    lr = float(1e-3)\n",
    "    get_2hop = True\n",
    "    use_2hop = True\n",
    "    partial_2hop = False\n",
    "    output_folder = \"./checkpoints/wn/out/\"\n",
    "\n",
    "    # arguments for GAT\n",
    "    batch_size_gat = 86835\n",
    "    # Tỷ lệ của tập valid so với tập invalid trong khi training GAT\n",
    "    valid_invalid_ratio_gat = 2\n",
    "    drop_GAT = 0.3  # Tỷ lệ dropout của lớp SpGAT\n",
    "    alpha = 0.2  # LeakyRelu alphs for SpGAT layer\n",
    "    entity_out_dim = [100, 200]  # Miền nhúng của đầu ra output\n",
    "    nheads_GAT = [2, 2]  # Multihead attention SpGAT\n",
    "    # Margin used in hinge loss ( Sử dụng margin trong hinge (khớp nối))\n",
    "    margin = 5\n",
    "\n",
    "    # arguments for convolution network\n",
    "    batch_size_conv = 128  # Batch size for conv\n",
    "    alpha_conv = 0.2  # LeakyRelu alphas for conv layer\n",
    "    # Ratio of valid to invalid triples for convolution training\n",
    "    valid_invalid_ratio_conv = 40\n",
    "    out_channels = 500  # Số lượng output channels trong lớp conv\n",
    "    drop_conv = 0.0  # Xắc xuất dropout cho lớp convolution\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "KaJDECMUbnL_",
    "outputId": "46e6ca93-2020-404e-8362-98a9b56d1f4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "4J9EZBfOblZC",
    "outputId": "cf16cea9-c7a6-4be5-9905-66b24475d7d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "zVnr2GHjAGGK",
    "outputId": "44e80839-c467-4e70-ac93-abbd8e8c3cb2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GCATs'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -r /content/*\n",
    "git clone https://github.com/hmthanh/GCATs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uY4U03W3AT_0"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv /content/GCATs/* /content/\n",
    "rm -r GCATs/\n",
    "rm /content/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U--J2OdaA5DG"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "sh \"/content/prepare.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlUuYNjCLSy4"
   },
   "source": [
    "# File : create_dataset_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "8oaT3a91LJjT",
    "outputId": "4f4f0189-1f10-4c0e-c43b-ebfa1fd0b884"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_marked set set  86835\n",
      "Size of VALID_marked set set  3034\n",
      "Size of test_marked set set  3134\n"
     ]
    }
   ],
   "source": [
    "def getID(folder='data/WN18RR/'):\n",
    "    lstEnts = {}\n",
    "    lstRels = {}\n",
    "    with open(folder + 'train.txt') as f, open(folder + 'train_marked.txt', 'w') as f2:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            line = [i.strip() for i in line]\n",
    "            # print(line[0], line[1], line[2])\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            count += 1\n",
    "            f2.write(str(line[0]) + '\\t' + str(line[1]) +\n",
    "                     '\\t' + str(line[2]) + '\\n')\n",
    "        print(\"Size of train_marked set set \", count)\n",
    "\n",
    "    with open(folder + 'valid.txt') as f, open(folder + 'valid_marked.txt', 'w') as f2:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            line = [i.strip() for i in line]\n",
    "            # print(line[0], line[1], line[2])\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            count += 1\n",
    "            f2.write(str(line[0]) + '\\t' + str(line[1]) +\n",
    "                     '\\t' + str(line[2]) + '\\n')\n",
    "        print(\"Size of VALID_marked set set \", count)\n",
    "\n",
    "    with open(folder + 'test.txt') as f, open(folder + 'test_marked.txt', 'w') as f2:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            line = [i.strip() for i in line]\n",
    "            # print(line[0], line[1], line[2])\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            count += 1\n",
    "            f2.write(str(line[0]) + '\\t' + str(line[1]) +\n",
    "                     '\\t' + str(line[2]) + '\\n')\n",
    "        print(\"Size of test_marked set set \", count)\n",
    "\n",
    "    wri = open(folder + 'entity2id.txt', 'w')\n",
    "    for entity in lstEnts:\n",
    "        wri.write(entity + '\\t' + str(lstEnts[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()\n",
    "\n",
    "    wri = open(folder + 'relation2id.txt', 'w')\n",
    "    for entity in lstRels:\n",
    "        wri.write(entity + '\\t' + str(lstRels[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()\n",
    "\n",
    "\n",
    "getID()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBdtg0VELehu"
   },
   "source": [
    "# File : preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1U6EnE9lBtk3"
   },
   "outputs": [],
   "source": [
    "def read_entity_from_id(filename='./data/WN18RR/entity2id.txt'):\n",
    "    entity2id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                entity, entity_id = line.strip().split(\n",
    "                )[0].strip(), line.strip().split()[1].strip()\n",
    "                entity2id[entity] = int(entity_id)\n",
    "    return entity2id\n",
    "\n",
    "\n",
    "def read_relation_from_id(filename='./data/WN18RR/relation2id.txt'):\n",
    "    relation2id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                relation, relation_id = line.strip().split(\n",
    "                )[0].strip(), line.strip().split()[1].strip()\n",
    "                relation2id[relation] = int(relation_id)\n",
    "    return relation2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIEYGt6jLlFp"
   },
   "outputs": [],
   "source": [
    "def init_embeddings(entity_file, relation_file):\n",
    "    entity_emb, relation_emb = [], []\n",
    "\n",
    "    with open(entity_file) as f:\n",
    "        for line in f:\n",
    "            entity_emb.append([float(val) for val in line.strip().split()])\n",
    "\n",
    "    with open(relation_file) as f:\n",
    "        for line in f:\n",
    "            relation_emb.append([float(val) for val in line.strip().split()])\n",
    "\n",
    "    return np.array(entity_emb, dtype=np.float32), np.array(relation_emb, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A4vo5ecrLnZH"
   },
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    line = line.strip().split()\n",
    "    e1, relation, e2 = line[0].strip(), line[1].strip(), line[2].strip()\n",
    "    return e1, relation, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rqTiOKupC7ZZ"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, entity2id, relation2id, is_unweigted=False, directed=True):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # this is list for relation triples\n",
    "    triples_data = []\n",
    "\n",
    "    # for sparse tensor, rows list contains corresponding row of sparse tensor, cols list contains corresponding\n",
    "    # columnn of sparse tensor, data contains the type of relation\n",
    "    # Adjacecny matrix of entities is undirected, as the source and tail entities should know, the relation\n",
    "    # type they are connected with\n",
    "    rows, cols, data = [], [], []\n",
    "    unique_entities = set()\n",
    "    for line in lines:\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "        unique_entities.add(e1)\n",
    "        unique_entities.add(e2)\n",
    "        triples_data.append(\n",
    "            (entity2id[e1], relation2id[relation], entity2id[e2]))\n",
    "        if not directed:\n",
    "                # Connecting source and tail entity\n",
    "            rows.append(entity2id[e1])\n",
    "            cols.append(entity2id[e2])\n",
    "            if is_unweigted:\n",
    "                data.append(1)\n",
    "            else:\n",
    "                data.append(relation2id[relation])\n",
    "\n",
    "        # Connecting tail and source entity\n",
    "        rows.append(entity2id[e2])\n",
    "        cols.append(entity2id[e1])\n",
    "        if is_unweigted:\n",
    "            data.append(1)\n",
    "        else:\n",
    "            data.append(relation2id[relation])\n",
    "\n",
    "    print(\"number of unique_entities ->\", len(unique_entities))\n",
    "    return triples_data, (rows, cols, data), list(unique_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8X9CT0_DAeA"
   },
   "outputs": [],
   "source": [
    "def build_data(path='./data/WN18RR/', is_unweigted=False, directed=True):\n",
    "    entity2id = read_entity_from_id(path + 'entity2id.txt')\n",
    "    relation2id = read_relation_from_id(path + 'relation2id.txt')\n",
    "\n",
    "    # Adjacency matrix only required for training phase\n",
    "    # Currenlty creating as unweighted, undirected\n",
    "    train_triples, train_adjacency_mat, unique_entities_train = load_data(os.path.join(\n",
    "        path, 'train.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    validation_triples, valid_adjacency_mat, unique_entities_validation = load_data(\n",
    "        os.path.join(path, 'valid.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    test_triples, test_adjacency_mat, unique_entities_test = load_data(os.path.join(\n",
    "        path, 'test.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "\n",
    "    id2entity = {v: k for k, v in entity2id.items()}\n",
    "    id2relation = {v: k for k, v in relation2id.items()}\n",
    "    left_entity, right_entity = {}, {}\n",
    "\n",
    "    with open(os.path.join(path, 'train.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "\n",
    "        # Count number of occurences for each (e1, relation)\n",
    "        if relation2id[relation] not in left_entity:\n",
    "            left_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e1] not in left_entity[relation2id[relation]]:\n",
    "            left_entity[relation2id[relation]][entity2id[e1]] = 0\n",
    "        left_entity[relation2id[relation]][entity2id[e1]] += 1\n",
    "\n",
    "        # Count number of occurences for each (relation, e2)\n",
    "        if relation2id[relation] not in right_entity:\n",
    "            right_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e2] not in right_entity[relation2id[relation]]:\n",
    "            right_entity[relation2id[relation]][entity2id[e2]] = 0\n",
    "        right_entity[relation2id[relation]][entity2id[e2]] += 1\n",
    "\n",
    "    left_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        left_entity_avg[i] = sum(\n",
    "            left_entity[i].values()) * 1.0 / len(left_entity[i])\n",
    "\n",
    "    right_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        right_entity_avg[i] = sum(\n",
    "            right_entity[i].values()) * 1.0 / len(right_entity[i])\n",
    "\n",
    "    headTailSelector = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        headTailSelector[i] = 1000 * right_entity_avg[i] / \\\n",
    "            (right_entity_avg[i] + left_entity_avg[i])\n",
    "\n",
    "    return (train_triples, train_adjacency_mat), (validation_triples, valid_adjacency_mat), (test_triples, test_adjacency_mat), \\\n",
    "        entity2id, relation2id, headTailSelector, unique_entities_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjIwefuvL1kC"
   },
   "source": [
    "# File : utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBgCEZXBLz2A"
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def save_model(model, name, epoch, folder_name):\n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(),\n",
    "               (folder_name + \"trained_{}.pth\").format(epoch))\n",
    "    print(\"Done saving Model\")\n",
    "\n",
    "\n",
    "gat_loss_func = nn.MarginRankingLoss(margin=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "132rXtSPL5HS"
   },
   "outputs": [],
   "source": [
    "def GAT_Loss(train_indices, valid_invalid_ratio):\n",
    "    len_pos_triples = train_indices.shape[0] // (int(valid_invalid_ratio) + 1)\n",
    "\n",
    "    pos_triples = train_indices[:len_pos_triples]\n",
    "    neg_triples = train_indices[len_pos_triples:]\n",
    "\n",
    "    pos_triples = pos_triples.repeat(int(valid_invalid_ratio), 1)\n",
    "\n",
    "    source_embeds = entity_embed[pos_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[pos_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[pos_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    pos_norm = torch.norm(x, p=2, dim=1)\n",
    "\n",
    "    source_embeds = entity_embed[neg_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[neg_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[neg_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    neg_norm = torch.norm(x, p=2, dim=1)\n",
    "\n",
    "    y = torch.ones(int(args.valid_invalid_ratio)\n",
    "                   * len_pos_triples).cuda()\n",
    "    loss = gat_loss_func(pos_norm, neg_norm, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-C-j-YPL6-9"
   },
   "outputs": [],
   "source": [
    "def render_model_graph(model, Corpus_, train_indices, relation_adj, averaged_entity_vectors):\n",
    "    graph = make_dot(model(Corpus_.train_adj_matrix, train_indices, relation_adj, averaged_entity_vectors),\n",
    "                     params=dict(model.named_parameters()))\n",
    "    graph.render()\n",
    "\n",
    "\n",
    "def print_grads(model):\n",
    "    print(model.relation_embed.weight.grad)\n",
    "    print(model.relation_gat_1.attention_0.a.grad)\n",
    "    print(model.convKB.fc_layer.weight.grad)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.grad)\n",
    "\n",
    "\n",
    "def clip_gradients(model, gradient_clip_norm):\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_norm)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, \"norm before clipping is -> \", param.grad.norm())\n",
    "            torch.nn.utils.clip_grad_norm_(param, args.gradient_clip_norm)\n",
    "            print(name, \"norm beafterfore clipping is -> \", param.grad.norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OUCsa6njL-PK"
   },
   "outputs": [],
   "source": [
    "def plot_grad_flow(named_parameters, parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "\n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads = []\n",
    "    layers = []\n",
    "\n",
    "    for n, p in zip(named_parameters, parameters):\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"r\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"g\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"r\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"g\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "    plt.savefig('initial.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_grad_flow_low(named_parameters, parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in zip(named_parameters, parameters):\n",
    "        # print(n)\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig('initial.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOprxDhkMGbQ"
   },
   "source": [
    "# File : models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ohtx6cpoMFSc"
   },
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, num_nodes, nfeat, nhid, relation_dim, dropout, alpha, nheads):\n",
    "        \"\"\"\n",
    "            Sparse version of GAT\n",
    "            nfeat -> Entity Input Embedding dimensions\n",
    "            nhid  -> Entity Output Embedding dimensions\n",
    "            relation_dim -> Relation Embedding dimensions\n",
    "            num_nodes -> number of nodes in the Graph\n",
    "            nheads -> Used for Multihead attention\n",
    "\n",
    "        \"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.attentions = [SpGraphAttentionLayer(num_nodes, nfeat,\n",
    "                                                 nhid,\n",
    "                                                 relation_dim,\n",
    "                                                 dropout=dropout,\n",
    "                                                 alpha=alpha,\n",
    "                                                 concat=True)\n",
    "                           for _ in range(nheads)]\n",
    "\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        # W matrix to convert h_input to h_output dimension\n",
    "        self.W = nn.Parameter(torch.zeros(size=(relation_dim, nheads * nhid)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(num_nodes, nhid * nheads,\n",
    "                                             nheads * nhid, nheads * nhid,\n",
    "                                             dropout=dropout,\n",
    "                                             alpha=alpha,\n",
    "                                             concat=False\n",
    "                                             )\n",
    "\n",
    "    def forward(self, Corpus_, batch_inputs, entity_embeddings, relation_embed,\n",
    "                edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop):\n",
    "        x = entity_embeddings\n",
    "\n",
    "        edge_embed_nhop = relation_embed[\n",
    "            edge_type_nhop[:, 0]] + relation_embed[edge_type_nhop[:, 1]]\n",
    "\n",
    "        x = torch.cat([att(x, edge_list, edge_embed, edge_list_nhop, edge_embed_nhop)\n",
    "                       for att in self.attentions], dim=1)\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        out_relation_1 = relation_embed.mm(self.W)\n",
    "\n",
    "        edge_embed = out_relation_1[edge_type]\n",
    "        edge_embed_nhop = out_relation_1[\n",
    "            edge_type_nhop[:, 0]] + out_relation_1[edge_type_nhop[:, 1]]\n",
    "\n",
    "        x = F.elu(self.out_att(x, edge_list, edge_embed,\n",
    "                               edge_list_nhop, edge_embed_nhop))\n",
    "        return x, out_relation_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rwLviOV9MK7L"
   },
   "outputs": [],
   "source": [
    "class SpKBGATModified(nn.Module):\n",
    "    def __init__(self, initial_entity_emb, initial_relation_emb, entity_out_dim, relation_out_dim,\n",
    "                 drop_GAT, alpha, nheads_GAT):\n",
    "        '''Sparse version of KBGAT\n",
    "        entity_in_dim -> Entity Input Embedding dimensions\n",
    "        entity_out_dim  -> Entity Output Embedding dimensions, passed as a list\n",
    "        num_relation -> number of unique relations\n",
    "        relation_dim -> Relation Embedding dimensions\n",
    "        num_nodes -> number of nodes in the Graph\n",
    "        nheads_GAT -> Used for Multihead attention, passed as a list '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = initial_entity_emb.shape[0]\n",
    "        self.entity_in_dim = initial_entity_emb.shape[1]\n",
    "        self.entity_out_dim_1 = entity_out_dim[0]\n",
    "        self.nheads_GAT_1 = nheads_GAT[0]\n",
    "        self.entity_out_dim_2 = entity_out_dim[1]\n",
    "        self.nheads_GAT_2 = nheads_GAT[1]\n",
    "\n",
    "        # Properties of Relations\n",
    "        self.num_relation = initial_relation_emb.shape[0]\n",
    "        self.relation_dim = initial_relation_emb.shape[1]\n",
    "        self.relation_out_dim_1 = relation_out_dim[0]\n",
    "\n",
    "        self.drop_GAT = drop_GAT\n",
    "        self.alpha = alpha      # For leaky relu\n",
    "\n",
    "        self.final_entity_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_nodes, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.final_relation_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_relation, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.entity_embeddings = nn.Parameter(initial_entity_emb)\n",
    "        self.relation_embeddings = nn.Parameter(initial_relation_emb)\n",
    "\n",
    "        self.sparse_gat_1 = SpGAT(self.num_nodes, self.entity_in_dim, self.entity_out_dim_1, self.relation_dim,\n",
    "                                  self.drop_GAT, self.alpha, self.nheads_GAT_1)\n",
    "\n",
    "        self.W_entities = nn.Parameter(torch.zeros(\n",
    "            size=(self.entity_in_dim, self.entity_out_dim_1 * self.nheads_GAT_1)))\n",
    "        nn.init.xavier_uniform_(self.W_entities.data, gain=1.414)\n",
    "\n",
    "    def forward(self, Corpus_, adj, batch_inputs, train_indices_nhop):\n",
    "        # getting edge list\n",
    "        edge_list = adj[0]\n",
    "        edge_type = adj[1]\n",
    "\n",
    "        edge_list_nhop = torch.cat(\n",
    "            (train_indices_nhop[:, 3].unsqueeze(-1), train_indices_nhop[:, 0].unsqueeze(-1)), dim=1).t()\n",
    "        edge_type_nhop = torch.cat(\n",
    "            [train_indices_nhop[:, 1].unsqueeze(-1), train_indices_nhop[:, 2].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        if(CUDA):\n",
    "            edge_list = edge_list.cuda()\n",
    "            edge_type = edge_type.cuda()\n",
    "            edge_list_nhop = edge_list_nhop.cuda()\n",
    "            edge_type_nhop = edge_type_nhop.cuda()\n",
    "\n",
    "        edge_embed = self.relation_embeddings[edge_type]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        self.entity_embeddings.data = F.normalize(\n",
    "            self.entity_embeddings.data, p=2, dim=1).detach()\n",
    "\n",
    "        # self.relation_embeddings.data = F.normalize(\n",
    "        #     self.relation_embeddings.data, p=2, dim=1)\n",
    "\n",
    "        out_entity_1, out_relation_1 = self.sparse_gat_1(\n",
    "            Corpus_, batch_inputs, self.entity_embeddings, self.relation_embeddings,\n",
    "            edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop)\n",
    "\n",
    "        mask_indices = torch.unique(batch_inputs[:, 2]).cuda()\n",
    "        mask = torch.zeros(self.entity_embeddings.shape[0]).cuda()\n",
    "        mask[mask_indices] = 1.0\n",
    "\n",
    "        entities_upgraded = self.entity_embeddings.mm(self.W_entities)\n",
    "        out_entity_1 = entities_upgraded + \\\n",
    "            mask.unsqueeze(-1).expand_as(out_entity_1) * out_entity_1\n",
    "\n",
    "        out_entity_1 = F.normalize(out_entity_1, p=2, dim=1)\n",
    "\n",
    "        self.final_entity_embeddings.data = out_entity_1.data\n",
    "        self.final_relation_embeddings.data = out_relation_1.data\n",
    "\n",
    "        return out_entity_1, out_relation_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BqOWhLg8MNq8"
   },
   "outputs": [],
   "source": [
    "class SpKBGATConvOnly(nn.Module):\n",
    "    def __init__(self, initial_entity_emb, initial_relation_emb, entity_out_dim, relation_out_dim,\n",
    "                 drop_GAT, drop_conv, alpha, alpha_conv, nheads_GAT, conv_out_channels):\n",
    "        '''Sparse version of KBGAT\n",
    "        entity_in_dim -> Entity Input Embedding dimensions\n",
    "        entity_out_dim  -> Entity Output Embedding dimensions, passed as a list\n",
    "        num_relation -> number of unique relations\n",
    "        relation_dim -> Relation Embedding dimensions\n",
    "        num_nodes -> number of nodes in the Graph\n",
    "        nheads_GAT -> Used for Multihead attention, passed as a list '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = initial_entity_emb.shape[0]\n",
    "        self.entity_in_dim = initial_entity_emb.shape[1]\n",
    "        self.entity_out_dim_1 = entity_out_dim[0]\n",
    "        self.nheads_GAT_1 = nheads_GAT[0]\n",
    "        self.entity_out_dim_2 = entity_out_dim[1]\n",
    "        self.nheads_GAT_2 = nheads_GAT[1]\n",
    "\n",
    "        # Properties of Relations\n",
    "        self.num_relation = initial_relation_emb.shape[0]\n",
    "        self.relation_dim = initial_relation_emb.shape[1]\n",
    "        self.relation_out_dim_1 = relation_out_dim[0]\n",
    "\n",
    "        self.drop_GAT = drop_GAT\n",
    "        self.drop_conv = drop_conv\n",
    "        self.alpha = alpha      # For leaky relu\n",
    "        self.alpha_conv = alpha_conv\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "\n",
    "        self.final_entity_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_nodes, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.final_relation_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_relation, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.convKB = ConvKB(self.entity_out_dim_1 * self.nheads_GAT_1, 3, 1,\n",
    "                             self.conv_out_channels, self.drop_conv, self.alpha_conv)\n",
    "\n",
    "    def forward(self, Corpus_, adj, batch_inputs):\n",
    "        conv_input = torch.cat((self.final_entity_embeddings[batch_inputs[:, 0], :].unsqueeze(1), self.final_relation_embeddings[\n",
    "            batch_inputs[:, 1]].unsqueeze(1), self.final_entity_embeddings[batch_inputs[:, 2], :].unsqueeze(1)), dim=1)\n",
    "        out_conv = self.convKB(conv_input)\n",
    "        return out_conv\n",
    "\n",
    "    def batch_test(self, batch_inputs):\n",
    "        conv_input = torch.cat((self.final_entity_embeddings[batch_inputs[:, 0], :].unsqueeze(1), self.final_relation_embeddings[\n",
    "            batch_inputs[:, 1]].unsqueeze(1), self.final_entity_embeddings[batch_inputs[:, 2], :].unsqueeze(1)), dim=1)\n",
    "        out_conv = self.convKB(conv_input)\n",
    "        return out_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXAQD6XVMTLJ"
   },
   "source": [
    "# File : layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d_SG2XyMSyV"
   },
   "outputs": [],
   "source": [
    "class ConvKB(nn.Module):\n",
    "    def __init__(self, input_dim, input_seq_len, in_channels, out_channels, drop_prob, alpha_leaky):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels, out_channels, (1, input_seq_len))  # kernel size -> 1*input_seq_length(i.e. 2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear((input_dim) * out_channels, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc_layer.weight, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.conv_layer.weight, gain=1.414)\n",
    "\n",
    "    def forward(self, conv_input):\n",
    "\n",
    "        batch_size, length, dim = conv_input.size()\n",
    "        # assuming inputs are of the form ->\n",
    "        conv_input = conv_input.transpose(1, 2)\n",
    "        # batch * length(which is 3 here -> entity,relation,entity) * dim\n",
    "        # To make tensor of size 4, where second dim is for input channels\n",
    "        conv_input = conv_input.unsqueeze(1)\n",
    "\n",
    "        out_conv = self.dropout(\n",
    "            self.non_linearity(self.conv_layer(conv_input)))\n",
    "\n",
    "        input_fc = out_conv.squeeze(-1).view(batch_size, -1)\n",
    "        output = self.fc_layer(input_fc)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mx1gfrIzMWsu"
   },
   "outputs": [],
   "source": [
    "class SpecialSpmmFunctionFinal(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, edge, edge_w, N, E, out_features):\n",
    "        # assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(\n",
    "            edge, edge_w, torch.Size([N, N, out_features]))\n",
    "        b = torch.sparse.sum(a, dim=1)\n",
    "        ctx.N = b.shape[0]\n",
    "        ctx.outfeat = b.shape[1]\n",
    "        ctx.E = E\n",
    "        ctx.indices = a._indices()[0, :]\n",
    "\n",
    "        return b.to_dense()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_values = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            edge_sources = ctx.indices\n",
    "\n",
    "            if(CUDA):\n",
    "                edge_sources = edge_sources.cuda()\n",
    "\n",
    "            grad_values = grad_output[edge_sources]\n",
    "            # grad_values = grad_values.view(ctx.E, ctx.outfeat)\n",
    "            # print(\"Grad Outputs-> \", grad_output)\n",
    "            # print(\"Grad values-> \", grad_values)\n",
    "        return None, grad_values, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xnz0LzjyMaD9"
   },
   "outputs": [],
   "source": [
    "class SpecialSpmmFinal(nn.Module):\n",
    "    def forward(self, edge, edge_w, N, E, out_features):\n",
    "        return SpecialSpmmFunctionFinal.apply(edge, edge_w, N, E, out_features)\n",
    "\n",
    "\n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, in_features, out_features, nrela_dim, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_nodes = num_nodes\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.nrela_dim = nrela_dim\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(\n",
    "            size=(out_features, 2 * in_features + nrela_dim)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "        self.a_2 = nn.Parameter(torch.zeros(size=(1, out_features)))\n",
    "        nn.init.xavier_normal_(self.a_2.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm_final = SpecialSpmmFinal()\n",
    "\n",
    "    def forward(self, input, edge, edge_embed, edge_list_nhop, edge_embed_nhop):\n",
    "        N = input.size()[0]\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge = torch.cat((edge[:, :], edge_list_nhop[:, :]), dim=1)\n",
    "        edge_embed = torch.cat(\n",
    "            (edge_embed[:, :], edge_embed_nhop[:, :]), dim=0)\n",
    "\n",
    "        edge_h = torch.cat(\n",
    "            (input[edge[0, :], :], input[edge[1, :], :], edge_embed[:, :]), dim=1).t()\n",
    "        # edge_h: (2*in_dim + nrela_dim) x E\n",
    "\n",
    "        edge_m = self.a.mm(edge_h)\n",
    "        # edge_m: D * E\n",
    "\n",
    "        # to be checked later\n",
    "        powers = -self.leakyrelu(self.a_2.mm(edge_m).squeeze())\n",
    "        edge_e = torch.exp(powers).unsqueeze(1)\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm_final(\n",
    "            edge, edge_e, N, edge_e.shape[0], 1)\n",
    "        e_rowsum[e_rowsum == 0.0] = 1e-12\n",
    "\n",
    "        e_rowsum = e_rowsum\n",
    "        # e_rowsum: N x 1\n",
    "        edge_e = edge_e.squeeze(1)\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        edge_w = (edge_e * edge_m).t()\n",
    "        # edge_w: E * D\n",
    "\n",
    "        h_prime = self.special_spmm_final(\n",
    "            edge, edge_w, N, edge_w.shape[0], self.out_features)\n",
    "\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0ZfxfQfMp21"
   },
   "source": [
    "# File : create_batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nxeYUF7MsV5"
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, args, train_data, validation_data, test_data, entity2id,\n",
    "                 relation2id, headTailSelector, batch_size, valid_to_invalid_samples_ratio, unique_entities_train, get_2hop=False):\n",
    "        self.train_triples = train_data[0]\n",
    "\n",
    "        # Converting to sparse tensor\n",
    "        adj_indices = torch.LongTensor(\n",
    "            [train_data[1][0], train_data[1][1]])  # rows and columns\n",
    "        adj_values = torch.LongTensor(train_data[1][2])\n",
    "        self.train_adj_matrix = (adj_indices, adj_values)\n",
    "\n",
    "        # adjacency matrix is needed for train_data only, as GAT is trained for\n",
    "        # training data\n",
    "        self.validation_triples = validation_data[0]\n",
    "        self.test_triples = test_data[0]\n",
    "\n",
    "        self.headTailSelector = headTailSelector  # for selecting random entities\n",
    "        self.entity2id = entity2id\n",
    "        self.id2entity = {v: k for k, v in self.entity2id.items()}\n",
    "        self.relation2id = relation2id\n",
    "        self.id2relation = {v: k for k, v in self.relation2id.items()}\n",
    "        self.batch_size = batch_size\n",
    "        # ratio of valid to invalid samples per batch for training ConvKB Model\n",
    "        self.invalid_valid_ratio = int(valid_to_invalid_samples_ratio)\n",
    "\n",
    "        if(get_2hop):\n",
    "            self.graph = self.get_graph()\n",
    "            self.node_neighbors_2hop = self.get_further_neighbors()\n",
    "\n",
    "        self.unique_entities_train = [self.entity2id[i]\n",
    "                                      for i in unique_entities_train]\n",
    "\n",
    "        self.train_indices = np.array(\n",
    "            list(self.train_triples)).astype(np.int32)\n",
    "        # These are valid triples, hence all have value 1\n",
    "        self.train_values = np.array(\n",
    "            [[1]] * len(self.train_triples)).astype(np.float32)\n",
    "\n",
    "        self.validation_indices = np.array(\n",
    "            list(self.validation_triples)).astype(np.int32)\n",
    "        self.validation_values = np.array(\n",
    "            [[1]] * len(self.validation_triples)).astype(np.float32)\n",
    "\n",
    "        self.test_indices = np.array(list(self.test_triples)).astype(np.int32)\n",
    "        self.test_values = np.array(\n",
    "            [[1]] * len(self.test_triples)).astype(np.float32)\n",
    "\n",
    "        self.valid_triples_dict = {j: i for i, j in enumerate(\n",
    "            self.train_triples + self.validation_triples + self.test_triples)}\n",
    "        print(\"Total triples count {}, training triples {}, validation_triples {}, test_triples {}\".format(len(self.valid_triples_dict), len(self.train_indices),\n",
    "                                                                                                           len(self.validation_indices), len(self.test_indices)))\n",
    "\n",
    "        # For training purpose\n",
    "        self.batch_indices = np.empty(\n",
    "            (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "        self.batch_values = np.empty(\n",
    "            (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "    def get_iteration_batch(self, iter_num):\n",
    "        if (iter_num + 1) * self.batch_size <= len(self.train_indices):\n",
    "            self.batch_indices = np.empty(\n",
    "                (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "            self.batch_values = np.empty(\n",
    "                (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "            indices = range(self.batch_size * iter_num,\n",
    "                            self.batch_size * (iter_num + 1))\n",
    "\n",
    "            self.batch_indices[:self.batch_size,\n",
    "                               :] = self.train_indices[indices, :]\n",
    "            self.batch_values[:self.batch_size,\n",
    "                              :] = self.train_values[indices, :]\n",
    "\n",
    "            last_index = self.batch_size\n",
    "\n",
    "            if self.invalid_valid_ratio > 0:\n",
    "                random_entities = np.random.randint(\n",
    "                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "                # Precopying the same valid indices from 0 to batch_size to rest\n",
    "                # of the indices\n",
    "                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "                for i in range(last_index):\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\n",
    "                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           0] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = last_index * \\\n",
    "                            (self.invalid_valid_ratio // 2) + \\\n",
    "                            (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\n",
    "                               random_entities[current_index]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           2] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                return self.batch_indices, self.batch_values\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "        else:\n",
    "            last_iter_size = len(self.train_indices) - \\\n",
    "                self.batch_size * iter_num\n",
    "            self.batch_indices = np.empty(\n",
    "                (last_iter_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "            self.batch_values = np.empty(\n",
    "                (last_iter_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "            indices = range(self.batch_size * iter_num,\n",
    "                            len(self.train_indices))\n",
    "            self.batch_indices[:last_iter_size,\n",
    "                               :] = self.train_indices[indices, :]\n",
    "            self.batch_values[:last_iter_size,\n",
    "                              :] = self.train_values[indices, :]\n",
    "\n",
    "            last_index = last_iter_size\n",
    "\n",
    "            if self.invalid_valid_ratio > 0:\n",
    "                random_entities = np.random.randint(\n",
    "                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "                # Precopying the same valid indices from 0 to batch_size to rest\n",
    "                # of the indices\n",
    "                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "                for i in range(last_index):\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\n",
    "                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           0] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = last_index * \\\n",
    "                            (self.invalid_valid_ratio // 2) + \\\n",
    "                            (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\n",
    "                               random_entities[current_index]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           2] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                return self.batch_indices, self.batch_values\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "    def get_iteration_batch_nhop(self, current_batch_indices, node_neighbors, batch_size):\n",
    "\n",
    "        self.batch_indices = np.empty(\n",
    "            (batch_size * (self.invalid_valid_ratio + 1), 4)).astype(np.int32)\n",
    "        self.batch_values = np.empty(\n",
    "            (batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "        indices = random.sample(range(len(current_batch_indices)), batch_size)\n",
    "\n",
    "        self.batch_indices[:batch_size,\n",
    "                           :] = current_batch_indices[indices, :]\n",
    "        self.batch_values[:batch_size,\n",
    "                          :] = np.ones((batch_size, 1))\n",
    "\n",
    "        last_index = batch_size\n",
    "\n",
    "        if self.invalid_valid_ratio > 0:\n",
    "            random_entities = np.random.randint(\n",
    "                0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "            # Precopying the same valid indices from 0 to batch_size to rest\n",
    "            # of the indices\n",
    "            self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "            self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "            for i in range(last_index):\n",
    "                for j in range(self.invalid_valid_ratio // 2):\n",
    "                    current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                    self.batch_indices[last_index + current_index,\n",
    "                                       0] = random_entities[current_index]\n",
    "                    self.batch_values[last_index + current_index, :] = [0]\n",
    "\n",
    "                for j in range(self.invalid_valid_ratio // 2):\n",
    "                    current_index = last_index * \\\n",
    "                        (self.invalid_valid_ratio // 2) + \\\n",
    "                        (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                    self.batch_indices[last_index + current_index,\n",
    "                                       3] = random_entities[current_index]\n",
    "                    self.batch_values[last_index + current_index, :] = [0]\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "        return self.batch_indices, self.batch_values\n",
    "\n",
    "    def get_graph(self):\n",
    "        graph = {}\n",
    "        all_tiples = torch.cat([self.train_adj_matrix[0].transpose(\n",
    "            0, 1), self.train_adj_matrix[1].unsqueeze(1)], dim=1)\n",
    "\n",
    "        for data in all_tiples:\n",
    "            source = data[1].data.item()\n",
    "            target = data[0].data.item()\n",
    "            value = data[2].data.item()\n",
    "\n",
    "            if(source not in graph.keys()):\n",
    "                graph[source] = {}\n",
    "                graph[source][target] = value\n",
    "            else:\n",
    "                graph[source][target] = value\n",
    "        print(\"Graph created\")\n",
    "        return graph\n",
    "\n",
    "    def bfs(self, graph, source, nbd_size=2):\n",
    "        visit = {}\n",
    "        distance = {}\n",
    "        parent = {}\n",
    "        distance_lengths = {}\n",
    "\n",
    "        visit[source] = 1\n",
    "        distance[source] = 0\n",
    "        parent[source] = (-1, -1)\n",
    "\n",
    "        q = queue.Queue()\n",
    "        q.put((source, -1))\n",
    "\n",
    "        while(not q.empty()):\n",
    "            top = q.get()\n",
    "            if top[0] in graph.keys():\n",
    "                for target in graph[top[0]].keys():\n",
    "                    if(target in visit.keys()):\n",
    "                        continue\n",
    "                    else:\n",
    "                        q.put((target, graph[top[0]][target]))\n",
    "\n",
    "                        distance[target] = distance[top[0]] + 1\n",
    "\n",
    "                        visit[target] = 1\n",
    "                        if distance[target] > 2:\n",
    "                            continue\n",
    "                        parent[target] = (top[0], graph[top[0]][target])\n",
    "\n",
    "                        if distance[target] not in distance_lengths.keys():\n",
    "                            distance_lengths[distance[target]] = 1\n",
    "\n",
    "        neighbors = {}\n",
    "        for target in visit.keys():\n",
    "            if(distance[target] != nbd_size):\n",
    "                continue\n",
    "            edges = [-1, parent[target][1]]\n",
    "            relations = []\n",
    "            entities = [target]\n",
    "            temp = target\n",
    "            while(parent[temp] != (-1, -1)):\n",
    "                relations.append(parent[temp][1])\n",
    "                entities.append(parent[temp][0])\n",
    "                temp = parent[temp][0]\n",
    "\n",
    "            if(distance[target] in neighbors.keys()):\n",
    "                neighbors[distance[target]].append(\n",
    "                    (tuple(relations), tuple(entities[:-1])))\n",
    "            else:\n",
    "                neighbors[distance[target]] = [\n",
    "                    (tuple(relations), tuple(entities[:-1]))]\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def get_further_neighbors(self, nbd_size=2):\n",
    "        neighbors = {}\n",
    "        start_time = time.time()\n",
    "        print(\"length of graph keys is \", len(self.graph.keys()))\n",
    "        for source in self.graph.keys():\n",
    "            # st_time = time.time()\n",
    "            temp_neighbors = self.bfs(self.graph, source, nbd_size)\n",
    "            for distance in temp_neighbors.keys():\n",
    "                if(source in neighbors.keys()):\n",
    "                    if(distance in neighbors[source].keys()):\n",
    "                        neighbors[source][distance].append(\n",
    "                            temp_neighbors[distance])\n",
    "                    else:\n",
    "                        neighbors[source][distance] = temp_neighbors[distance]\n",
    "                else:\n",
    "                    neighbors[source] = {}\n",
    "                    neighbors[source][distance] = temp_neighbors[distance]\n",
    "\n",
    "        print(\"time taken \", time.time() - start_time)\n",
    "\n",
    "        print(\"length of neighbors dict is \", len(neighbors))\n",
    "        return neighbors\n",
    "\n",
    "    def get_batch_nhop_neighbors_all(self, args, batch_sources, node_neighbors, nbd_size=2):\n",
    "        batch_source_triples = []\n",
    "        print(\"length of unique_entities \", len(batch_sources))\n",
    "        count = 0\n",
    "        for source in batch_sources:\n",
    "            # randomly select from the list of neighbors\n",
    "            if source in node_neighbors.keys():\n",
    "                nhop_list = node_neighbors[source][nbd_size]\n",
    "\n",
    "                for i, tup in enumerate(nhop_list):\n",
    "                    if(args.partial_2hop and i >= 2):\n",
    "                        break\n",
    "\n",
    "                    count += 1\n",
    "                    batch_source_triples.append([source, nhop_list[i][0][-1], nhop_list[i][0][0],\n",
    "                                                 nhop_list[i][1][0]])\n",
    "\n",
    "        return np.array(batch_source_triples).astype(np.int32)\n",
    "\n",
    "    def transe_scoring(self, batch_inputs, entity_embeddings, relation_embeddings):\n",
    "        source_embeds = entity_embeddings[batch_inputs[:, 0]]\n",
    "        relation_embeds = relation_embeddings[batch_inputs[:, 1]]\n",
    "        tail_embeds = entity_embeddings[batch_inputs[:, 2]]\n",
    "        x = source_embeds + relation_embed - tail_embeds\n",
    "        x = torch.norm(x, p=1, dim=1)\n",
    "        return x\n",
    "\n",
    "    def get_validation_pred(self, args, model, unique_entities):\n",
    "        average_hits_at_100_head, average_hits_at_100_tail = [], []\n",
    "        average_hits_at_ten_head, average_hits_at_ten_tail = [], []\n",
    "        average_hits_at_three_head, average_hits_at_three_tail = [], []\n",
    "        average_hits_at_one_head, average_hits_at_one_tail = [], []\n",
    "        average_mean_rank_head, average_mean_rank_tail = [], []\n",
    "        average_mean_recip_rank_head, average_mean_recip_rank_tail = [], []\n",
    "\n",
    "        for iters in range(1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            indices = [i for i in range(len(self.test_indices))]\n",
    "            batch_indices = self.test_indices[indices, :]\n",
    "            print(\"Sampled indices\")\n",
    "            print(\"test set length \", len(self.test_indices))\n",
    "            entity_list = [j for i, j in self.entity2id.items()]\n",
    "\n",
    "            ranks_head, ranks_tail = [], []\n",
    "            reciprocal_ranks_head, reciprocal_ranks_tail = [], []\n",
    "            hits_at_100_head, hits_at_100_tail = 0, 0\n",
    "            hits_at_ten_head, hits_at_ten_tail = 0, 0\n",
    "            hits_at_three_head, hits_at_three_tail = 0, 0\n",
    "            hits_at_one_head, hits_at_one_tail = 0, 0\n",
    "\n",
    "            for i in range(batch_indices.shape[0]):\n",
    "                print(len(ranks_head))\n",
    "                start_time_it = time.time()\n",
    "                new_x_batch_head = np.tile(\n",
    "                    batch_indices[i, :], (len(self.entity2id), 1))\n",
    "                new_x_batch_tail = np.tile(\n",
    "                    batch_indices[i, :], (len(self.entity2id), 1))\n",
    "\n",
    "                if(batch_indices[i, 0] not in unique_entities or batch_indices[i, 2] not in unique_entities):\n",
    "                    continue\n",
    "\n",
    "                new_x_batch_head[:, 0] = entity_list\n",
    "                new_x_batch_tail[:, 2] = entity_list\n",
    "\n",
    "                last_index_head = []  # array of already existing triples\n",
    "                last_index_tail = []\n",
    "                for tmp_index in range(len(new_x_batch_head)):\n",
    "                    temp_triple_head = (new_x_batch_head[tmp_index][0], new_x_batch_head[tmp_index][1],\n",
    "                                        new_x_batch_head[tmp_index][2])\n",
    "                    if temp_triple_head in self.valid_triples_dict.keys():\n",
    "                        last_index_head.append(tmp_index)\n",
    "\n",
    "                    temp_triple_tail = (new_x_batch_tail[tmp_index][0], new_x_batch_tail[tmp_index][1],\n",
    "                                        new_x_batch_tail[tmp_index][2])\n",
    "                    if temp_triple_tail in self.valid_triples_dict.keys():\n",
    "                        last_index_tail.append(tmp_index)\n",
    "\n",
    "                # Deleting already existing triples, leftover triples are invalid, according\n",
    "                # to train, validation and test data\n",
    "                # Note, all of them maynot be actually invalid\n",
    "                new_x_batch_head = np.delete(\n",
    "                    new_x_batch_head, last_index_head, axis=0)\n",
    "                new_x_batch_tail = np.delete(\n",
    "                    new_x_batch_tail, last_index_tail, axis=0)\n",
    "\n",
    "                # adding the current valid triples to the top, i.e, index 0\n",
    "                new_x_batch_head = np.insert(\n",
    "                    new_x_batch_head, 0, batch_indices[i], axis=0)\n",
    "                new_x_batch_tail = np.insert(\n",
    "                    new_x_batch_tail, 0, batch_indices[i], axis=0)\n",
    "\n",
    "                import math\n",
    "                # Have to do this, because it doesn't fit in memory\n",
    "\n",
    "                if 'WN' in args.data:\n",
    "                    num_triples_each_shot = int(\n",
    "                        math.ceil(new_x_batch_head.shape[0] / 4))\n",
    "\n",
    "                    scores1_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[:num_triples_each_shot, :]).cuda())\n",
    "                    scores2_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())\n",
    "                    scores3_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())\n",
    "                    scores4_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores5_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores6_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[5 * num_triples_each_shot: 6 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores7_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[6 * num_triples_each_shot: 7 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores8_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[7 * num_triples_each_shot: 8 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores9_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[8 * num_triples_each_shot: 9 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores10_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[9 * num_triples_each_shot:, :]).cuda())\n",
    "\n",
    "                    scores_head = torch.cat(\n",
    "                        [scores1_head, scores2_head, scores3_head, scores4_head], dim=0)\n",
    "                    #scores5_head, scores6_head, scores7_head, scores8_head,\n",
    "                    # cores9_head, scores10_head], dim=0)\n",
    "                else:\n",
    "                    scores_head = model.batch_test(new_x_batch_head)\n",
    "\n",
    "                sorted_scores_head, sorted_indices_head = torch.sort(\n",
    "                    scores_head.view(-1), dim=-1, descending=True)\n",
    "                # Just search for zeroth index in the sorted scores, we appended valid triple at top\n",
    "                ranks_head.append(\n",
    "                    np.where(sorted_indices_head.cpu().numpy() == 0)[0][0] + 1)\n",
    "                reciprocal_ranks_head.append(1.0 / ranks_head[-1])\n",
    "\n",
    "                # Tail part here\n",
    "\n",
    "                if 'WN' in args.data:\n",
    "                    num_triples_each_shot = int(\n",
    "                        math.ceil(new_x_batch_tail.shape[0] / 4))\n",
    "\n",
    "                    scores1_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[:num_triples_each_shot, :]).cuda())\n",
    "                    scores2_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())\n",
    "                    scores3_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())\n",
    "                    scores4_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores5_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores6_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[5 * num_triples_each_shot: 6 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores7_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[6 * num_triples_each_shot: 7 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores8_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[7 * num_triples_each_shot: 8 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores9_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[8 * num_triples_each_shot: 9 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores10_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[9 * num_triples_each_shot:, :]).cuda())\n",
    "\n",
    "                    scores_tail = torch.cat(\n",
    "                        [scores1_tail, scores2_tail, scores3_tail, scores4_tail], dim=0)\n",
    "                    #     scores5_tail, scores6_tail, scores7_tail, scores8_tail,\n",
    "                    #     scores9_tail, scores10_tail], dim=0)\n",
    "\n",
    "                else:\n",
    "                    scores_tail = model.batch_test(new_x_batch_tail)\n",
    "\n",
    "                sorted_scores_tail, sorted_indices_tail = torch.sort(\n",
    "                    scores_tail.view(-1), dim=-1, descending=True)\n",
    "\n",
    "                # Just search for zeroth index in the sorted scores, we appended valid triple at top\n",
    "                ranks_tail.append(\n",
    "                    np.where(sorted_indices_tail.cpu().numpy() == 0)[0][0] + 1)\n",
    "                reciprocal_ranks_tail.append(1.0 / ranks_tail[-1])\n",
    "                print(\"sample - \", ranks_head[-1], ranks_tail[-1])\n",
    "\n",
    "            for i in range(len(ranks_head)):\n",
    "                if ranks_head[i] <= 100:\n",
    "                    hits_at_100_head = hits_at_100_head + 1\n",
    "                if ranks_head[i] <= 10:\n",
    "                    hits_at_ten_head = hits_at_ten_head + 1\n",
    "                if ranks_head[i] <= 3:\n",
    "                    hits_at_three_head = hits_at_three_head + 1\n",
    "                if ranks_head[i] == 1:\n",
    "                    hits_at_one_head = hits_at_one_head + 1\n",
    "\n",
    "            for i in range(len(ranks_tail)):\n",
    "                if ranks_tail[i] <= 100:\n",
    "                    hits_at_100_tail = hits_at_100_tail + 1\n",
    "                if ranks_tail[i] <= 10:\n",
    "                    hits_at_ten_tail = hits_at_ten_tail + 1\n",
    "                if ranks_tail[i] <= 3:\n",
    "                    hits_at_three_tail = hits_at_three_tail + 1\n",
    "                if ranks_tail[i] == 1:\n",
    "                    hits_at_one_tail = hits_at_one_tail + 1\n",
    "\n",
    "            assert len(ranks_head) == len(reciprocal_ranks_head)\n",
    "            assert len(ranks_tail) == len(reciprocal_ranks_tail)\n",
    "            print(\"here {}\".format(len(ranks_head)))\n",
    "            print(\"\\nCurrent iteration time {}\".format(time.time() - start_time))\n",
    "            print(\"Stats for replacing head are -> \")\n",
    "            print(\"Current iteration Hits@100 are {}\".format(\n",
    "                hits_at_100_head / float(len(ranks_head))))\n",
    "            print(\"Current iteration Hits@10 are {}\".format(\n",
    "                hits_at_ten_head / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@3 are {}\".format(\n",
    "                hits_at_three_head / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@1 are {}\".format(\n",
    "                hits_at_one_head / len(ranks_head)))\n",
    "            print(\"Current iteration Mean rank {}\".format(\n",
    "                sum(ranks_head) / len(ranks_head)))\n",
    "            print(\"Current iteration Mean Reciprocal Rank {}\".format(\n",
    "                sum(reciprocal_ranks_head) / len(reciprocal_ranks_head)))\n",
    "\n",
    "            print(\"\\nStats for replacing tail are -> \")\n",
    "            print(\"Current iteration Hits@100 are {}\".format(\n",
    "                hits_at_100_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@10 are {}\".format(\n",
    "                hits_at_ten_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@3 are {}\".format(\n",
    "                hits_at_three_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@1 are {}\".format(\n",
    "                hits_at_one_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Mean rank {}\".format(\n",
    "                sum(ranks_tail) / len(ranks_tail)))\n",
    "            print(\"Current iteration Mean Reciprocal Rank {}\".format(\n",
    "                sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail)))\n",
    "\n",
    "            average_hits_at_100_head.append(\n",
    "                hits_at_100_head / len(ranks_head))\n",
    "            average_hits_at_ten_head.append(\n",
    "                hits_at_ten_head / len(ranks_head))\n",
    "            average_hits_at_three_head.append(\n",
    "                hits_at_three_head / len(ranks_head))\n",
    "            average_hits_at_one_head.append(\n",
    "                hits_at_one_head / len(ranks_head))\n",
    "            average_mean_rank_head.append(sum(ranks_head) / len(ranks_head))\n",
    "            average_mean_recip_rank_head.append(\n",
    "                sum(reciprocal_ranks_head) / len(reciprocal_ranks_head))\n",
    "\n",
    "            average_hits_at_100_tail.append(\n",
    "                hits_at_100_tail / len(ranks_head))\n",
    "            average_hits_at_ten_tail.append(\n",
    "                hits_at_ten_tail / len(ranks_head))\n",
    "            average_hits_at_three_tail.append(\n",
    "                hits_at_three_tail / len(ranks_head))\n",
    "            average_hits_at_one_tail.append(\n",
    "                hits_at_one_tail / len(ranks_head))\n",
    "            average_mean_rank_tail.append(sum(ranks_tail) / len(ranks_tail))\n",
    "            average_mean_recip_rank_tail.append(\n",
    "                sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail))\n",
    "\n",
    "        print(\"\\nAveraged stats for replacing head are -> \")\n",
    "        print(\"Hits@100 are {}\".format(\n",
    "            sum(average_hits_at_100_head) / len(average_hits_at_100_head)))\n",
    "        print(\"Hits@10 are {}\".format(\n",
    "            sum(average_hits_at_ten_head) / len(average_hits_at_ten_head)))\n",
    "        print(\"Hits@3 are {}\".format(\n",
    "            sum(average_hits_at_three_head) / len(average_hits_at_three_head)))\n",
    "        print(\"Hits@1 are {}\".format(\n",
    "            sum(average_hits_at_one_head) / len(average_hits_at_one_head)))\n",
    "        print(\"Mean rank {}\".format(\n",
    "            sum(average_mean_rank_head) / len(average_mean_rank_head)))\n",
    "        print(\"Mean Reciprocal Rank {}\".format(\n",
    "            sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head)))\n",
    "\n",
    "        print(\"\\nAveraged stats for replacing tail are -> \")\n",
    "        print(\"Hits@100 are {}\".format(\n",
    "            sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)))\n",
    "        print(\"Hits@10 are {}\".format(\n",
    "            sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)))\n",
    "        print(\"Hits@3 are {}\".format(\n",
    "            sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)))\n",
    "        print(\"Hits@1 are {}\".format(\n",
    "            sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)))\n",
    "        print(\"Mean rank {}\".format(\n",
    "            sum(average_mean_rank_tail) / len(average_mean_rank_tail)))\n",
    "        print(\"Mean Reciprocal Rank {}\".format(\n",
    "            sum(average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)))\n",
    "\n",
    "        cumulative_hits_100 = (sum(average_hits_at_100_head) / len(average_hits_at_100_head)\n",
    "                               + sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)) / 2\n",
    "        cumulative_hits_ten = (sum(average_hits_at_ten_head) / len(average_hits_at_ten_head)\n",
    "                               + sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)) / 2\n",
    "        cumulative_hits_three = (sum(average_hits_at_three_head) / len(average_hits_at_three_head)\n",
    "                                 + sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)) / 2\n",
    "        cumulative_hits_one = (sum(average_hits_at_one_head) / len(average_hits_at_one_head)\n",
    "                               + sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)) / 2\n",
    "        cumulative_mean_rank = (sum(average_mean_rank_head) / len(average_mean_rank_head)\n",
    "                                + sum(average_mean_rank_tail) / len(average_mean_rank_tail)) / 2\n",
    "        cumulative_mean_recip_rank = (sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head) + sum(\n",
    "            average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)) / 2\n",
    "\n",
    "        print(\"\\nCumulative stats are -> \")\n",
    "        print(\"Hits@100 are {}\".format(cumulative_hits_100))\n",
    "        print(\"Hits@10 are {}\".format(cumulative_hits_ten))\n",
    "        print(\"Hits@3 are {}\".format(cumulative_hits_three))\n",
    "        print(\"Hits@1 are {}\".format(cumulative_hits_one))\n",
    "        print(\"Mean rank {}\".format(cumulative_mean_rank))\n",
    "        print(\"Mean Reciprocal Rank {}\".format(cumulative_mean_recip_rank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WTVD2Y3EeyO"
   },
   "outputs": [],
   "source": [
    "def build_data(path='./data/WN18RR/', is_unweigted=False, directed=True):\n",
    "    entity2id = read_entity_from_id(path + 'entity2id.txt')\n",
    "    relation2id = read_relation_from_id(path + 'relation2id.txt')\n",
    "\n",
    "    # Adjacency matrix only required for training phase\n",
    "    # Currenlty creating as unweighted, undirected\n",
    "    train_triples, train_adjacency_mat, unique_entities_train = load_data(os.path.join(\n",
    "        path, 'train.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    validation_triples, valid_adjacency_mat, unique_entities_validation = load_data(\n",
    "        os.path.join(path, 'valid.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    test_triples, test_adjacency_mat, unique_entities_test = load_data(os.path.join(\n",
    "        path, 'test.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "\n",
    "    id2entity = {v: k for k, v in entity2id.items()}\n",
    "    id2relation = {v: k for k, v in relation2id.items()}\n",
    "    left_entity, right_entity = {}, {}\n",
    "\n",
    "    with open(os.path.join(path, 'train.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "\n",
    "        # Count number of occurences for each (e1, relation)\n",
    "        if relation2id[relation] not in left_entity:\n",
    "            left_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e1] not in left_entity[relation2id[relation]]:\n",
    "            left_entity[relation2id[relation]][entity2id[e1]] = 0\n",
    "        left_entity[relation2id[relation]][entity2id[e1]] += 1\n",
    "\n",
    "        # Count number of occurences for each (relation, e2)\n",
    "        if relation2id[relation] not in right_entity:\n",
    "            right_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e2] not in right_entity[relation2id[relation]]:\n",
    "            right_entity[relation2id[relation]][entity2id[e2]] = 0\n",
    "        right_entity[relation2id[relation]][entity2id[e2]] += 1\n",
    "\n",
    "    left_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        left_entity_avg[i] = sum(\n",
    "            left_entity[i].values()) * 1.0 / len(left_entity[i])\n",
    "\n",
    "    right_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        right_entity_avg[i] = sum(\n",
    "            right_entity[i].values()) * 1.0 / len(right_entity[i])\n",
    "\n",
    "    headTailSelector = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        headTailSelector[i] = 1000 * right_entity_avg[i] / \\\n",
    "            (right_entity_avg[i] + left_entity_avg[i])\n",
    "\n",
    "    return (train_triples, train_adjacency_mat), (validation_triples, valid_adjacency_mat), (test_triples, test_adjacency_mat), \\\n",
    "        entity2id, relation2id, headTailSelector, unique_entities_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJjp1V-rF0Bk"
   },
   "outputs": [],
   "source": [
    "def main_load_data(args):\n",
    "    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\n",
    "                                                                 os.path.join(args.data, 'relation2vec.txt'))\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        entity_embeddings = np.random.randn(\n",
    "            len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(\n",
    "            len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\n",
    "\n",
    "    return corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "gO6BWCO9GPxN",
    "outputId": "abf5f5ef-deac-431c-dc92-40663d791cf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique_entities -> 40559\n",
      "number of unique_entities -> 5173\n",
      "number of unique_entities -> 5323\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        \"./data/WN18RR/\", is_unweigted=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "dVDLIBLTHSHp",
    "outputId": "1432dc23-a87c-46ea-a725-5d1e62e8ab00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_triples (3134, 3)\n",
      "test_adjacency_mat (3, 3134)\n",
      "entity2id 40943\n",
      "relation2id 11\n",
      "headTailSelector 11\n",
      "unique_entities_train (40559,)\n"
     ]
    }
   ],
   "source": [
    "#(train_triples, train_adjacency_mat) = train_data\n",
    "(test_triples, test_adjacency_mat) = test_data\n",
    "print(\"test_triples\", np.array(test_triples).shape)\n",
    "print(\"test_adjacency_mat\", np.array(test_adjacency_mat).shape)\n",
    "print(\"entity2id\", len(entity2id))\n",
    "print(\"relation2id\", len(relation2id))\n",
    "print(\"headTailSelector\", len(headTailSelector))\n",
    "print(\"unique_entities_train\", np.array(unique_entities_train).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "GFBwb46WJZ6E",
    "outputId": "d89b0edf-d2a0-402d-c50f-60be44aa9aa5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1678, 7, 9202),\n",
       " (17935, 9, 35175),\n",
       " (785, 8, 23537),\n",
       " (16176, 4, 33688),\n",
       " (18671, 0, 11241),\n",
       " (15582, 0, 4030),\n",
       " (19165, 0, 2969),\n",
       " (16748, 6, 29001),\n",
       " (31740, 0, 1183),\n",
       " (26184, 1, 32091)]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_triples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mLa2jXp7JscG",
    "outputId": "c881a583-3fb3-4c92-bd34-c612433881f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9202, 35175, 23537, 33688, 11241, 4030, 2969, 29001, 1183, 32091]"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_adjacency_mat[0][:10]\n",
    "# test_adjacency_mat[1][:10]\n",
    "# test_adjacency_mat[2][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "SfhCp6I7KBWH",
    "outputId": "e30d7413-75a9-499b-9866-c64b9788d85c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "785"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity2id[\"08860123\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3-MjMH9hKPxn",
    "outputId": "37c52c46-349f-43e9-a1a5-0471102bcbc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation2id[\"_member_of_domain_region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VXCahQKyKikS",
    "outputId": "7155a914-9bab-492e-9fb7-dda15046264f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])"
      ]
     },
     "execution_count": 36,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headTailSelector.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "nGt3NrgfLdRd",
    "outputId": "541cf6d6-26db-4224-8f4f-44deeba169f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([781.7747455952955, 499.8913414672007, 859.2334494773519, 473.22623828647926, 296.5979875419263, 905.8213959158792, 331.43431635388737, 40.38772213247173, 115.50151975683893, 499.4892747701736, 503.2679738562091])"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headTailSelector.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_yz1pZuiC_aG"
   },
   "source": [
    "# Bỏ qua đoạn kiểm tra này"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ZJPC4tHuDjbl",
    "outputId": "7c8d2629-f101-49d9-d9f2-f040f1bb3e91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total triples count 93003, training triples 86835, validation_triples 3034, test_triples 3134\n"
     ]
    }
   ],
   "source": [
    "corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    batch_size_gat, valid_invalid_ratio_gat, unique_entities_train, get_2hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMH5Q3w4DmUr"
   },
   "outputs": [],
   "source": [
    "Corpus_, entity_embeddings, relation_embeddings = corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PvEQLUTNDrUd",
    "outputId": "a54c0678-7055-4497-86be-debfb9a393e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40943, 50])"
      ]
     },
     "execution_count": 122,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "S5RZ3wC-G035",
    "outputId": "ced2fc51-d3c6-4641-c8d1-d89c85b8ad5b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([11, 50])"
      ]
     },
     "execution_count": 123,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Z_eIHwOQG2tr",
    "outputId": "d80e0992-e04c-4ed7-cd45-133b4c2d8de2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9957, -1.1905,  0.2528,  1.2205,  0.2159, -0.5909, -0.5544,  0.7191,\n",
       "         -1.5233,  1.1198,  0.0863,  0.9041,  0.1274,  0.5563, -0.0929, -0.0117,\n",
       "          1.2185,  0.8542, -0.5128,  0.1235,  1.1303, -0.4221, -0.7805, -0.4411,\n",
       "         -0.4397, -1.0779, -0.8092,  0.3473, -1.0209, -0.7630, -0.0816, -0.3541,\n",
       "         -0.0238, -1.5182,  0.8176, -2.0052,  2.3407, -0.2853,  0.2744, -0.8679,\n",
       "          0.7527,  0.5025,  0.1100,  0.0049,  2.3610, -0.0921,  1.1440, -0.1057,\n",
       "          0.7845,  0.6430]])"
      ]
     },
     "execution_count": 124,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_embeddings[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "i6RX0PLqG8ya",
    "outputId": "a30e28d1-f3a8-4082-9c61-671ad33d30f3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4600,  0.6388, -0.4167, -0.0617, -0.3684, -0.8644,  1.4947, -1.9514,\n",
       "          0.4064,  0.7494,  1.2722, -1.6969, -1.0620, -0.5447, -0.0468,  0.2542,\n",
       "          0.7984,  0.7184,  0.6269,  1.4628,  0.3126, -1.2833, -0.7911, -0.3806,\n",
       "          0.3122,  1.2646, -1.1545,  0.3389, -0.1835, -0.2077,  1.8141,  1.3248,\n",
       "         -2.0536, -1.5047, -1.7833, -0.9011,  0.9049, -1.3002,  0.3065,  0.3812,\n",
       "          0.1603, -2.0816, -0.5988,  0.8691, -0.0203,  0.2274,  0.0140, -0.5235,\n",
       "         -0.3634,  1.4274]])"
      ]
     },
     "execution_count": 125,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation_embeddings[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75qumRDtG-S0"
   },
   "outputs": [],
   "source": [
    "if(args.get_2hop):\n",
    "    file = args.data + \"/2hop.pickle\"\n",
    "    with open(file, 'wb') as handle:\n",
    "        pickle.dump(Corpus_.node_neighbors_2hop, handle,\n",
    "                    protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Hln4kwihHTpQ",
    "outputId": "e5e9c212-9e28-450c-9c73-ea6c855173b3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 127,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if(args.use_2hop):\n",
    "#     print(\"Opening node_neighbors pickle object\")\n",
    "#     file = args.data + \"/2hop.pickle\"\n",
    "#     with open(file, 'rb') as handle:\n",
    "#         node_neighbors_2hop = pickle.load(handle)\n",
    "args.get_2hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kvU7GGeXHVzc"
   },
   "outputs": [],
   "source": [
    "entity_embeddings_copied = deepcopy(entity_embeddings)\n",
    "relation_embeddings_copied = deepcopy(relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XJkcupMqHdh3",
    "outputId": "136e1808-cdc4-46dd-dc37-440871c18d41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial entity dimensions torch.Size([40943, 50]) , relation dimensions torch.Size([11, 50])\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial entity dimensions {} , relation dimensions {}\".format(\n",
    "    entity_embeddings.size(), relation_embeddings.size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "tfdoYX6RHe2O",
    "outputId": "31004a3a-ac32-4def-ed91-e4f770aafdc3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rf6ahWwKdQLY"
   },
   "source": [
    "# Bắt đầu lại ở đây"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xy-rjzy9fluF"
   },
   "source": [
    "### Không chạy cái này, chạy dòng dưới để load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "o1tn_TuhHs27",
    "outputId": "0e9fc2b7-486a-47ae-ad4e-87b71e390dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique_entities -> 40559\n",
      "number of unique_entities -> 5173\n",
      "number of unique_entities -> 5323\n",
      "Initialised relations and entities from TransE\n",
      "Graph created\n",
      "length of graph keys is  39610\n",
      "time taken  3225.77739071846\n",
      "length of neighbors dict is  39115\n",
      "Total triples count 93003, training triples 86835, validation_triples 3034, test_triples 3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def load_data_main(args):\n",
    "    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\n",
    "                                                                 os.path.join(args.data, 'relation2vec.txt'))\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        entity_embeddings = np.random.randn(\n",
    "            len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(\n",
    "            len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\n",
    "\n",
    "    return corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)\n",
    "\n",
    "\n",
    "Corpus_, entity_embeddings, relation_embeddings = load_data_main(args)\n",
    "\n",
    "with open(\"Corpus.pk\", 'wb') as Corpus_pk:\n",
    "  pickle.dump(Corpus_, Corpus_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHjGer0jM-kb"
   },
   "outputs": [],
   "source": [
    "print(entity_embeddings.type(n), relation_embeddings.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIBEq0go0DmH"
   },
   "outputs": [],
   "source": [
    "torch.save(Corpus_, \"Corpus_torch.pt\")\n",
    "torch.save(entity_embeddings, \"./data/WN18RR/entity_embeddings.pt\")\n",
    "torch.save(relation_embeddings, \"./data/WN18RR/relation_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKwcZLp_Cg5a"
   },
   "source": [
    "### Load entity và relation trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAaeubw9RMzd"
   },
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb97zZQZZwZ8"
   },
   "outputs": [],
   "source": [
    "entity_embeddings = torch.load(\"./data/WN18RR/entity_embeddings.pt\")\n",
    "relation_embeddings = torch.load(\"./data/WN18RR/relation_embeddings.pt\")\n",
    "Corpus_ = torch.load(\"/content/Corpus_torch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "cV2uiNpKwHnX",
    "outputId": "fc410c53-1215-4fa5-d37c-de52933233b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening node_neighbors pickle object\n",
      "Initial entity dimensions torch.Size([40943, 50]) , relation dimensions torch.Size([11, 50])\n"
     ]
    }
   ],
   "source": [
    "if(args.use_2hop):\n",
    "    print(\"Opening node_neighbors pickle object\")\n",
    "    file = \"/content/2hop.pickle\"\n",
    "    with open(file, 'rb') as handle:\n",
    "        node_neighbors_2hop = pickle.load(handle)\n",
    "\n",
    "entity_embeddings_copied = deepcopy(entity_embeddings)\n",
    "relation_embeddings_copied = deepcopy(relation_embeddings)\n",
    "\n",
    "print(\"Initial entity dimensions {} , relation dimensions {}\".format(\n",
    "    entity_embeddings.size(), relation_embeddings.size()))\n",
    "# %%\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHFOSgYsgUAG"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mAIHCibchEka"
   },
   "outputs": [],
   "source": [
    "def save_model(model, name, epoch, model_name):\n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(), \"/content/{0}/trained_{1}.pth\".format(model_name, epoch))\n",
    "    print(\"Done saving Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxejLRbqaMVo"
   },
   "outputs": [],
   "source": [
    "def batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):\n",
    "    len_pos_triples = int(\n",
    "        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))\n",
    "\n",
    "    pos_triples = train_indices[:len_pos_triples]\n",
    "    neg_triples = train_indices[len_pos_triples:]\n",
    "\n",
    "    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)\n",
    "\n",
    "    source_embeds = entity_embed[pos_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[pos_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[pos_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    pos_norm = torch.norm(x, p=1, dim=1)\n",
    "\n",
    "    source_embeds = entity_embed[neg_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[neg_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[neg_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    neg_norm = torch.norm(x, p=1, dim=1)\n",
    "\n",
    "    y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()\n",
    "\n",
    "    loss = gat_loss_func(pos_norm, neg_norm, y)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_gat(args):\n",
    "\n",
    "    # Creating the gat model here.\n",
    "    ####################################\n",
    "\n",
    "    print(\"Defining model\")\n",
    "\n",
    "    print(\n",
    "        \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\n",
    "    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "\n",
    "    if CUDA:\n",
    "        model_gat.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=500, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "    gat_loss_func = nn.MarginRankingLoss(margin=args.margin)\n",
    "\n",
    "    current_batch_2hop_indices = torch.tensor([])\n",
    "    if(args.use_2hop):\n",
    "        current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\n",
    "                                                                          Corpus_.unique_entities_train, node_neighbors_2hop)\n",
    "\n",
    "    if CUDA:\n",
    "        current_batch_2hop_indices = Variable(\n",
    "            torch.LongTensor(current_batch_2hop_indices)).cuda()\n",
    "    else:\n",
    "        current_batch_2hop_indices = Variable(\n",
    "            torch.LongTensor(current_batch_2hop_indices))\n",
    "\n",
    "    epoch_losses = []   # losses of all epochs\n",
    "    print(\"Number of epochs {}\".format(args.epochs_gat))\n",
    "\n",
    "    for epoch in range(args.epochs_gat):\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_gat.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_gat == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_gat\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_gat) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            # forward pass\n",
    "            entity_embed, relation_embed = model_gat(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = batch_gat_loss(\n",
    "                gat_loss_func, train_indices, entity_embed, relation_embed)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "        if epoch > 3595:\n",
    "            save_model(model_gat, args.data, epoch, \"gat\")      \n",
    "\n",
    "\n",
    "def train_conv(args):\n",
    "\n",
    "    # Creating convolution model here.\n",
    "    ####################################\n",
    "\n",
    "    print(\"Defining model\")\n",
    "    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "    print(\"Only Conv model trained\")\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "\n",
    "    if CUDA:\n",
    "        model_conv.cuda()\n",
    "        model_gat.cuda()\n",
    "\n",
    "    model_gat.load_state_dict(torch.load(\n",
    "        '{}/trained_{}.pth'.format(\"/content/gat\", args.epochs_gat - 1)), strict=False)\n",
    "    model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "    model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "    Corpus_.batch_size = args.batch_size_conv\n",
    "    Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "    margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "    epoch_losses = []   # losses of all epochs\n",
    "    print(\"Number of epochs {}\".format(args.epochs_conv))\n",
    "\n",
    "    for epoch in range(args.epochs_conv):\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_conv.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_conv == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_conv\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_conv) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            preds = model_conv(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = margin_loss(preds.view(-1), train_values.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "        if epoch > 195:\n",
    "            save_model(model_conv, args.data, epoch, \"conv\")\n",
    "        \n",
    "\n",
    "def evaluate_conv(args, unique_entities):\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "    model_conv.load_state_dict(torch.load(\n",
    "        '{0}conv/trained_{1}.pth'.format(args.output_folder, args.epochs_conv - 1)), strict=False)\n",
    "\n",
    "    model_conv.cuda()\n",
    "    model_conv.eval()\n",
    "    with torch.no_grad():\n",
    "        Corpus_.get_validation_pred(args, model_conv, unique_entities)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdIoKzwogXDf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3doI8Ydgj5k"
   },
   "source": [
    "Traning mô hình GAT trước rồi mới traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "XO7FxwFJadXt",
    "outputId": "7eded5d4-26bc-4c7d-c8c0-0d99c33e4931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "\n",
      "Model type -> GAT layer with 2 heads used , Initital Embeddings training\n",
      "length of unique_entities  40559\n",
      "Number of epochs 3600\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n"
     ]
    }
   ],
   "source": [
    "train_gat(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 171
    },
    "colab_type": "code",
    "id": "O6J3WFRsRaug",
    "outputId": "5e0891f0-a8c6-4505-fcfe-f839121ab036"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-3f3524384c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"/content/model_gat.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_gat' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(model_gat, \"/content/model_gat.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "-4ghLUbV3u4y",
    "outputId": "8df4d77f-1b97-4ece-9a1e-157ffed2ad6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "Only Conv model trained\n",
      "Number of epochs 200\n"
     ]
    }
   ],
   "source": [
    "train_conv(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8kRuQBb34-O"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-D2GaRUQ34bx"
   },
   "outputs": [],
   "source": [
    "def evaluate_conv(args, unique_entities):\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "    model_conv.load_state_dict(torch.load(\n",
    "        '{0}/trained_{1}.pth'.format(\"/content/conv\", args.epochs_conv - 1)), strict=False)\n",
    "\n",
    "    model_conv.cuda()\n",
    "    model_conv.eval()\n",
    "    with torch.no_grad():\n",
    "        Corpus_.get_validation_pred(args, model_conv, unique_entities)\n",
    "\n",
    "\n",
    "evaluate_conv(args, Corpus_.unique_entities_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "CoU8rzbcRbYM",
    "outputId": "a759f7b6-b56f-4efc-836e-d8c472b34ab8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-074c29f8f510>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# summary(model_gat, entity_embeddings.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdict_parameter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_gat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for key, value in dict_parameter:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#   print(key)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_gat' is not defined"
     ]
    }
   ],
   "source": [
    "# summary(model_gat, entity_embeddings.shape)\n",
    "dict_parameter = model_gat.state_dict()\n",
    "\n",
    "# for key, value in dict_parameter:\n",
    "#   print(key)\n",
    "\n",
    "type(dict_parameter)\n",
    "\n",
    "for para_name in dict_parameter:\n",
    "  print(\"{0} : {1}\".format(para_name, dict_parameter[para_name].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZCDezP-bWSoY"
   },
   "outputs": [],
   "source": [
    "summary(model_gat, Corpus_.shape, adj.shape, batch_inputs.shape, train_indices_nhop.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "fkjYcTl5WvDL",
    "outputId": "49cd1c67-a1c5-4135-c5cd-0132bf6fc80c"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-7b5ff7260cc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m sparse_gat_1 = SpGAT(num_nodes, entity_in_dim, entity_out_dim_1, relation_dim,\n\u001b[1;32m     11\u001b[0m                                   args.drop_GAT, alpha, nheads_GAT_1)\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_gat_1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_nodes\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mentity_in_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity_out_dim_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelation_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_GAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnheads_GAT_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# batch_size of 2 for batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# print(type(x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# batch_size of 2 for batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# print(type(x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: rand(): argument 'size' must be tuple of ints, but found element of type float at pos 6"
     ]
    }
   ],
   "source": [
    "num_nodes = 40943\n",
    "entity_in_dim = 200\n",
    "entity_out_dim_1 = 200\n",
    "relation_dim = 200\n",
    "drop_GAT = 0.3\n",
    "alpha = 0.2\n",
    "nheads_GAT_1 = 2\n",
    "\n",
    "\n",
    "sparse_gat_1 = SpGAT(num_nodes, entity_in_dim, entity_out_dim_1, relation_dim,\n",
    "                                  args.drop_GAT, alpha, nheads_GAT_1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEBxQs9m4O9v"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(\n",
    "        model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
    "opt_param = optimizer.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "STZvSD6w4dN3",
    "outputId": "9de959db-9337-47fb-dffc-cc56bd71a002"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['params', 'lr', 'betas', 'eps', 'weight_decay', 'amsgrad'])"
      ]
     },
     "execution_count": 117,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_param[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "OVCSXJkn1dSd",
    "outputId": "b0469aa6-0e32-4307-da04-146d745685f7"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-09e6eaed3f67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mCUDA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel_gat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer = torch.optim.Adam(\n\u001b[1;32m      5\u001b[0m     model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_gat' is not defined"
     ]
    }
   ],
   "source": [
    "if CUDA:\n",
    "    model_gat.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=500, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "gat_loss_func = nn.MarginRankingLoss(margin=args.margin)\n",
    "\n",
    "current_batch_2hop_indices = torch.tensor([])\n",
    "if(args.use_2hop):\n",
    "    current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\n",
    "                                                                      Corpus_.unique_entities_train, node_neighbors_2hop)\n",
    "\n",
    "if CUDA:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices)).cuda()\n",
    "else:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices))\n",
    "\n",
    "epoch_losses = []   # losses of all epochs\n",
    "print(\"Number of epochs {}\".format(args.epochs_gat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 244
    },
    "colab_type": "code",
    "id": "jWT0L27O3UL7",
    "outputId": "4e61147a-07ed-4e23-9a03-f2caf48ad308"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-faf325dcff94>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCorpus_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_triples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m Corpus_.train_indices = np.array(\n\u001b[1;32m      3\u001b[0m     list(Corpus_.train_triples)).astype(np.int32)\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Corpus_' is not defined"
     ]
    }
   ],
   "source": [
    "random.shuffle(Corpus_.train_triples)\n",
    "Corpus_.train_indices = np.array(\n",
    "    list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "start_time = time.time()\n",
    "epoch_loss = []\n",
    "\n",
    "if len(Corpus_.train_indices) % args.batch_size_gat == 0:\n",
    "    num_iters_per_epoch = len(\n",
    "        Corpus_.train_indices) // args.batch_size_gat\n",
    "else:\n",
    "    num_iters_per_epoch = (\n",
    "        len(Corpus_.train_indices) // args.batch_size_gat) + 1\n",
    "\n",
    "for iters in range(num_iters_per_epoch):\n",
    "    start_time_iter = time.time()\n",
    "    train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "    if CUDA:\n",
    "        train_indices = Variable(\n",
    "            torch.LongTensor(train_indices)).cuda()\n",
    "        train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "    else:\n",
    "        train_indices = Variable(torch.LongTensor(train_indices))\n",
    "        train_values = Variable(torch.FloatTensor(train_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "6RbJTiTg1Ckj",
    "outputId": "5fa1dc37-a408-4d9d-a0d5-7a0b099c15ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpKBGATModified(\n",
       "  (sparse_gat_1): SpGAT(\n",
       "    (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "    (attention_0): SpGraphAttentionLayer (50 -> 100)\n",
       "    (attention_1): SpGraphAttentionLayer (50 -> 100)\n",
       "    (out_att): SpGraphAttentionLayer (200 -> 200)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# entity_embed, relation_embed = model_gat(\n",
    "#                 Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\n",
    "model_gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "colab_type": "code",
    "id": "zUeedpyeYjz0",
    "outputId": "4eacd5a5-9215-4141-db56-d24271d30c69"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-09093e4ac4d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m entity_embed, relation_embed = model_gat(\n\u001b[0;32m----> 2\u001b[0;31m                 Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_indices' is not defined"
     ]
    }
   ],
   "source": [
    "entity_embed, relation_embed = model_gat(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrkCqFO8WvLi"
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1XZ30kxWvTs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0hMcLCGiar_-",
    "outputId": "e6dd9cc5-8ad1-441d-8b4d-d1681a335025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "\n",
      "Model type -> GAT layer with 2 heads used , Initital Embeddings training\n",
      "length of unique_entities  40559\n",
      "Number of epochs 3600\n"
     ]
    }
   ],
   "source": [
    "# Creating the gat model here.\n",
    "####################################\n",
    "\n",
    "print(\"Defining model\")\n",
    "\n",
    "print(\n",
    "    \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\n",
    "model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                            args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "\n",
    "if CUDA:\n",
    "    model_gat.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=500, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "gat_loss_func = nn.MarginRankingLoss(margin=args.margin)\n",
    "\n",
    "current_batch_2hop_indices = torch.tensor([])\n",
    "if(args.use_2hop):\n",
    "    current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\n",
    "                                                                      Corpus_.unique_entities_train, node_neighbors_2hop)\n",
    "\n",
    "if CUDA:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices)).cuda()\n",
    "else:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices))\n",
    "\n",
    "epoch_losses = []   # losses of all epochs\n",
    "print(\"Number of epochs {}\".format(args.epochs_gat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "et7l0bIAa37y",
    "outputId": "9fe559dd-5570-4229-a99b-9faf0a36d593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch->  0\n",
      "Iteration-> 0  , Iteration_time-> 1.6560 , Iteration_loss 9.7971\n",
      "Epoch 0 , average loss 9.797070503234863 , epoch_time 1.6561825275421143\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  1\n",
      "Iteration-> 0  , Iteration_time-> 1.2222 , Iteration_loss 9.4253\n",
      "Epoch 1 , average loss 9.425337791442871 , epoch_time 1.2224769592285156\n",
      "\n",
      "epoch->  2\n",
      "Iteration-> 0  , Iteration_time-> 1.2487 , Iteration_loss 9.0414\n",
      "Epoch 2 , average loss 9.041407585144043 , epoch_time 1.2488822937011719\n",
      "\n",
      "epoch->  3\n",
      "Iteration-> 0  , Iteration_time-> 1.2173 , Iteration_loss 8.6528\n",
      "Epoch 3 , average loss 8.652764320373535 , epoch_time 1.2175490856170654\n",
      "\n",
      "epoch->  4\n",
      "Iteration-> 0  , Iteration_time-> 1.2242 , Iteration_loss 8.2719\n",
      "Epoch 4 , average loss 8.271947860717773 , epoch_time 1.224367380142212\n",
      "\n",
      "epoch->  5\n",
      "Iteration-> 0  , Iteration_time-> 1.2199 , Iteration_loss 7.9355\n",
      "Epoch 5 , average loss 7.935461521148682 , epoch_time 1.220139980316162\n",
      "\n",
      "epoch->  6\n",
      "Iteration-> 0  , Iteration_time-> 1.2123 , Iteration_loss 7.6180\n",
      "Epoch 6 , average loss 7.618046283721924 , epoch_time 1.2125353813171387\n",
      "\n",
      "epoch->  7\n",
      "Iteration-> 0  , Iteration_time-> 1.2232 , Iteration_loss 7.3481\n",
      "Epoch 7 , average loss 7.348085403442383 , epoch_time 1.2234573364257812\n",
      "\n",
      "epoch->  8\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 7.1338\n",
      "Epoch 8 , average loss 7.133815765380859 , epoch_time 1.2175836563110352\n",
      "\n",
      "epoch->  9\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 6.9518\n",
      "Epoch 9 , average loss 6.951755523681641 , epoch_time 1.207761287689209\n",
      "\n",
      "epoch->  10\n",
      "Iteration-> 0  , Iteration_time-> 1.2060 , Iteration_loss 6.7867\n",
      "Epoch 10 , average loss 6.78674840927124 , epoch_time 1.2062084674835205\n",
      "\n",
      "epoch->  11\n",
      "Iteration-> 0  , Iteration_time-> 1.2038 , Iteration_loss 6.6151\n",
      "Epoch 11 , average loss 6.6151227951049805 , epoch_time 1.2041025161743164\n",
      "\n",
      "epoch->  12\n",
      "Iteration-> 0  , Iteration_time-> 1.2129 , Iteration_loss 6.4675\n",
      "Epoch 12 , average loss 6.467486381530762 , epoch_time 1.213120937347412\n",
      "\n",
      "epoch->  13\n",
      "Iteration-> 0  , Iteration_time-> 1.2066 , Iteration_loss 6.3430\n",
      "Epoch 13 , average loss 6.342957019805908 , epoch_time 1.2068390846252441\n",
      "\n",
      "epoch->  14\n",
      "Iteration-> 0  , Iteration_time-> 1.2086 , Iteration_loss 6.2003\n",
      "Epoch 14 , average loss 6.200253486633301 , epoch_time 1.208909273147583\n",
      "\n",
      "epoch->  15\n",
      "Iteration-> 0  , Iteration_time-> 1.2105 , Iteration_loss 6.0757\n",
      "Epoch 15 , average loss 6.07565450668335 , epoch_time 1.2107152938842773\n",
      "\n",
      "epoch->  16\n",
      "Iteration-> 0  , Iteration_time-> 1.2245 , Iteration_loss 5.9690\n",
      "Epoch 16 , average loss 5.968954563140869 , epoch_time 1.2247836589813232\n",
      "\n",
      "epoch->  17\n",
      "Iteration-> 0  , Iteration_time-> 1.2222 , Iteration_loss 5.8324\n",
      "Epoch 17 , average loss 5.832404136657715 , epoch_time 1.222463607788086\n",
      "\n",
      "epoch->  18\n",
      "Iteration-> 0  , Iteration_time-> 1.2156 , Iteration_loss 5.7211\n",
      "Epoch 18 , average loss 5.721078872680664 , epoch_time 1.2158286571502686\n",
      "\n",
      "epoch->  19\n",
      "Iteration-> 0  , Iteration_time-> 1.2083 , Iteration_loss 5.5980\n",
      "Epoch 19 , average loss 5.59796142578125 , epoch_time 1.2085638046264648\n",
      "\n",
      "epoch->  20\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 5.5105\n",
      "Epoch 20 , average loss 5.510478973388672 , epoch_time 1.210944652557373\n",
      "\n",
      "epoch->  21\n",
      "Iteration-> 0  , Iteration_time-> 1.2059 , Iteration_loss 5.4060\n",
      "Epoch 21 , average loss 5.406010627746582 , epoch_time 1.2060964107513428\n",
      "\n",
      "epoch->  22\n",
      "Iteration-> 0  , Iteration_time-> 1.2136 , Iteration_loss 5.3171\n",
      "Epoch 22 , average loss 5.317124843597412 , epoch_time 1.2137658596038818\n",
      "\n",
      "epoch->  23\n",
      "Iteration-> 0  , Iteration_time-> 1.2182 , Iteration_loss 5.2119\n",
      "Epoch 23 , average loss 5.211884021759033 , epoch_time 1.218358039855957\n",
      "\n",
      "epoch->  24\n",
      "Iteration-> 0  , Iteration_time-> 1.2265 , Iteration_loss 5.1300\n",
      "Epoch 24 , average loss 5.1300458908081055 , epoch_time 1.2266554832458496\n",
      "\n",
      "epoch->  25\n",
      "Iteration-> 0  , Iteration_time-> 1.2117 , Iteration_loss 5.0325\n",
      "Epoch 25 , average loss 5.032534122467041 , epoch_time 1.21189284324646\n",
      "\n",
      "epoch->  26\n",
      "Iteration-> 0  , Iteration_time-> 1.2205 , Iteration_loss 4.9477\n",
      "Epoch 26 , average loss 4.947713375091553 , epoch_time 1.220729112625122\n",
      "\n",
      "epoch->  27\n",
      "Iteration-> 0  , Iteration_time-> 1.2241 , Iteration_loss 4.8909\n",
      "Epoch 27 , average loss 4.890949249267578 , epoch_time 1.2243413925170898\n",
      "\n",
      "epoch->  28\n",
      "Iteration-> 0  , Iteration_time-> 1.2339 , Iteration_loss 4.7954\n",
      "Epoch 28 , average loss 4.795355319976807 , epoch_time 1.2341184616088867\n",
      "\n",
      "epoch->  29\n",
      "Iteration-> 0  , Iteration_time-> 1.2097 , Iteration_loss 4.7301\n",
      "Epoch 29 , average loss 4.730057239532471 , epoch_time 1.2099313735961914\n",
      "\n",
      "epoch->  30\n",
      "Iteration-> 0  , Iteration_time-> 1.2259 , Iteration_loss 4.6670\n",
      "Epoch 30 , average loss 4.667011737823486 , epoch_time 1.2261316776275635\n",
      "\n",
      "epoch->  31\n",
      "Iteration-> 0  , Iteration_time-> 1.2261 , Iteration_loss 4.5948\n",
      "Epoch 31 , average loss 4.594799995422363 , epoch_time 1.22636079788208\n",
      "\n",
      "epoch->  32\n",
      "Iteration-> 0  , Iteration_time-> 1.2040 , Iteration_loss 4.5441\n",
      "Epoch 32 , average loss 4.544144630432129 , epoch_time 1.204211711883545\n",
      "\n",
      "epoch->  33\n",
      "Iteration-> 0  , Iteration_time-> 1.2089 , Iteration_loss 4.4778\n",
      "Epoch 33 , average loss 4.477794170379639 , epoch_time 1.209123134613037\n",
      "\n",
      "epoch->  34\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 4.4322\n",
      "Epoch 34 , average loss 4.432223320007324 , epoch_time 1.2107949256896973\n",
      "\n",
      "epoch->  35\n",
      "Iteration-> 0  , Iteration_time-> 1.2082 , Iteration_loss 4.3687\n",
      "Epoch 35 , average loss 4.368653297424316 , epoch_time 1.2084050178527832\n",
      "\n",
      "epoch->  36\n",
      "Iteration-> 0  , Iteration_time-> 1.2239 , Iteration_loss 4.3125\n",
      "Epoch 36 , average loss 4.312453746795654 , epoch_time 1.2241413593292236\n",
      "\n",
      "epoch->  37\n",
      "Iteration-> 0  , Iteration_time-> 1.2061 , Iteration_loss 4.2689\n",
      "Epoch 37 , average loss 4.268913745880127 , epoch_time 1.206317663192749\n",
      "\n",
      "epoch->  38\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 4.2164\n",
      "Epoch 38 , average loss 4.21642541885376 , epoch_time 1.2108874320983887\n",
      "\n",
      "epoch->  39\n",
      "Iteration-> 0  , Iteration_time-> 1.2306 , Iteration_loss 4.1547\n",
      "Epoch 39 , average loss 4.154722213745117 , epoch_time 1.230872631072998\n",
      "\n",
      "epoch->  40\n",
      "Iteration-> 0  , Iteration_time-> 1.2142 , Iteration_loss 4.1273\n",
      "Epoch 40 , average loss 4.127281665802002 , epoch_time 1.214385747909546\n",
      "\n",
      "epoch->  41\n",
      "Iteration-> 0  , Iteration_time-> 1.2043 , Iteration_loss 4.0782\n",
      "Epoch 41 , average loss 4.078161716461182 , epoch_time 1.2044410705566406\n",
      "\n",
      "epoch->  42\n",
      "Iteration-> 0  , Iteration_time-> 1.2204 , Iteration_loss 4.0336\n",
      "Epoch 42 , average loss 4.033588409423828 , epoch_time 1.2206602096557617\n",
      "\n",
      "epoch->  43\n",
      "Iteration-> 0  , Iteration_time-> 1.2165 , Iteration_loss 3.9988\n",
      "Epoch 43 , average loss 3.9988362789154053 , epoch_time 1.2167508602142334\n",
      "\n",
      "epoch->  44\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 3.9576\n",
      "Epoch 44 , average loss 3.9576401710510254 , epoch_time 1.2109284400939941\n",
      "\n",
      "epoch->  45\n",
      "Iteration-> 0  , Iteration_time-> 1.2186 , Iteration_loss 3.9084\n",
      "Epoch 45 , average loss 3.9083991050720215 , epoch_time 1.2188286781311035\n",
      "\n",
      "epoch->  46\n",
      "Iteration-> 0  , Iteration_time-> 1.2067 , Iteration_loss 3.8793\n",
      "Epoch 46 , average loss 3.8793036937713623 , epoch_time 1.2068772315979004\n",
      "\n",
      "epoch->  47\n",
      "Iteration-> 0  , Iteration_time-> 1.2081 , Iteration_loss 3.8525\n",
      "Epoch 47 , average loss 3.852468729019165 , epoch_time 1.2083179950714111\n",
      "\n",
      "epoch->  48\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 3.8028\n",
      "Epoch 48 , average loss 3.8028225898742676 , epoch_time 1.2106733322143555\n",
      "\n",
      "epoch->  49\n",
      "Iteration-> 0  , Iteration_time-> 1.2243 , Iteration_loss 3.7708\n",
      "Epoch 49 , average loss 3.7708399295806885 , epoch_time 1.2245523929595947\n",
      "\n",
      "epoch->  50\n",
      "Iteration-> 0  , Iteration_time-> 1.2100 , Iteration_loss 3.7305\n",
      "Epoch 50 , average loss 3.7304835319519043 , epoch_time 1.2102196216583252\n",
      "\n",
      "epoch->  51\n",
      "Iteration-> 0  , Iteration_time-> 1.2057 , Iteration_loss 3.7063\n",
      "Epoch 51 , average loss 3.706298589706421 , epoch_time 1.2059485912322998\n",
      "\n",
      "epoch->  52\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 3.6805\n",
      "Epoch 52 , average loss 3.680532693862915 , epoch_time 1.2074966430664062\n",
      "\n",
      "epoch->  53\n",
      "Iteration-> 0  , Iteration_time-> 1.2259 , Iteration_loss 3.6530\n",
      "Epoch 53 , average loss 3.6530497074127197 , epoch_time 1.2261419296264648\n",
      "\n",
      "epoch->  54\n",
      "Iteration-> 0  , Iteration_time-> 1.2237 , Iteration_loss 3.6248\n",
      "Epoch 54 , average loss 3.6248178482055664 , epoch_time 1.223853588104248\n",
      "\n",
      "epoch->  55\n",
      "Iteration-> 0  , Iteration_time-> 1.2034 , Iteration_loss 3.5819\n",
      "Epoch 55 , average loss 3.581871509552002 , epoch_time 1.203596591949463\n",
      "\n",
      "epoch->  56\n",
      "Iteration-> 0  , Iteration_time-> 1.2112 , Iteration_loss 3.5559\n",
      "Epoch 56 , average loss 3.5558581352233887 , epoch_time 1.2114453315734863\n",
      "\n",
      "epoch->  57\n",
      "Iteration-> 0  , Iteration_time-> 1.2081 , Iteration_loss 3.5355\n",
      "Epoch 57 , average loss 3.535529375076294 , epoch_time 1.2082910537719727\n",
      "\n",
      "epoch->  58\n",
      "Iteration-> 0  , Iteration_time-> 1.2160 , Iteration_loss 3.5023\n",
      "Epoch 58 , average loss 3.5022521018981934 , epoch_time 1.2161877155303955\n",
      "\n",
      "epoch->  59\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 3.4731\n",
      "Epoch 59 , average loss 3.473106622695923 , epoch_time 1.2076964378356934\n",
      "\n",
      "epoch->  60\n",
      "Iteration-> 0  , Iteration_time-> 1.2115 , Iteration_loss 3.4530\n",
      "Epoch 60 , average loss 3.4530341625213623 , epoch_time 1.2117702960968018\n",
      "\n",
      "epoch->  61\n",
      "Iteration-> 0  , Iteration_time-> 1.2119 , Iteration_loss 3.4321\n",
      "Epoch 61 , average loss 3.4321136474609375 , epoch_time 1.2121303081512451\n",
      "\n",
      "epoch->  62\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 3.3960\n",
      "Epoch 62 , average loss 3.3960068225860596 , epoch_time 1.2169229984283447\n",
      "\n",
      "epoch->  63\n",
      "Iteration-> 0  , Iteration_time-> 1.2282 , Iteration_loss 3.3581\n",
      "Epoch 63 , average loss 3.358102321624756 , epoch_time 1.2284231185913086\n",
      "\n",
      "epoch->  64\n",
      "Iteration-> 0  , Iteration_time-> 1.2207 , Iteration_loss 3.3386\n",
      "Epoch 64 , average loss 3.3385989665985107 , epoch_time 1.220914363861084\n",
      "\n",
      "epoch->  65\n",
      "Iteration-> 0  , Iteration_time-> 1.2255 , Iteration_loss 3.3049\n",
      "Epoch 65 , average loss 3.3048784732818604 , epoch_time 1.2257328033447266\n",
      "\n",
      "epoch->  66\n",
      "Iteration-> 0  , Iteration_time-> 1.2212 , Iteration_loss 3.2797\n",
      "Epoch 66 , average loss 3.279653310775757 , epoch_time 1.221428632736206\n",
      "\n",
      "epoch->  67\n",
      "Iteration-> 0  , Iteration_time-> 1.2153 , Iteration_loss 3.2582\n",
      "Epoch 67 , average loss 3.258209466934204 , epoch_time 1.2155060768127441\n",
      "\n",
      "epoch->  68\n",
      "Iteration-> 0  , Iteration_time-> 1.2127 , Iteration_loss 3.2365\n",
      "Epoch 68 , average loss 3.2365009784698486 , epoch_time 1.2129154205322266\n",
      "\n",
      "epoch->  69\n",
      "Iteration-> 0  , Iteration_time-> 1.2104 , Iteration_loss 3.2024\n",
      "Epoch 69 , average loss 3.2023630142211914 , epoch_time 1.210662603378296\n",
      "\n",
      "epoch->  70\n",
      "Iteration-> 0  , Iteration_time-> 1.2282 , Iteration_loss 3.1789\n",
      "Epoch 70 , average loss 3.178863048553467 , epoch_time 1.2284460067749023\n",
      "\n",
      "epoch->  71\n",
      "Iteration-> 0  , Iteration_time-> 1.2308 , Iteration_loss 3.1498\n",
      "Epoch 71 , average loss 3.149822235107422 , epoch_time 1.2310664653778076\n",
      "\n",
      "epoch->  72\n",
      "Iteration-> 0  , Iteration_time-> 1.2209 , Iteration_loss 3.1268\n",
      "Epoch 72 , average loss 3.1268486976623535 , epoch_time 1.2210874557495117\n",
      "\n",
      "epoch->  73\n",
      "Iteration-> 0  , Iteration_time-> 1.2185 , Iteration_loss 3.1101\n",
      "Epoch 73 , average loss 3.1100540161132812 , epoch_time 1.21870756149292\n",
      "\n",
      "epoch->  74\n",
      "Iteration-> 0  , Iteration_time-> 1.2243 , Iteration_loss 3.0785\n",
      "Epoch 74 , average loss 3.078479051589966 , epoch_time 1.2245204448699951\n",
      "\n",
      "epoch->  75\n",
      "Iteration-> 0  , Iteration_time-> 1.2092 , Iteration_loss 3.0568\n",
      "Epoch 75 , average loss 3.0568161010742188 , epoch_time 1.209428310394287\n",
      "\n",
      "epoch->  76\n",
      "Iteration-> 0  , Iteration_time-> 1.2169 , Iteration_loss 3.0385\n",
      "Epoch 76 , average loss 3.038536787033081 , epoch_time 1.217099905014038\n",
      "\n",
      "epoch->  77\n",
      "Iteration-> 0  , Iteration_time-> 1.2227 , Iteration_loss 3.0066\n",
      "Epoch 77 , average loss 3.006619691848755 , epoch_time 1.2228844165802002\n",
      "\n",
      "epoch->  78\n",
      "Iteration-> 0  , Iteration_time-> 1.2222 , Iteration_loss 2.9875\n",
      "Epoch 78 , average loss 2.9874908924102783 , epoch_time 1.2223868370056152\n",
      "\n",
      "epoch->  79\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 2.9612\n",
      "Epoch 79 , average loss 2.961238384246826 , epoch_time 1.2169396877288818\n",
      "\n",
      "epoch->  80\n",
      "Iteration-> 0  , Iteration_time-> 1.2177 , Iteration_loss 2.9443\n",
      "Epoch 80 , average loss 2.9443280696868896 , epoch_time 1.2179481983184814\n",
      "\n",
      "epoch->  81\n",
      "Iteration-> 0  , Iteration_time-> 1.2074 , Iteration_loss 2.9202\n",
      "Epoch 81 , average loss 2.9202146530151367 , epoch_time 1.207592487335205\n",
      "\n",
      "epoch->  82\n",
      "Iteration-> 0  , Iteration_time-> 1.2131 , Iteration_loss 2.9071\n",
      "Epoch 82 , average loss 2.907106399536133 , epoch_time 1.2132997512817383\n",
      "\n",
      "epoch->  83\n",
      "Iteration-> 0  , Iteration_time-> 1.2221 , Iteration_loss 2.8762\n",
      "Epoch 83 , average loss 2.876174211502075 , epoch_time 1.2223434448242188\n",
      "\n",
      "epoch->  84\n",
      "Iteration-> 0  , Iteration_time-> 1.2230 , Iteration_loss 2.8587\n",
      "Epoch 84 , average loss 2.8587050437927246 , epoch_time 1.2231948375701904\n",
      "\n",
      "epoch->  85\n",
      "Iteration-> 0  , Iteration_time-> 1.2221 , Iteration_loss 2.8540\n",
      "Epoch 85 , average loss 2.854005813598633 , epoch_time 1.222259283065796\n",
      "\n",
      "epoch->  86\n",
      "Iteration-> 0  , Iteration_time-> 1.2150 , Iteration_loss 2.8289\n",
      "Epoch 86 , average loss 2.828937530517578 , epoch_time 1.2151832580566406\n",
      "\n",
      "epoch->  87\n",
      "Iteration-> 0  , Iteration_time-> 1.2223 , Iteration_loss 2.8213\n",
      "Epoch 87 , average loss 2.821279525756836 , epoch_time 1.2225656509399414\n",
      "\n",
      "epoch->  88\n",
      "Iteration-> 0  , Iteration_time-> 1.2200 , Iteration_loss 2.7981\n",
      "Epoch 88 , average loss 2.798144578933716 , epoch_time 1.2201805114746094\n",
      "\n",
      "epoch->  89\n",
      "Iteration-> 0  , Iteration_time-> 1.2192 , Iteration_loss 2.7892\n",
      "Epoch 89 , average loss 2.7892110347747803 , epoch_time 1.2193970680236816\n",
      "\n",
      "epoch->  90\n",
      "Iteration-> 0  , Iteration_time-> 1.2096 , Iteration_loss 2.7588\n",
      "Epoch 90 , average loss 2.7587509155273438 , epoch_time 1.2098150253295898\n",
      "\n",
      "epoch->  91\n",
      "Iteration-> 0  , Iteration_time-> 1.2103 , Iteration_loss 2.7380\n",
      "Epoch 91 , average loss 2.7379589080810547 , epoch_time 1.2104980945587158\n",
      "\n",
      "epoch->  92\n",
      "Iteration-> 0  , Iteration_time-> 1.2095 , Iteration_loss 2.7283\n",
      "Epoch 92 , average loss 2.7282845973968506 , epoch_time 1.2096941471099854\n",
      "\n",
      "epoch->  93\n",
      "Iteration-> 0  , Iteration_time-> 1.2442 , Iteration_loss 2.7159\n",
      "Epoch 93 , average loss 2.7158613204956055 , epoch_time 1.244431972503662\n",
      "\n",
      "epoch->  94\n",
      "Iteration-> 0  , Iteration_time-> 1.2229 , Iteration_loss 2.7005\n",
      "Epoch 94 , average loss 2.7005133628845215 , epoch_time 1.223142385482788\n",
      "\n",
      "epoch->  95\n",
      "Iteration-> 0  , Iteration_time-> 1.2078 , Iteration_loss 2.6852\n",
      "Epoch 95 , average loss 2.685185194015503 , epoch_time 1.2080368995666504\n",
      "\n",
      "epoch->  96\n",
      "Iteration-> 0  , Iteration_time-> 1.2828 , Iteration_loss 2.6642\n",
      "Epoch 96 , average loss 2.6642439365386963 , epoch_time 1.2830281257629395\n",
      "\n",
      "epoch->  97\n",
      "Iteration-> 0  , Iteration_time-> 1.2183 , Iteration_loss 2.6620\n",
      "Epoch 97 , average loss 2.6620121002197266 , epoch_time 1.2185652256011963\n",
      "\n",
      "epoch->  98\n",
      "Iteration-> 0  , Iteration_time-> 1.2205 , Iteration_loss 2.6379\n",
      "Epoch 98 , average loss 2.6379220485687256 , epoch_time 1.2207412719726562\n",
      "\n",
      "epoch->  99\n",
      "Iteration-> 0  , Iteration_time-> 1.2291 , Iteration_loss 2.6326\n",
      "Epoch 99 , average loss 2.6325623989105225 , epoch_time 1.2293736934661865\n",
      "\n",
      "epoch->  100\n",
      "Iteration-> 0  , Iteration_time-> 1.2385 , Iteration_loss 2.6103\n",
      "Epoch 100 , average loss 2.61028790473938 , epoch_time 1.2388296127319336\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  101\n",
      "Iteration-> 0  , Iteration_time-> 1.2306 , Iteration_loss 2.6050\n",
      "Epoch 101 , average loss 2.6050281524658203 , epoch_time 1.230818271636963\n",
      "\n",
      "epoch->  102\n",
      "Iteration-> 0  , Iteration_time-> 1.2336 , Iteration_loss 2.5945\n",
      "Epoch 102 , average loss 2.5945029258728027 , epoch_time 1.2338545322418213\n",
      "\n",
      "epoch->  103\n",
      "Iteration-> 0  , Iteration_time-> 1.2293 , Iteration_loss 2.5743\n",
      "Epoch 103 , average loss 2.5743443965911865 , epoch_time 1.2295198440551758\n",
      "\n",
      "epoch->  104\n",
      "Iteration-> 0  , Iteration_time-> 1.2235 , Iteration_loss 2.5642\n",
      "Epoch 104 , average loss 2.564239025115967 , epoch_time 1.2236878871917725\n",
      "\n",
      "epoch->  105\n",
      "Iteration-> 0  , Iteration_time-> 1.2152 , Iteration_loss 2.5546\n",
      "Epoch 105 , average loss 2.5546092987060547 , epoch_time 1.2154302597045898\n",
      "\n",
      "epoch->  106\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 2.5320\n",
      "Epoch 106 , average loss 2.531999349594116 , epoch_time 1.2118263244628906\n",
      "\n",
      "epoch->  107\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 2.5242\n",
      "Epoch 107 , average loss 2.524198532104492 , epoch_time 1.217695951461792\n",
      "\n",
      "epoch->  108\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 2.5203\n",
      "Epoch 108 , average loss 2.5203404426574707 , epoch_time 1.217611312866211\n",
      "\n",
      "epoch->  109\n",
      "Iteration-> 0  , Iteration_time-> 1.2055 , Iteration_loss 2.5012\n",
      "Epoch 109 , average loss 2.501164197921753 , epoch_time 1.2057201862335205\n",
      "\n",
      "epoch->  110\n",
      "Iteration-> 0  , Iteration_time-> 1.2117 , Iteration_loss 2.4915\n",
      "Epoch 110 , average loss 2.49153995513916 , epoch_time 1.2119359970092773\n",
      "\n",
      "epoch->  111\n",
      "Iteration-> 0  , Iteration_time-> 1.2099 , Iteration_loss 2.4856\n",
      "Epoch 111 , average loss 2.485605478286743 , epoch_time 1.210118293762207\n",
      "\n",
      "epoch->  112\n",
      "Iteration-> 0  , Iteration_time-> 1.2152 , Iteration_loss 2.4690\n",
      "Epoch 112 , average loss 2.4689528942108154 , epoch_time 1.2154178619384766\n",
      "\n",
      "epoch->  113\n",
      "Iteration-> 0  , Iteration_time-> 1.2146 , Iteration_loss 2.4566\n",
      "Epoch 113 , average loss 2.456618070602417 , epoch_time 1.2148017883300781\n",
      "\n",
      "epoch->  114\n",
      "Iteration-> 0  , Iteration_time-> 1.2371 , Iteration_loss 2.4502\n",
      "Epoch 114 , average loss 2.450197219848633 , epoch_time 1.2373261451721191\n",
      "\n",
      "epoch->  115\n",
      "Iteration-> 0  , Iteration_time-> 1.2497 , Iteration_loss 2.4419\n",
      "Epoch 115 , average loss 2.4419102668762207 , epoch_time 1.2498764991760254\n",
      "\n",
      "epoch->  116\n",
      "Iteration-> 0  , Iteration_time-> 1.2160 , Iteration_loss 2.4245\n",
      "Epoch 116 , average loss 2.424459934234619 , epoch_time 1.2162106037139893\n",
      "\n",
      "epoch->  117\n",
      "Iteration-> 0  , Iteration_time-> 1.2212 , Iteration_loss 2.4209\n",
      "Epoch 117 , average loss 2.4208505153656006 , epoch_time 1.2213952541351318\n",
      "\n",
      "epoch->  118\n",
      "Iteration-> 0  , Iteration_time-> 1.2172 , Iteration_loss 2.4045\n",
      "Epoch 118 , average loss 2.4044675827026367 , epoch_time 1.2173678874969482\n",
      "\n",
      "epoch->  119\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 2.4010\n",
      "Epoch 119 , average loss 2.401026487350464 , epoch_time 1.2155845165252686\n",
      "\n",
      "epoch->  120\n",
      "Iteration-> 0  , Iteration_time-> 1.2098 , Iteration_loss 2.3931\n",
      "Epoch 120 , average loss 2.3930835723876953 , epoch_time 1.2100181579589844\n",
      "\n",
      "epoch->  121\n",
      "Iteration-> 0  , Iteration_time-> 1.2110 , Iteration_loss 2.3813\n",
      "Epoch 121 , average loss 2.3813014030456543 , epoch_time 1.211216926574707\n",
      "\n",
      "epoch->  122\n",
      "Iteration-> 0  , Iteration_time-> 1.2394 , Iteration_loss 2.3643\n",
      "Epoch 122 , average loss 2.3642702102661133 , epoch_time 1.239609718322754\n",
      "\n",
      "epoch->  123\n",
      "Iteration-> 0  , Iteration_time-> 1.2282 , Iteration_loss 2.3531\n",
      "Epoch 123 , average loss 2.3530502319335938 , epoch_time 1.228440523147583\n",
      "\n",
      "epoch->  124\n",
      "Iteration-> 0  , Iteration_time-> 1.2292 , Iteration_loss 2.3551\n",
      "Epoch 124 , average loss 2.3551175594329834 , epoch_time 1.229475498199463\n",
      "\n",
      "epoch->  125\n",
      "Iteration-> 0  , Iteration_time-> 1.2294 , Iteration_loss 2.3529\n",
      "Epoch 125 , average loss 2.352851390838623 , epoch_time 1.229576587677002\n",
      "\n",
      "epoch->  126\n",
      "Iteration-> 0  , Iteration_time-> 1.2437 , Iteration_loss 2.3367\n",
      "Epoch 126 , average loss 2.336731195449829 , epoch_time 1.243966817855835\n",
      "\n",
      "epoch->  127\n",
      "Iteration-> 0  , Iteration_time-> 1.2223 , Iteration_loss 2.3296\n",
      "Epoch 127 , average loss 2.329554557800293 , epoch_time 1.2225286960601807\n",
      "\n",
      "epoch->  128\n",
      "Iteration-> 0  , Iteration_time-> 1.2298 , Iteration_loss 2.3240\n",
      "Epoch 128 , average loss 2.3240435123443604 , epoch_time 1.2300465106964111\n",
      "\n",
      "epoch->  129\n",
      "Iteration-> 0  , Iteration_time-> 1.2163 , Iteration_loss 2.3116\n",
      "Epoch 129 , average loss 2.311612367630005 , epoch_time 1.2165277004241943\n",
      "\n",
      "epoch->  130\n",
      "Iteration-> 0  , Iteration_time-> 1.2072 , Iteration_loss 2.3066\n",
      "Epoch 130 , average loss 2.306567668914795 , epoch_time 1.207364797592163\n",
      "\n",
      "epoch->  131\n",
      "Iteration-> 0  , Iteration_time-> 1.2166 , Iteration_loss 2.2984\n",
      "Epoch 131 , average loss 2.2984421253204346 , epoch_time 1.2167754173278809\n",
      "\n",
      "epoch->  132\n",
      "Iteration-> 0  , Iteration_time-> 1.2149 , Iteration_loss 2.2790\n",
      "Epoch 132 , average loss 2.2790439128875732 , epoch_time 1.2154748439788818\n",
      "\n",
      "epoch->  133\n",
      "Iteration-> 0  , Iteration_time-> 1.2121 , Iteration_loss 2.2809\n",
      "Epoch 133 , average loss 2.2808799743652344 , epoch_time 1.2122783660888672\n",
      "\n",
      "epoch->  134\n",
      "Iteration-> 0  , Iteration_time-> 1.2137 , Iteration_loss 2.2760\n",
      "Epoch 134 , average loss 2.276029348373413 , epoch_time 1.2138879299163818\n",
      "\n",
      "epoch->  135\n",
      "Iteration-> 0  , Iteration_time-> 1.2136 , Iteration_loss 2.2580\n",
      "Epoch 135 , average loss 2.257982015609741 , epoch_time 1.2138400077819824\n",
      "\n",
      "epoch->  136\n",
      "Iteration-> 0  , Iteration_time-> 1.2254 , Iteration_loss 2.2588\n",
      "Epoch 136 , average loss 2.25877046585083 , epoch_time 1.2256660461425781\n",
      "\n",
      "epoch->  137\n",
      "Iteration-> 0  , Iteration_time-> 1.2324 , Iteration_loss 2.2554\n",
      "Epoch 137 , average loss 2.2554197311401367 , epoch_time 1.2325923442840576\n",
      "\n",
      "epoch->  138\n",
      "Iteration-> 0  , Iteration_time-> 1.2640 , Iteration_loss 2.2475\n",
      "Epoch 138 , average loss 2.2475149631500244 , epoch_time 1.2642457485198975\n",
      "\n",
      "epoch->  139\n",
      "Iteration-> 0  , Iteration_time-> 1.2214 , Iteration_loss 2.2312\n",
      "Epoch 139 , average loss 2.2312467098236084 , epoch_time 1.2215754985809326\n",
      "\n",
      "epoch->  140\n",
      "Iteration-> 0  , Iteration_time-> 1.2129 , Iteration_loss 2.2327\n",
      "Epoch 140 , average loss 2.23266863822937 , epoch_time 1.2131116390228271\n",
      "\n",
      "epoch->  141\n",
      "Iteration-> 0  , Iteration_time-> 1.2211 , Iteration_loss 2.2287\n",
      "Epoch 141 , average loss 2.2286951541900635 , epoch_time 1.2212998867034912\n",
      "\n",
      "epoch->  142\n",
      "Iteration-> 0  , Iteration_time-> 1.2219 , Iteration_loss 2.2199\n",
      "Epoch 142 , average loss 2.219851016998291 , epoch_time 1.2220754623413086\n",
      "\n",
      "epoch->  143\n",
      "Iteration-> 0  , Iteration_time-> 1.2161 , Iteration_loss 2.2076\n",
      "Epoch 143 , average loss 2.207552671432495 , epoch_time 1.2163102626800537\n",
      "\n",
      "epoch->  144\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 2.1926\n",
      "Epoch 144 , average loss 2.192625045776367 , epoch_time 1.2169029712677002\n",
      "\n",
      "epoch->  145\n",
      "Iteration-> 0  , Iteration_time-> 1.2138 , Iteration_loss 2.2017\n",
      "Epoch 145 , average loss 2.2017343044281006 , epoch_time 1.2139732837677002\n",
      "\n",
      "epoch->  146\n",
      "Iteration-> 0  , Iteration_time-> 1.2094 , Iteration_loss 2.1829\n",
      "Epoch 146 , average loss 2.1828815937042236 , epoch_time 1.2095866203308105\n",
      "\n",
      "epoch->  147\n",
      "Iteration-> 0  , Iteration_time-> 1.2082 , Iteration_loss 2.1866\n",
      "Epoch 147 , average loss 2.1866390705108643 , epoch_time 1.208409070968628\n",
      "\n",
      "epoch->  148\n",
      "Iteration-> 0  , Iteration_time-> 1.2210 , Iteration_loss 2.1730\n",
      "Epoch 148 , average loss 2.173022508621216 , epoch_time 1.2212269306182861\n",
      "\n",
      "epoch->  149\n",
      "Iteration-> 0  , Iteration_time-> 1.2119 , Iteration_loss 2.1817\n",
      "Epoch 149 , average loss 2.1816701889038086 , epoch_time 1.2121505737304688\n",
      "\n",
      "epoch->  150\n",
      "Iteration-> 0  , Iteration_time-> 1.2119 , Iteration_loss 2.1676\n",
      "Epoch 150 , average loss 2.1675944328308105 , epoch_time 1.2121672630310059\n",
      "\n",
      "epoch->  151\n",
      "Iteration-> 0  , Iteration_time-> 1.2176 , Iteration_loss 2.1637\n",
      "Epoch 151 , average loss 2.163651943206787 , epoch_time 1.2178034782409668\n",
      "\n",
      "epoch->  152\n",
      "Iteration-> 0  , Iteration_time-> 1.2225 , Iteration_loss 2.1612\n",
      "Epoch 152 , average loss 2.161186456680298 , epoch_time 1.222722053527832\n",
      "\n",
      "epoch->  153\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 2.1545\n",
      "Epoch 153 , average loss 2.1545186042785645 , epoch_time 1.2155802249908447\n",
      "\n",
      "epoch->  154\n",
      "Iteration-> 0  , Iteration_time-> 1.2068 , Iteration_loss 2.1451\n",
      "Epoch 154 , average loss 2.145054817199707 , epoch_time 1.2070438861846924\n",
      "\n",
      "epoch->  155\n",
      "Iteration-> 0  , Iteration_time-> 1.2097 , Iteration_loss 2.1456\n",
      "Epoch 155 , average loss 2.145634889602661 , epoch_time 1.2099800109863281\n",
      "\n",
      "epoch->  156\n",
      "Iteration-> 0  , Iteration_time-> 1.2108 , Iteration_loss 2.1364\n",
      "Epoch 156 , average loss 2.1364047527313232 , epoch_time 1.2110309600830078\n",
      "\n",
      "epoch->  157\n",
      "Iteration-> 0  , Iteration_time-> 1.2099 , Iteration_loss 2.1268\n",
      "Epoch 157 , average loss 2.1268296241760254 , epoch_time 1.2101311683654785\n",
      "\n",
      "epoch->  158\n",
      "Iteration-> 0  , Iteration_time-> 1.2051 , Iteration_loss 2.1262\n",
      "Epoch 158 , average loss 2.126166820526123 , epoch_time 1.2053287029266357\n",
      "\n",
      "epoch->  159\n",
      "Iteration-> 0  , Iteration_time-> 1.2142 , Iteration_loss 2.1220\n",
      "Epoch 159 , average loss 2.1219866275787354 , epoch_time 1.2144136428833008\n",
      "\n",
      "epoch->  160\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 2.1143\n",
      "Epoch 160 , average loss 2.1143417358398438 , epoch_time 1.211812973022461\n",
      "\n",
      "epoch->  161\n",
      "Iteration-> 0  , Iteration_time-> 1.2091 , Iteration_loss 2.1155\n",
      "Epoch 161 , average loss 2.115527629852295 , epoch_time 1.2093255519866943\n",
      "\n",
      "epoch->  162\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 2.1003\n",
      "Epoch 162 , average loss 2.10025691986084 , epoch_time 1.2075371742248535\n",
      "\n",
      "epoch->  163\n",
      "Iteration-> 0  , Iteration_time-> 1.2121 , Iteration_loss 2.0938\n",
      "Epoch 163 , average loss 2.093848705291748 , epoch_time 1.2123425006866455\n",
      "\n",
      "epoch->  164\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 2.0945\n",
      "Epoch 164 , average loss 2.0944929122924805 , epoch_time 1.211827278137207\n",
      "\n",
      "epoch->  165\n",
      "Iteration-> 0  , Iteration_time-> 1.2111 , Iteration_loss 2.0909\n",
      "Epoch 165 , average loss 2.090879440307617 , epoch_time 1.2113163471221924\n",
      "\n",
      "epoch->  166\n",
      "Iteration-> 0  , Iteration_time-> 1.2124 , Iteration_loss 2.0771\n",
      "Epoch 166 , average loss 2.0771453380584717 , epoch_time 1.212597370147705\n",
      "\n",
      "epoch->  167\n",
      "Iteration-> 0  , Iteration_time-> 1.2012 , Iteration_loss 2.0797\n",
      "Epoch 167 , average loss 2.079737424850464 , epoch_time 1.2014532089233398\n",
      "\n",
      "epoch->  168\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 2.0677\n",
      "Epoch 168 , average loss 2.0677196979522705 , epoch_time 1.2077374458312988\n",
      "\n",
      "epoch->  169\n",
      "Iteration-> 0  , Iteration_time-> 1.2416 , Iteration_loss 2.0697\n",
      "Epoch 169 , average loss 2.069669246673584 , epoch_time 1.2418558597564697\n",
      "\n",
      "epoch->  170\n",
      "Iteration-> 0  , Iteration_time-> 1.2345 , Iteration_loss 2.0662\n",
      "Epoch 170 , average loss 2.0661604404449463 , epoch_time 1.2347455024719238\n",
      "\n",
      "epoch->  171\n",
      "Iteration-> 0  , Iteration_time-> 1.2316 , Iteration_loss 2.0689\n",
      "Epoch 171 , average loss 2.068859338760376 , epoch_time 1.231769323348999\n",
      "\n",
      "epoch->  172\n",
      "Iteration-> 0  , Iteration_time-> 1.2321 , Iteration_loss 2.0616\n",
      "Epoch 172 , average loss 2.061624765396118 , epoch_time 1.2323169708251953\n",
      "\n",
      "epoch->  173\n",
      "Iteration-> 0  , Iteration_time-> 1.2235 , Iteration_loss 2.0528\n",
      "Epoch 173 , average loss 2.0528080463409424 , epoch_time 1.223750352859497\n",
      "\n",
      "epoch->  174\n",
      "Iteration-> 0  , Iteration_time-> 1.2351 , Iteration_loss 2.0564\n",
      "Epoch 174 , average loss 2.056428909301758 , epoch_time 1.2352547645568848\n",
      "\n",
      "epoch->  175\n",
      "Iteration-> 0  , Iteration_time-> 1.2141 , Iteration_loss 2.0537\n",
      "Epoch 175 , average loss 2.053687572479248 , epoch_time 1.2142822742462158\n",
      "\n",
      "epoch->  176\n",
      "Iteration-> 0  , Iteration_time-> 1.2210 , Iteration_loss 2.0411\n",
      "Epoch 176 , average loss 2.0411362648010254 , epoch_time 1.221261739730835\n",
      "\n",
      "epoch->  177\n",
      "Iteration-> 0  , Iteration_time-> 1.2095 , Iteration_loss 2.0310\n",
      "Epoch 177 , average loss 2.0309979915618896 , epoch_time 1.2097389698028564\n",
      "\n",
      "epoch->  178\n",
      "Iteration-> 0  , Iteration_time-> 1.2381 , Iteration_loss 2.0330\n",
      "Epoch 178 , average loss 2.03300142288208 , epoch_time 1.2383170127868652\n",
      "\n",
      "epoch->  179\n",
      "Iteration-> 0  , Iteration_time-> 1.2259 , Iteration_loss 2.0330\n",
      "Epoch 179 , average loss 2.0330443382263184 , epoch_time 1.226142406463623\n",
      "\n",
      "epoch->  180\n",
      "Iteration-> 0  , Iteration_time-> 1.2323 , Iteration_loss 2.0284\n",
      "Epoch 180 , average loss 2.0283663272857666 , epoch_time 1.2333729267120361\n",
      "\n",
      "epoch->  181\n",
      "Iteration-> 0  , Iteration_time-> 1.2146 , Iteration_loss 2.0226\n",
      "Epoch 181 , average loss 2.022571086883545 , epoch_time 1.2148568630218506\n",
      "\n",
      "epoch->  182\n",
      "Iteration-> 0  , Iteration_time-> 1.2118 , Iteration_loss 2.0144\n",
      "Epoch 182 , average loss 2.0144412517547607 , epoch_time 1.2121169567108154\n",
      "\n",
      "epoch->  183\n",
      "Iteration-> 0  , Iteration_time-> 1.2378 , Iteration_loss 2.0207\n",
      "Epoch 183 , average loss 2.0206587314605713 , epoch_time 1.238037109375\n",
      "\n",
      "epoch->  184\n",
      "Iteration-> 0  , Iteration_time-> 1.2286 , Iteration_loss 2.0103\n",
      "Epoch 184 , average loss 2.010310173034668 , epoch_time 1.228832483291626\n",
      "\n",
      "epoch->  185\n",
      "Iteration-> 0  , Iteration_time-> 1.2388 , Iteration_loss 2.0073\n",
      "Epoch 185 , average loss 2.0072784423828125 , epoch_time 1.239046573638916\n",
      "\n",
      "epoch->  186\n",
      "Iteration-> 0  , Iteration_time-> 1.2263 , Iteration_loss 2.0093\n",
      "Epoch 186 , average loss 2.009276866912842 , epoch_time 1.2266027927398682\n",
      "\n",
      "epoch->  187\n",
      "Iteration-> 0  , Iteration_time-> 1.2326 , Iteration_loss 2.0034\n",
      "Epoch 187 , average loss 2.003354072570801 , epoch_time 1.232846975326538\n",
      "\n",
      "epoch->  188\n",
      "Iteration-> 0  , Iteration_time-> 1.2247 , Iteration_loss 2.0004\n",
      "Epoch 188 , average loss 2.0003697872161865 , epoch_time 1.224905252456665\n",
      "\n",
      "epoch->  189\n",
      "Iteration-> 0  , Iteration_time-> 1.2378 , Iteration_loss 1.9934\n",
      "Epoch 189 , average loss 1.9934498071670532 , epoch_time 1.237999439239502\n",
      "\n",
      "epoch->  190\n",
      "Iteration-> 0  , Iteration_time-> 1.2179 , Iteration_loss 1.9877\n",
      "Epoch 190 , average loss 1.9876950979232788 , epoch_time 1.218200445175171\n",
      "\n",
      "epoch->  191\n",
      "Iteration-> 0  , Iteration_time-> 1.2205 , Iteration_loss 1.9847\n",
      "Epoch 191 , average loss 1.9846519231796265 , epoch_time 1.2207696437835693\n",
      "\n",
      "epoch->  192\n",
      "Iteration-> 0  , Iteration_time-> 1.2112 , Iteration_loss 1.9834\n",
      "Epoch 192 , average loss 1.9833742380142212 , epoch_time 1.2114160060882568\n",
      "\n",
      "epoch->  193\n",
      "Iteration-> 0  , Iteration_time-> 1.2298 , Iteration_loss 1.9685\n",
      "Epoch 193 , average loss 1.9685293436050415 , epoch_time 1.229954719543457\n",
      "\n",
      "epoch->  194\n",
      "Iteration-> 0  , Iteration_time-> 1.2168 , Iteration_loss 1.9722\n",
      "Epoch 194 , average loss 1.9722496271133423 , epoch_time 1.216982126235962\n",
      "\n",
      "epoch->  195\n",
      "Iteration-> 0  , Iteration_time-> 1.2168 , Iteration_loss 1.9714\n",
      "Epoch 195 , average loss 1.9714244604110718 , epoch_time 1.216975212097168\n",
      "\n",
      "epoch->  196\n",
      "Iteration-> 0  , Iteration_time-> 1.2414 , Iteration_loss 1.9723\n",
      "Epoch 196 , average loss 1.9722626209259033 , epoch_time 1.24159836769104\n",
      "\n",
      "epoch->  197\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.9731\n",
      "Epoch 197 , average loss 1.9730772972106934 , epoch_time 1.2049264907836914\n",
      "\n",
      "epoch->  198\n",
      "Iteration-> 0  , Iteration_time-> 1.2151 , Iteration_loss 1.9644\n",
      "Epoch 198 , average loss 1.9644172191619873 , epoch_time 1.21527099609375\n",
      "\n",
      "epoch->  199\n",
      "Iteration-> 0  , Iteration_time-> 1.2127 , Iteration_loss 1.9637\n",
      "Epoch 199 , average loss 1.9637272357940674 , epoch_time 1.2129089832305908\n",
      "\n",
      "epoch->  200\n",
      "Iteration-> 0  , Iteration_time-> 1.2074 , Iteration_loss 1.9512\n",
      "Epoch 200 , average loss 1.9512380361557007 , epoch_time 1.2076163291931152\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  201\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 1.9537\n",
      "Epoch 201 , average loss 1.9537088871002197 , epoch_time 1.2118184566497803\n",
      "\n",
      "epoch->  202\n",
      "Iteration-> 0  , Iteration_time-> 1.2104 , Iteration_loss 1.9468\n",
      "Epoch 202 , average loss 1.9467679262161255 , epoch_time 1.2106034755706787\n",
      "\n",
      "epoch->  203\n",
      "Iteration-> 0  , Iteration_time-> 1.2095 , Iteration_loss 1.9385\n",
      "Epoch 203 , average loss 1.9384618997573853 , epoch_time 1.2097322940826416\n",
      "\n",
      "epoch->  204\n",
      "Iteration-> 0  , Iteration_time-> 1.2135 , Iteration_loss 1.9459\n",
      "Epoch 204 , average loss 1.9459148645401 , epoch_time 1.2136752605438232\n",
      "\n",
      "epoch->  205\n",
      "Iteration-> 0  , Iteration_time-> 1.2048 , Iteration_loss 1.9449\n",
      "Epoch 205 , average loss 1.9448708295822144 , epoch_time 1.2049942016601562\n",
      "\n",
      "epoch->  206\n",
      "Iteration-> 0  , Iteration_time-> 1.2202 , Iteration_loss 1.9408\n",
      "Epoch 206 , average loss 1.9407950639724731 , epoch_time 1.2203984260559082\n",
      "\n",
      "epoch->  207\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.9417\n",
      "Epoch 207 , average loss 1.9416526556015015 , epoch_time 1.211625099182129\n",
      "\n",
      "epoch->  208\n",
      "Iteration-> 0  , Iteration_time-> 1.2215 , Iteration_loss 1.9326\n",
      "Epoch 208 , average loss 1.9325833320617676 , epoch_time 1.2217390537261963\n",
      "\n",
      "epoch->  209\n",
      "Iteration-> 0  , Iteration_time-> 1.2161 , Iteration_loss 1.9323\n",
      "Epoch 209 , average loss 1.9323289394378662 , epoch_time 1.2163431644439697\n",
      "\n",
      "epoch->  210\n",
      "Iteration-> 0  , Iteration_time-> 1.2261 , Iteration_loss 1.9247\n",
      "Epoch 210 , average loss 1.9246978759765625 , epoch_time 1.226306676864624\n",
      "\n",
      "epoch->  211\n",
      "Iteration-> 0  , Iteration_time-> 1.2157 , Iteration_loss 1.9231\n",
      "Epoch 211 , average loss 1.9231419563293457 , epoch_time 1.215888500213623\n",
      "\n",
      "epoch->  212\n",
      "Iteration-> 0  , Iteration_time-> 1.2254 , Iteration_loss 1.9266\n",
      "Epoch 212 , average loss 1.9266126155853271 , epoch_time 1.225581407546997\n",
      "\n",
      "epoch->  213\n",
      "Iteration-> 0  , Iteration_time-> 1.2183 , Iteration_loss 1.9166\n",
      "Epoch 213 , average loss 1.916593074798584 , epoch_time 1.2185347080230713\n",
      "\n",
      "epoch->  214\n",
      "Iteration-> 0  , Iteration_time-> 1.2131 , Iteration_loss 1.9146\n",
      "Epoch 214 , average loss 1.9145519733428955 , epoch_time 1.2133164405822754\n",
      "\n",
      "epoch->  215\n",
      "Iteration-> 0  , Iteration_time-> 1.2139 , Iteration_loss 1.9138\n",
      "Epoch 215 , average loss 1.91383695602417 , epoch_time 1.2141499519348145\n",
      "\n",
      "epoch->  216\n",
      "Iteration-> 0  , Iteration_time-> 1.2212 , Iteration_loss 1.9108\n",
      "Epoch 216 , average loss 1.910778522491455 , epoch_time 1.2213943004608154\n",
      "\n",
      "epoch->  217\n",
      "Iteration-> 0  , Iteration_time-> 1.2183 , Iteration_loss 1.9166\n",
      "Epoch 217 , average loss 1.916623830795288 , epoch_time 1.2184770107269287\n",
      "\n",
      "epoch->  218\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.9094\n",
      "Epoch 218 , average loss 1.9093942642211914 , epoch_time 1.2049109935760498\n",
      "\n",
      "epoch->  219\n",
      "Iteration-> 0  , Iteration_time-> 1.2100 , Iteration_loss 1.9012\n",
      "Epoch 219 , average loss 1.9012075662612915 , epoch_time 1.210235834121704\n",
      "\n",
      "epoch->  220\n",
      "Iteration-> 0  , Iteration_time-> 1.2080 , Iteration_loss 1.8978\n",
      "Epoch 220 , average loss 1.897786259651184 , epoch_time 1.2082412242889404\n",
      "\n",
      "epoch->  221\n",
      "Iteration-> 0  , Iteration_time-> 1.2092 , Iteration_loss 1.8951\n",
      "Epoch 221 , average loss 1.895054578781128 , epoch_time 1.2093501091003418\n",
      "\n",
      "epoch->  222\n",
      "Iteration-> 0  , Iteration_time-> 1.2241 , Iteration_loss 1.8904\n",
      "Epoch 222 , average loss 1.890353798866272 , epoch_time 1.2243471145629883\n",
      "\n",
      "epoch->  223\n",
      "Iteration-> 0  , Iteration_time-> 1.2100 , Iteration_loss 1.8941\n",
      "Epoch 223 , average loss 1.8941174745559692 , epoch_time 1.210216999053955\n",
      "\n",
      "epoch->  224\n",
      "Iteration-> 0  , Iteration_time-> 1.2053 , Iteration_loss 1.8851\n",
      "Epoch 224 , average loss 1.8851197957992554 , epoch_time 1.2054708003997803\n",
      "\n",
      "epoch->  225\n",
      "Iteration-> 0  , Iteration_time-> 1.2363 , Iteration_loss 1.8908\n",
      "Epoch 225 , average loss 1.8908436298370361 , epoch_time 1.2365386486053467\n",
      "\n",
      "epoch->  226\n",
      "Iteration-> 0  , Iteration_time-> 1.2123 , Iteration_loss 1.8785\n",
      "Epoch 226 , average loss 1.8785083293914795 , epoch_time 1.2125215530395508\n",
      "\n",
      "epoch->  227\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 1.8801\n",
      "Epoch 227 , average loss 1.8801145553588867 , epoch_time 1.2075285911560059\n",
      "\n",
      "epoch->  228\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 1.8741\n",
      "Epoch 228 , average loss 1.8740766048431396 , epoch_time 1.2108206748962402\n",
      "\n",
      "epoch->  229\n",
      "Iteration-> 0  , Iteration_time-> 1.2318 , Iteration_loss 1.8723\n",
      "Epoch 229 , average loss 1.8723481893539429 , epoch_time 1.232018232345581\n",
      "\n",
      "epoch->  230\n",
      "Iteration-> 0  , Iteration_time-> 1.2188 , Iteration_loss 1.8704\n",
      "Epoch 230 , average loss 1.8704311847686768 , epoch_time 1.218984603881836\n",
      "\n",
      "epoch->  231\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 1.8661\n",
      "Epoch 231 , average loss 1.8660880327224731 , epoch_time 1.2175631523132324\n",
      "\n",
      "epoch->  232\n",
      "Iteration-> 0  , Iteration_time-> 1.2211 , Iteration_loss 1.8650\n",
      "Epoch 232 , average loss 1.8649543523788452 , epoch_time 1.2213187217712402\n",
      "\n",
      "epoch->  233\n",
      "Iteration-> 0  , Iteration_time-> 1.2040 , Iteration_loss 1.8561\n",
      "Epoch 233 , average loss 1.8560765981674194 , epoch_time 1.2042157649993896\n",
      "\n",
      "epoch->  234\n",
      "Iteration-> 0  , Iteration_time-> 1.2063 , Iteration_loss 1.8681\n",
      "Epoch 234 , average loss 1.868112325668335 , epoch_time 1.2065215110778809\n",
      "\n",
      "epoch->  235\n",
      "Iteration-> 0  , Iteration_time-> 1.2098 , Iteration_loss 1.8520\n",
      "Epoch 235 , average loss 1.8519681692123413 , epoch_time 1.2099952697753906\n",
      "\n",
      "epoch->  236\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 1.8529\n",
      "Epoch 236 , average loss 1.8529497385025024 , epoch_time 1.2077527046203613\n",
      "\n",
      "epoch->  237\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.8522\n",
      "Epoch 237 , average loss 1.852221131324768 , epoch_time 1.2048859596252441\n",
      "\n",
      "epoch->  238\n",
      "Iteration-> 0  , Iteration_time-> 1.2132 , Iteration_loss 1.8521\n",
      "Epoch 238 , average loss 1.852052927017212 , epoch_time 1.2133841514587402\n",
      "\n",
      "epoch->  239\n",
      "Iteration-> 0  , Iteration_time-> 1.2156 , Iteration_loss 1.8479\n",
      "Epoch 239 , average loss 1.8478895425796509 , epoch_time 1.2157702445983887\n",
      "\n",
      "epoch->  240\n",
      "Iteration-> 0  , Iteration_time-> 1.2044 , Iteration_loss 1.8449\n",
      "Epoch 240 , average loss 1.8448950052261353 , epoch_time 1.2045996189117432\n",
      "\n",
      "epoch->  241\n",
      "Iteration-> 0  , Iteration_time-> 1.2156 , Iteration_loss 1.8397\n",
      "Epoch 241 , average loss 1.8396823406219482 , epoch_time 1.2158689498901367\n",
      "\n",
      "epoch->  242\n",
      "Iteration-> 0  , Iteration_time-> 1.2129 , Iteration_loss 1.8400\n",
      "Epoch 242 , average loss 1.840009093284607 , epoch_time 1.2133634090423584\n",
      "\n",
      "epoch->  243\n",
      "Iteration-> 0  , Iteration_time-> 1.2184 , Iteration_loss 1.8372\n",
      "Epoch 243 , average loss 1.8372241258621216 , epoch_time 1.2186393737792969\n",
      "\n",
      "epoch->  244\n",
      "Iteration-> 0  , Iteration_time-> 1.2103 , Iteration_loss 1.8438\n",
      "Epoch 244 , average loss 1.8438050746917725 , epoch_time 1.2105138301849365\n",
      "\n",
      "epoch->  245\n",
      "Iteration-> 0  , Iteration_time-> 1.2181 , Iteration_loss 1.8301\n",
      "Epoch 245 , average loss 1.8300681114196777 , epoch_time 1.218273401260376\n",
      "\n",
      "epoch->  246\n",
      "Iteration-> 0  , Iteration_time-> 1.2201 , Iteration_loss 1.8294\n",
      "Epoch 246 , average loss 1.8294495344161987 , epoch_time 1.2202684879302979\n",
      "\n",
      "epoch->  247\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 1.8251\n",
      "Epoch 247 , average loss 1.8251005411148071 , epoch_time 1.2155625820159912\n",
      "\n",
      "epoch->  248\n",
      "Iteration-> 0  , Iteration_time-> 1.2220 , Iteration_loss 1.8266\n",
      "Epoch 248 , average loss 1.8266199827194214 , epoch_time 1.2222237586975098\n",
      "\n",
      "epoch->  249\n",
      "Iteration-> 0  , Iteration_time-> 1.2162 , Iteration_loss 1.8188\n",
      "Epoch 249 , average loss 1.8187510967254639 , epoch_time 1.21641206741333\n",
      "\n",
      "epoch->  250\n",
      "Iteration-> 0  , Iteration_time-> 1.2126 , Iteration_loss 1.8226\n",
      "Epoch 250 , average loss 1.822636365890503 , epoch_time 1.2128486633300781\n",
      "\n",
      "epoch->  251\n",
      "Iteration-> 0  , Iteration_time-> 1.2145 , Iteration_loss 1.8269\n",
      "Epoch 251 , average loss 1.8268706798553467 , epoch_time 1.2146849632263184\n",
      "\n",
      "epoch->  252\n",
      "Iteration-> 0  , Iteration_time-> 1.2049 , Iteration_loss 1.8125\n",
      "Epoch 252 , average loss 1.8124624490737915 , epoch_time 1.205547571182251\n",
      "\n",
      "epoch->  253\n",
      "Iteration-> 0  , Iteration_time-> 1.2052 , Iteration_loss 1.8168\n",
      "Epoch 253 , average loss 1.8168174028396606 , epoch_time 1.205397367477417\n",
      "\n",
      "epoch->  254\n",
      "Iteration-> 0  , Iteration_time-> 1.2137 , Iteration_loss 1.8088\n",
      "Epoch 254 , average loss 1.8088300228118896 , epoch_time 1.2139246463775635\n",
      "\n",
      "epoch->  255\n",
      "Iteration-> 0  , Iteration_time-> 1.2145 , Iteration_loss 1.8079\n",
      "Epoch 255 , average loss 1.8079065084457397 , epoch_time 1.2147433757781982\n",
      "\n",
      "epoch->  256\n",
      "Iteration-> 0  , Iteration_time-> 1.2094 , Iteration_loss 1.8018\n",
      "Epoch 256 , average loss 1.8017765283584595 , epoch_time 1.2095625400543213\n",
      "\n",
      "epoch->  257\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 1.8064\n",
      "Epoch 257 , average loss 1.8063865900039673 , epoch_time 1.2155802249908447\n",
      "\n",
      "epoch->  258\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 1.8052\n",
      "Epoch 258 , average loss 1.8051800727844238 , epoch_time 1.2176165580749512\n",
      "\n",
      "epoch->  259\n",
      "Iteration-> 0  , Iteration_time-> 1.2055 , Iteration_loss 1.8004\n",
      "Epoch 259 , average loss 1.8004488945007324 , epoch_time 1.2056560516357422\n",
      "\n",
      "epoch->  260\n",
      "Iteration-> 0  , Iteration_time-> 1.2171 , Iteration_loss 1.7867\n",
      "Epoch 260 , average loss 1.7867215871810913 , epoch_time 1.2172894477844238\n",
      "\n",
      "epoch->  261\n",
      "Iteration-> 0  , Iteration_time-> 1.2066 , Iteration_loss 1.7955\n",
      "Epoch 261 , average loss 1.7954986095428467 , epoch_time 1.2068164348602295\n",
      "\n",
      "epoch->  262\n",
      "Iteration-> 0  , Iteration_time-> 1.2187 , Iteration_loss 1.7909\n",
      "Epoch 262 , average loss 1.7909480333328247 , epoch_time 1.2189357280731201\n",
      "\n",
      "epoch->  263\n",
      "Iteration-> 0  , Iteration_time-> 1.2133 , Iteration_loss 1.7839\n",
      "Epoch 263 , average loss 1.7839341163635254 , epoch_time 1.2135131359100342\n",
      "\n",
      "epoch->  264\n",
      "Iteration-> 0  , Iteration_time-> 1.2101 , Iteration_loss 1.7873\n",
      "Epoch 264 , average loss 1.7872707843780518 , epoch_time 1.2102556228637695\n",
      "\n",
      "epoch->  265\n",
      "Iteration-> 0  , Iteration_time-> 1.2170 , Iteration_loss 1.7914\n",
      "Epoch 265 , average loss 1.791441559791565 , epoch_time 1.2172777652740479\n",
      "\n",
      "epoch->  266\n",
      "Iteration-> 0  , Iteration_time-> 1.2189 , Iteration_loss 1.7850\n",
      "Epoch 266 , average loss 1.784998893737793 , epoch_time 1.2191245555877686\n",
      "\n",
      "epoch->  267\n",
      "Iteration-> 0  , Iteration_time-> 1.2096 , Iteration_loss 1.7797\n",
      "Epoch 267 , average loss 1.779678225517273 , epoch_time 1.2098262310028076\n",
      "\n",
      "epoch->  268\n",
      "Iteration-> 0  , Iteration_time-> 1.2150 , Iteration_loss 1.7785\n",
      "Epoch 268 , average loss 1.77845299243927 , epoch_time 1.2152307033538818\n",
      "\n",
      "epoch->  269\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 1.7820\n",
      "Epoch 269 , average loss 1.7819722890853882 , epoch_time 1.211871862411499\n",
      "\n",
      "epoch->  270\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 1.7632\n",
      "Epoch 270 , average loss 1.76324462890625 , epoch_time 1.2168653011322021\n",
      "\n",
      "epoch->  271\n",
      "Iteration-> 0  , Iteration_time-> 1.2137 , Iteration_loss 1.7736\n",
      "Epoch 271 , average loss 1.773577094078064 , epoch_time 1.2138855457305908\n",
      "\n",
      "epoch->  272\n",
      "Iteration-> 0  , Iteration_time-> 1.2191 , Iteration_loss 1.7680\n",
      "Epoch 272 , average loss 1.7679654359817505 , epoch_time 1.2193853855133057\n",
      "\n",
      "epoch->  273\n",
      "Iteration-> 0  , Iteration_time-> 1.2180 , Iteration_loss 1.7686\n",
      "Epoch 273 , average loss 1.7685803174972534 , epoch_time 1.2182223796844482\n",
      "\n",
      "epoch->  274\n",
      "Iteration-> 0  , Iteration_time-> 1.2193 , Iteration_loss 1.7662\n",
      "Epoch 274 , average loss 1.7661901712417603 , epoch_time 1.2195179462432861\n",
      "\n",
      "epoch->  275\n",
      "Iteration-> 0  , Iteration_time-> 1.2216 , Iteration_loss 1.7577\n",
      "Epoch 275 , average loss 1.7577003240585327 , epoch_time 1.221834421157837\n",
      "\n",
      "epoch->  276\n",
      "Iteration-> 0  , Iteration_time-> 1.2201 , Iteration_loss 1.7547\n",
      "Epoch 276 , average loss 1.7547115087509155 , epoch_time 1.220346450805664\n",
      "\n",
      "epoch->  277\n",
      "Iteration-> 0  , Iteration_time-> 1.2051 , Iteration_loss 1.7570\n",
      "Epoch 277 , average loss 1.7570048570632935 , epoch_time 1.2053122520446777\n",
      "\n",
      "epoch->  278\n",
      "Iteration-> 0  , Iteration_time-> 1.2185 , Iteration_loss 1.7563\n",
      "Epoch 278 , average loss 1.7563050985336304 , epoch_time 1.2187035083770752\n",
      "\n",
      "epoch->  279\n",
      "Iteration-> 0  , Iteration_time-> 1.2138 , Iteration_loss 1.7453\n",
      "Epoch 279 , average loss 1.7452564239501953 , epoch_time 1.2140398025512695\n",
      "\n",
      "epoch->  280\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.7483\n",
      "Epoch 280 , average loss 1.7482718229293823 , epoch_time 1.2049081325531006\n",
      "\n",
      "epoch->  281\n",
      "Iteration-> 0  , Iteration_time-> 1.2184 , Iteration_loss 1.7475\n",
      "Epoch 281 , average loss 1.7474614381790161 , epoch_time 1.2186672687530518\n",
      "\n",
      "epoch->  282\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.7475\n",
      "Epoch 282 , average loss 1.7475420236587524 , epoch_time 1.2103965282440186\n",
      "\n",
      "epoch->  283\n",
      "Iteration-> 0  , Iteration_time-> 1.2044 , Iteration_loss 1.7486\n",
      "Epoch 283 , average loss 1.748584270477295 , epoch_time 1.2046236991882324\n",
      "\n",
      "epoch->  284\n",
      "Iteration-> 0  , Iteration_time-> 1.2041 , Iteration_loss 1.7414\n",
      "Epoch 284 , average loss 1.741400122642517 , epoch_time 1.2043256759643555\n",
      "\n",
      "epoch->  285\n",
      "Iteration-> 0  , Iteration_time-> 1.2096 , Iteration_loss 1.7441\n",
      "Epoch 285 , average loss 1.7440890073776245 , epoch_time 1.2098002433776855\n",
      "\n",
      "epoch->  286\n",
      "Iteration-> 0  , Iteration_time-> 1.2036 , Iteration_loss 1.7378\n",
      "Epoch 286 , average loss 1.7377902269363403 , epoch_time 1.2038016319274902\n",
      "\n",
      "epoch->  287\n",
      "Iteration-> 0  , Iteration_time-> 1.2069 , Iteration_loss 1.7387\n",
      "Epoch 287 , average loss 1.7386870384216309 , epoch_time 1.2071452140808105\n",
      "\n",
      "epoch->  288\n",
      "Iteration-> 0  , Iteration_time-> 1.2159 , Iteration_loss 1.7300\n",
      "Epoch 288 , average loss 1.7299944162368774 , epoch_time 1.216076135635376\n",
      "\n",
      "epoch->  289\n",
      "Iteration-> 0  , Iteration_time-> 1.2059 , Iteration_loss 1.7350\n",
      "Epoch 289 , average loss 1.7350093126296997 , epoch_time 1.2060577869415283\n",
      "\n",
      "epoch->  290\n",
      "Iteration-> 0  , Iteration_time-> 1.2115 , Iteration_loss 1.7365\n",
      "Epoch 290 , average loss 1.736507773399353 , epoch_time 1.2116882801055908\n",
      "\n",
      "epoch->  291\n",
      "Iteration-> 0  , Iteration_time-> 1.2120 , Iteration_loss 1.7239\n",
      "Epoch 291 , average loss 1.723869800567627 , epoch_time 1.2121822834014893\n",
      "\n",
      "epoch->  292\n",
      "Iteration-> 0  , Iteration_time-> 1.2109 , Iteration_loss 1.7213\n",
      "Epoch 292 , average loss 1.7212570905685425 , epoch_time 1.2111396789550781\n",
      "\n",
      "epoch->  293\n",
      "Iteration-> 0  , Iteration_time-> 1.2079 , Iteration_loss 1.7235\n",
      "Epoch 293 , average loss 1.7234967947006226 , epoch_time 1.2081520557403564\n",
      "\n",
      "epoch->  294\n",
      "Iteration-> 0  , Iteration_time-> 1.2147 , Iteration_loss 1.7216\n",
      "Epoch 294 , average loss 1.7216473817825317 , epoch_time 1.2148945331573486\n",
      "\n",
      "epoch->  295\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.7064\n",
      "Epoch 295 , average loss 1.7063572406768799 , epoch_time 1.211594820022583\n",
      "\n",
      "epoch->  296\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.7065\n",
      "Epoch 296 , average loss 1.7065116167068481 , epoch_time 1.210383415222168\n",
      "\n",
      "epoch->  297\n",
      "Iteration-> 0  , Iteration_time-> 1.2036 , Iteration_loss 1.7109\n",
      "Epoch 297 , average loss 1.7109143733978271 , epoch_time 1.2037675380706787\n",
      "\n",
      "epoch->  298\n",
      "Iteration-> 0  , Iteration_time-> 1.2104 , Iteration_loss 1.7129\n",
      "Epoch 298 , average loss 1.7128961086273193 , epoch_time 1.2105612754821777\n",
      "\n",
      "epoch->  299\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.7019\n",
      "Epoch 299 , average loss 1.7018747329711914 , epoch_time 1.2115528583526611\n",
      "\n",
      "epoch->  300\n",
      "Iteration-> 0  , Iteration_time-> 1.2093 , Iteration_loss 1.7023\n",
      "Epoch 300 , average loss 1.7023006677627563 , epoch_time 1.2095379829406738\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  301\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.7000\n",
      "Epoch 301 , average loss 1.7000164985656738 , epoch_time 1.2103502750396729\n",
      "\n",
      "epoch->  302\n",
      "Iteration-> 0  , Iteration_time-> 1.2164 , Iteration_loss 1.6963\n",
      "Epoch 302 , average loss 1.6963437795639038 , epoch_time 1.2165956497192383\n",
      "\n",
      "epoch->  303\n",
      "Iteration-> 0  , Iteration_time-> 1.2034 , Iteration_loss 1.7048\n",
      "Epoch 303 , average loss 1.7048425674438477 , epoch_time 1.2038750648498535\n",
      "\n",
      "epoch->  304\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 1.6918\n",
      "Epoch 304 , average loss 1.6918352842330933 , epoch_time 1.2108688354492188\n",
      "\n",
      "epoch->  305\n",
      "Iteration-> 0  , Iteration_time-> 1.2144 , Iteration_loss 1.6952\n",
      "Epoch 305 , average loss 1.695172905921936 , epoch_time 1.2146148681640625\n",
      "\n",
      "epoch->  306\n",
      "Iteration-> 0  , Iteration_time-> 1.2123 , Iteration_loss 1.6908\n",
      "Epoch 306 , average loss 1.6907801628112793 , epoch_time 1.212533950805664\n",
      "\n",
      "epoch->  307\n",
      "Iteration-> 0  , Iteration_time-> 1.2155 , Iteration_loss 1.6874\n",
      "Epoch 307 , average loss 1.6873618364334106 , epoch_time 1.2156586647033691\n",
      "\n",
      "epoch->  308\n",
      "Iteration-> 0  , Iteration_time-> 1.2112 , Iteration_loss 1.6771\n",
      "Epoch 308 , average loss 1.6771472692489624 , epoch_time 1.2113840579986572\n",
      "\n",
      "epoch->  309\n",
      "Iteration-> 0  , Iteration_time-> 1.2087 , Iteration_loss 1.6851\n",
      "Epoch 309 , average loss 1.6851152181625366 , epoch_time 1.2089533805847168\n",
      "\n",
      "epoch->  310\n",
      "Iteration-> 0  , Iteration_time-> 1.2164 , Iteration_loss 1.6753\n",
      "Epoch 310 , average loss 1.67534601688385 , epoch_time 1.216588020324707\n",
      "\n",
      "epoch->  311\n",
      "Iteration-> 0  , Iteration_time-> 1.2306 , Iteration_loss 1.6721\n",
      "Epoch 311 , average loss 1.6721091270446777 , epoch_time 1.2308835983276367\n",
      "\n",
      "epoch->  312\n",
      "Iteration-> 0  , Iteration_time-> 1.2126 , Iteration_loss 1.6732\n",
      "Epoch 312 , average loss 1.6731617450714111 , epoch_time 1.2127957344055176\n",
      "\n",
      "epoch->  313\n",
      "Iteration-> 0  , Iteration_time-> 1.2346 , Iteration_loss 1.6742\n",
      "Epoch 313 , average loss 1.6742061376571655 , epoch_time 1.2347767353057861\n",
      "\n",
      "epoch->  314\n",
      "Iteration-> 0  , Iteration_time-> 1.2146 , Iteration_loss 1.6732\n",
      "Epoch 314 , average loss 1.6732341051101685 , epoch_time 1.2151579856872559\n",
      "\n",
      "epoch->  315\n",
      "Iteration-> 0  , Iteration_time-> 1.2143 , Iteration_loss 1.6701\n",
      "Epoch 315 , average loss 1.6700859069824219 , epoch_time 1.214526653289795\n",
      "\n",
      "epoch->  316\n",
      "Iteration-> 0  , Iteration_time-> 1.2172 , Iteration_loss 1.6596\n",
      "Epoch 316 , average loss 1.6595771312713623 , epoch_time 1.2174592018127441\n",
      "\n",
      "epoch->  317\n",
      "Iteration-> 0  , Iteration_time-> 1.2056 , Iteration_loss 1.6700\n",
      "Epoch 317 , average loss 1.6699531078338623 , epoch_time 1.2058544158935547\n",
      "\n",
      "epoch->  318\n",
      "Iteration-> 0  , Iteration_time-> 1.2098 , Iteration_loss 1.6602\n",
      "Epoch 318 , average loss 1.6601837873458862 , epoch_time 1.2100088596343994\n",
      "\n",
      "epoch->  319\n",
      "Iteration-> 0  , Iteration_time-> 1.2101 , Iteration_loss 1.6573\n",
      "Epoch 319 , average loss 1.6572809219360352 , epoch_time 1.2102758884429932\n",
      "\n",
      "epoch->  320\n",
      "Iteration-> 0  , Iteration_time-> 1.2315 , Iteration_loss 1.6547\n",
      "Epoch 320 , average loss 1.6546777486801147 , epoch_time 1.2317025661468506\n",
      "\n",
      "epoch->  321\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.6517\n",
      "Epoch 321 , average loss 1.6516716480255127 , epoch_time 1.2115592956542969\n",
      "\n",
      "epoch->  322\n",
      "Iteration-> 0  , Iteration_time-> 1.2003 , Iteration_loss 1.6493\n",
      "Epoch 322 , average loss 1.64930260181427 , epoch_time 1.2005460262298584\n",
      "\n",
      "epoch->  323\n",
      "Iteration-> 0  , Iteration_time-> 1.1981 , Iteration_loss 1.6494\n",
      "Epoch 323 , average loss 1.6493613719940186 , epoch_time 1.198324203491211\n",
      "\n",
      "epoch->  324\n",
      "Iteration-> 0  , Iteration_time-> 1.2029 , Iteration_loss 1.6454\n",
      "Epoch 324 , average loss 1.6453561782836914 , epoch_time 1.203141212463379\n",
      "\n",
      "epoch->  325\n",
      "Iteration-> 0  , Iteration_time-> 1.1996 , Iteration_loss 1.6379\n",
      "Epoch 325 , average loss 1.6379098892211914 , epoch_time 1.1998467445373535\n",
      "\n",
      "epoch->  326\n",
      "Iteration-> 0  , Iteration_time-> 1.2029 , Iteration_loss 1.6429\n",
      "Epoch 326 , average loss 1.6428667306900024 , epoch_time 1.2030932903289795\n",
      "\n",
      "epoch->  327\n",
      "Iteration-> 0  , Iteration_time-> 1.2031 , Iteration_loss 1.6355\n",
      "Epoch 327 , average loss 1.6354994773864746 , epoch_time 1.20328950881958\n",
      "\n",
      "epoch->  328\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.6291\n",
      "Epoch 328 , average loss 1.6291208267211914 , epoch_time 1.2104685306549072\n",
      "\n",
      "epoch->  329\n",
      "Iteration-> 0  , Iteration_time-> 1.2036 , Iteration_loss 1.6290\n",
      "Epoch 329 , average loss 1.628955364227295 , epoch_time 1.2037591934204102\n",
      "\n",
      "epoch->  330\n",
      "Iteration-> 0  , Iteration_time-> 1.2043 , Iteration_loss 1.6252\n",
      "Epoch 330 , average loss 1.625213384628296 , epoch_time 1.2044670581817627\n",
      "\n",
      "epoch->  331\n",
      "Iteration-> 0  , Iteration_time-> 1.2200 , Iteration_loss 1.6172\n",
      "Epoch 331 , average loss 1.6172163486480713 , epoch_time 1.2202098369598389\n",
      "\n",
      "epoch->  332\n",
      "Iteration-> 0  , Iteration_time-> 1.2057 , Iteration_loss 1.6157\n",
      "Epoch 332 , average loss 1.6156518459320068 , epoch_time 1.2059361934661865\n",
      "\n",
      "epoch->  333\n",
      "Iteration-> 0  , Iteration_time-> 1.2186 , Iteration_loss 1.6137\n",
      "Epoch 333 , average loss 1.6136997938156128 , epoch_time 1.218926191329956\n",
      "\n",
      "epoch->  334\n",
      "Iteration-> 0  , Iteration_time-> 1.2223 , Iteration_loss 1.6116\n",
      "Epoch 334 , average loss 1.611610770225525 , epoch_time 1.2225017547607422\n",
      "\n",
      "epoch->  335\n",
      "Iteration-> 0  , Iteration_time-> 1.2117 , Iteration_loss 1.6154\n",
      "Epoch 335 , average loss 1.615362286567688 , epoch_time 1.2118830680847168\n",
      "\n",
      "epoch->  336\n",
      "Iteration-> 0  , Iteration_time-> 1.2230 , Iteration_loss 1.6056\n",
      "Epoch 336 , average loss 1.605606198310852 , epoch_time 1.2232484817504883\n",
      "\n",
      "epoch->  337\n",
      "Iteration-> 0  , Iteration_time-> 1.2149 , Iteration_loss 1.6064\n",
      "Epoch 337 , average loss 1.6064008474349976 , epoch_time 1.215196132659912\n",
      "\n",
      "epoch->  338\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 1.6019\n",
      "Epoch 338 , average loss 1.6018836498260498 , epoch_time 1.2168738842010498\n",
      "\n",
      "epoch->  339\n",
      "Iteration-> 0  , Iteration_time-> 1.2366 , Iteration_loss 1.5981\n",
      "Epoch 339 , average loss 1.5981214046478271 , epoch_time 1.2368214130401611\n",
      "\n",
      "epoch->  340\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 1.5977\n",
      "Epoch 340 , average loss 1.5977317094802856 , epoch_time 1.2075159549713135\n",
      "\n",
      "epoch->  341\n",
      "Iteration-> 0  , Iteration_time-> 1.2057 , Iteration_loss 1.5961\n",
      "Epoch 341 , average loss 1.5960676670074463 , epoch_time 1.205885887145996\n",
      "\n",
      "epoch->  342\n",
      "Iteration-> 0  , Iteration_time-> 1.2069 , Iteration_loss 1.5890\n",
      "Epoch 342 , average loss 1.5890392065048218 , epoch_time 1.2070579528808594\n",
      "\n",
      "epoch->  343\n",
      "Iteration-> 0  , Iteration_time-> 1.2142 , Iteration_loss 1.5933\n",
      "Epoch 343 , average loss 1.5933250188827515 , epoch_time 1.2144203186035156\n",
      "\n",
      "epoch->  344\n",
      "Iteration-> 0  , Iteration_time-> 1.2039 , Iteration_loss 1.5879\n",
      "Epoch 344 , average loss 1.5878580808639526 , epoch_time 1.2041232585906982\n",
      "\n",
      "epoch->  345\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 1.5901\n",
      "Epoch 345 , average loss 1.5901433229446411 , epoch_time 1.2107815742492676\n",
      "\n",
      "epoch->  346\n",
      "Iteration-> 0  , Iteration_time-> 1.2062 , Iteration_loss 1.5841\n",
      "Epoch 346 , average loss 1.5840797424316406 , epoch_time 1.2063970565795898\n",
      "\n",
      "epoch->  347\n",
      "Iteration-> 0  , Iteration_time-> 1.2238 , Iteration_loss 1.5786\n",
      "Epoch 347 , average loss 1.578605055809021 , epoch_time 1.224045753479004\n",
      "\n",
      "epoch->  348\n",
      "Iteration-> 0  , Iteration_time-> 1.2166 , Iteration_loss 1.5776\n",
      "Epoch 348 , average loss 1.5776472091674805 , epoch_time 1.216813087463379\n",
      "\n",
      "epoch->  349\n",
      "Iteration-> 0  , Iteration_time-> 1.2032 , Iteration_loss 1.5822\n",
      "Epoch 349 , average loss 1.5821540355682373 , epoch_time 1.2034249305725098\n",
      "\n",
      "epoch->  350\n",
      "Iteration-> 0  , Iteration_time-> 1.2165 , Iteration_loss 1.5603\n",
      "Epoch 350 , average loss 1.5603429079055786 , epoch_time 1.2167682647705078\n",
      "\n",
      "epoch->  351\n",
      "Iteration-> 0  , Iteration_time-> 1.1992 , Iteration_loss 1.5613\n",
      "Epoch 351 , average loss 1.5613102912902832 , epoch_time 1.199418067932129\n",
      "\n",
      "epoch->  352\n",
      "Iteration-> 0  , Iteration_time-> 1.2056 , Iteration_loss 1.5579\n",
      "Epoch 352 , average loss 1.5579286813735962 , epoch_time 1.2058210372924805\n",
      "\n",
      "epoch->  353\n",
      "Iteration-> 0  , Iteration_time-> 1.2031 , Iteration_loss 1.5625\n",
      "Epoch 353 , average loss 1.5624678134918213 , epoch_time 1.2033300399780273\n",
      "\n",
      "epoch->  354\n",
      "Iteration-> 0  , Iteration_time-> 1.2051 , Iteration_loss 1.5575\n",
      "Epoch 354 , average loss 1.557465672492981 , epoch_time 1.2053484916687012\n",
      "\n",
      "epoch->  355\n",
      "Iteration-> 0  , Iteration_time-> 1.1993 , Iteration_loss 1.5492\n",
      "Epoch 355 , average loss 1.549163818359375 , epoch_time 1.1994953155517578\n",
      "\n",
      "epoch->  356\n",
      "Iteration-> 0  , Iteration_time-> 1.2193 , Iteration_loss 1.5490\n",
      "Epoch 356 , average loss 1.5489740371704102 , epoch_time 1.2195489406585693\n",
      "\n",
      "epoch->  357\n",
      "Iteration-> 0  , Iteration_time-> 1.2042 , Iteration_loss 1.5470\n",
      "Epoch 357 , average loss 1.547032117843628 , epoch_time 1.2043876647949219\n",
      "\n",
      "epoch->  358\n",
      "Iteration-> 0  , Iteration_time-> 1.2094 , Iteration_loss 1.5470\n",
      "Epoch 358 , average loss 1.5470354557037354 , epoch_time 1.2095634937286377\n",
      "\n",
      "epoch->  359\n",
      "Iteration-> 0  , Iteration_time-> 1.2071 , Iteration_loss 1.5501\n",
      "Epoch 359 , average loss 1.5501482486724854 , epoch_time 1.2072784900665283\n",
      "\n",
      "epoch->  360\n",
      "Iteration-> 0  , Iteration_time-> 1.2053 , Iteration_loss 1.5355\n",
      "Epoch 360 , average loss 1.5355253219604492 , epoch_time 1.2054908275604248\n",
      "\n",
      "epoch->  361\n",
      "Iteration-> 0  , Iteration_time-> 1.2000 , Iteration_loss 1.5343\n",
      "Epoch 361 , average loss 1.5343085527420044 , epoch_time 1.2002136707305908\n",
      "\n",
      "epoch->  362\n",
      "Iteration-> 0  , Iteration_time-> 1.2178 , Iteration_loss 1.5305\n",
      "Epoch 362 , average loss 1.5305209159851074 , epoch_time 1.2179696559906006\n",
      "\n",
      "epoch->  363\n",
      "Iteration-> 0  , Iteration_time-> 1.2026 , Iteration_loss 1.5317\n",
      "Epoch 363 , average loss 1.5317426919937134 , epoch_time 1.2027919292449951\n",
      "\n",
      "epoch->  364\n",
      "Iteration-> 0  , Iteration_time-> 1.2131 , Iteration_loss 1.5280\n",
      "Epoch 364 , average loss 1.5280450582504272 , epoch_time 1.2132596969604492\n",
      "\n",
      "epoch->  365\n",
      "Iteration-> 0  , Iteration_time-> 1.2113 , Iteration_loss 1.5235\n",
      "Epoch 365 , average loss 1.5234898328781128 , epoch_time 1.2115592956542969\n",
      "\n",
      "epoch->  366\n",
      "Iteration-> 0  , Iteration_time-> 1.2202 , Iteration_loss 1.5212\n",
      "Epoch 366 , average loss 1.521173119544983 , epoch_time 1.2204203605651855\n",
      "\n",
      "epoch->  367\n",
      "Iteration-> 0  , Iteration_time-> 1.2139 , Iteration_loss 1.5139\n",
      "Epoch 367 , average loss 1.513898491859436 , epoch_time 1.214045763015747\n",
      "\n",
      "epoch->  368\n",
      "Iteration-> 0  , Iteration_time-> 1.2093 , Iteration_loss 1.5141\n",
      "Epoch 368 , average loss 1.5140879154205322 , epoch_time 1.2095026969909668\n",
      "\n",
      "epoch->  369\n",
      "Iteration-> 0  , Iteration_time-> 1.2035 , Iteration_loss 1.5036\n",
      "Epoch 369 , average loss 1.5036070346832275 , epoch_time 1.20363187789917\n",
      "\n",
      "epoch->  370\n",
      "Iteration-> 0  , Iteration_time-> 1.2162 , Iteration_loss 1.5060\n",
      "Epoch 370 , average loss 1.505972146987915 , epoch_time 1.2163655757904053\n",
      "\n",
      "epoch->  371\n",
      "Iteration-> 0  , Iteration_time-> 1.2108 , Iteration_loss 1.5072\n",
      "Epoch 371 , average loss 1.507236123085022 , epoch_time 1.2110531330108643\n",
      "\n",
      "epoch->  372\n",
      "Iteration-> 0  , Iteration_time-> 1.2111 , Iteration_loss 1.4995\n",
      "Epoch 372 , average loss 1.499458909034729 , epoch_time 1.2113244533538818\n",
      "\n",
      "epoch->  373\n",
      "Iteration-> 0  , Iteration_time-> 1.2086 , Iteration_loss 1.4928\n",
      "Epoch 373 , average loss 1.4928362369537354 , epoch_time 1.2087864875793457\n",
      "\n",
      "epoch->  374\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 1.4937\n",
      "Epoch 374 , average loss 1.49374520778656 , epoch_time 1.210822343826294\n",
      "\n",
      "epoch->  375\n",
      "Iteration-> 0  , Iteration_time-> 1.2042 , Iteration_loss 1.4931\n",
      "Epoch 375 , average loss 1.493117094039917 , epoch_time 1.2043850421905518\n",
      "\n",
      "epoch->  376\n",
      "Iteration-> 0  , Iteration_time-> 1.2069 , Iteration_loss 1.4845\n",
      "Epoch 376 , average loss 1.4845491647720337 , epoch_time 1.2071287631988525\n",
      "\n",
      "epoch->  377\n",
      "Iteration-> 0  , Iteration_time-> 1.2038 , Iteration_loss 1.4873\n",
      "Epoch 377 , average loss 1.4872822761535645 , epoch_time 1.203974962234497\n",
      "\n",
      "epoch->  378\n",
      "Iteration-> 0  , Iteration_time-> 1.2141 , Iteration_loss 1.4835\n",
      "Epoch 378 , average loss 1.4835373163223267 , epoch_time 1.2143440246582031\n",
      "\n",
      "epoch->  379\n",
      "Iteration-> 0  , Iteration_time-> 1.2011 , Iteration_loss 1.4770\n",
      "Epoch 379 , average loss 1.4769562482833862 , epoch_time 1.2013144493103027\n",
      "\n",
      "epoch->  380\n",
      "Iteration-> 0  , Iteration_time-> 1.2148 , Iteration_loss 1.4798\n",
      "Epoch 380 , average loss 1.4798409938812256 , epoch_time 1.2149944305419922\n",
      "\n",
      "epoch->  381\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.4702\n",
      "Epoch 381 , average loss 1.4701939821243286 , epoch_time 1.2048718929290771\n",
      "\n",
      "epoch->  382\n",
      "Iteration-> 0  , Iteration_time-> 1.2078 , Iteration_loss 1.4671\n",
      "Epoch 382 , average loss 1.4671119451522827 , epoch_time 1.20798921585083\n",
      "\n",
      "epoch->  383\n",
      "Iteration-> 0  , Iteration_time-> 1.2010 , Iteration_loss 1.4590\n",
      "Epoch 383 , average loss 1.458957552909851 , epoch_time 1.2012255191802979\n",
      "\n",
      "epoch->  384\n",
      "Iteration-> 0  , Iteration_time-> 1.2059 , Iteration_loss 1.4629\n",
      "Epoch 384 , average loss 1.4628510475158691 , epoch_time 1.2060985565185547\n",
      "\n",
      "epoch->  385\n",
      "Iteration-> 0  , Iteration_time-> 1.2130 , Iteration_loss 1.4524\n",
      "Epoch 385 , average loss 1.4524025917053223 , epoch_time 1.2131593227386475\n",
      "\n",
      "epoch->  386\n",
      "Iteration-> 0  , Iteration_time-> 1.1996 , Iteration_loss 1.4538\n",
      "Epoch 386 , average loss 1.453837513923645 , epoch_time 1.1998183727264404\n",
      "\n",
      "epoch->  387\n",
      "Iteration-> 0  , Iteration_time-> 1.2109 , Iteration_loss 1.4454\n",
      "Epoch 387 , average loss 1.4453933238983154 , epoch_time 1.2111167907714844\n",
      "\n",
      "epoch->  388\n",
      "Iteration-> 0  , Iteration_time-> 1.2149 , Iteration_loss 1.4453\n",
      "Epoch 388 , average loss 1.445340871810913 , epoch_time 1.2150940895080566\n",
      "\n",
      "epoch->  389\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 1.4439\n",
      "Epoch 389 , average loss 1.4438693523406982 , epoch_time 1.2076592445373535\n",
      "\n",
      "epoch->  390\n",
      "Iteration-> 0  , Iteration_time-> 1.2130 , Iteration_loss 1.4448\n",
      "Epoch 390 , average loss 1.4447673559188843 , epoch_time 1.2132141590118408\n",
      "\n",
      "epoch->  391\n",
      "Iteration-> 0  , Iteration_time-> 1.2011 , Iteration_loss 1.4360\n",
      "Epoch 391 , average loss 1.4359699487686157 , epoch_time 1.2013213634490967\n",
      "\n",
      "epoch->  392\n",
      "Iteration-> 0  , Iteration_time-> 1.2061 , Iteration_loss 1.4294\n",
      "Epoch 392 , average loss 1.4293609857559204 , epoch_time 1.2063260078430176\n",
      "\n",
      "epoch->  393\n",
      "Iteration-> 0  , Iteration_time-> 1.2101 , Iteration_loss 1.4318\n",
      "Epoch 393 , average loss 1.4317986965179443 , epoch_time 1.2102515697479248\n",
      "\n",
      "epoch->  394\n",
      "Iteration-> 0  , Iteration_time-> 1.2065 , Iteration_loss 1.4258\n",
      "Epoch 394 , average loss 1.4258432388305664 , epoch_time 1.2066740989685059\n",
      "\n",
      "epoch->  395\n",
      "Iteration-> 0  , Iteration_time-> 1.2011 , Iteration_loss 1.4227\n",
      "Epoch 395 , average loss 1.422664761543274 , epoch_time 1.2012584209442139\n",
      "\n",
      "epoch->  396\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs_gat):\n",
    "    print(\"\\nepoch-> \", epoch)\n",
    "    random.shuffle(Corpus_.train_triples)\n",
    "    Corpus_.train_indices = np.array(\n",
    "        list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "    model_gat.train()  # getting in training mode\n",
    "    start_time = time.time()\n",
    "    epoch_loss = []\n",
    "\n",
    "    if len(Corpus_.train_indices) % args.batch_size_gat == 0:\n",
    "        num_iters_per_epoch = len(\n",
    "            Corpus_.train_indices) // args.batch_size_gat\n",
    "    else:\n",
    "        num_iters_per_epoch = (\n",
    "            len(Corpus_.train_indices) // args.batch_size_gat) + 1\n",
    "\n",
    "    for iters in range(num_iters_per_epoch):\n",
    "        start_time_iter = time.time()\n",
    "        train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "        if CUDA:\n",
    "            train_indices = Variable(\n",
    "                torch.LongTensor(train_indices)).cuda()\n",
    "            train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "        else:\n",
    "            train_indices = Variable(torch.LongTensor(train_indices))\n",
    "            train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "        # forward pass\n",
    "        entity_embed, relation_embed = model_gat(\n",
    "            Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = batch_gat_loss(\n",
    "            gat_loss_func, train_indices, entity_embed, relation_embed)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.data.item())\n",
    "\n",
    "        end_time_iter = time.time()\n",
    "\n",
    "        print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\n",
    "            iters, end_time_iter - start_time_iter, loss.data.item()))\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Epoch {} , average loss {} , epoch_time {}\".format(\n",
    "        epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\n",
    "    epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "\n",
    "    if (epoch % 100 == 0):\n",
    "        save_model(model_gat, args.data, epoch,args.output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHN3vmlRbRRf"
   },
   "outputs": [],
   "source": [
    "torch.save(model_gat.state_dict(), \"gat.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RD9n-7Q-beBU",
    "outputId": "ffaaa4f1-e459-4cb3-b82e-135e873568b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "RqMSuugCxTJU",
    "outputId": "9eb8f89f-f08a-4290-cf4b-236e729f0a3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d82940ca97e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Defining model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n\u001b[0m\u001b[1;32m      6\u001b[0m                             args.drop_GAT, args.alpha, args.nheads_GAT)\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only Conv model trained\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'entity_embeddings' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating convolution model here.\n",
    "    ####################################\n",
    "\n",
    "print(\"Defining model\")\n",
    "model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                            args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "print(\"Only Conv model trained\")\n",
    "model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                              args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                              args.nheads_GAT, args.out_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InVqT_jQF3Lq"
   },
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    model_conv.cuda()\n",
    "    model_gat.cuda()\n",
    "\n",
    "model_gat.load_state_dict(torch.load(\n",
    "    '{}/trained_{}.pth'.format(args.output_folder, args.epochs_gat - 1)), strict=False)\n",
    "model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "Corpus_.batch_size = args.batch_size_conv\n",
    "Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "epoch_losses = []   # losses of all epochs\n",
    "print(\"Number of epochs {}\".format(args.epochs_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "1XnMK4kAzTqB",
    "outputId": "8f1ee640-af25-45e6-a74b-136c6c1c93f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch->  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3acd0a34a9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             list(Corpus_.train_triples)).astype(np.int32)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# getting in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_conv' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs_conv):\n",
    "        print(\"\\nepoch-> \", epoch)\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_conv.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_conv == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_conv\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_conv) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            preds = model_conv(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = margin_loss(preds.view(-1), train_values.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "            print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\n",
    "                iters, end_time_iter - start_time_iter, loss.data.item()))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\"Epoch {} , average loss {} , epoch_time {}\".format(\n",
    "            epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_DrHllJzWze"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "yswke-BDzeV7",
    "outputId": "f0ced3c0-3115-4916-aabc-27ef25395609"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-cc5482ee7137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_GAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  args.nheads_GAT, args.out_channels)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gat.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gat.pth'"
     ]
    }
   ],
   "source": [
    "unique_entities = Corpus_.unique_entities_train\n",
    "model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "model_conv.load_state_dict(torch.load('gat.pth'), strict=False)\n",
    "\n",
    "model_conv.cuda()\n",
    "model_conv.eval()\n",
    "with torch.no_grad():\n",
    "    Corpus_.get_validation_pred(args, model_conv, unique_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 163
    },
    "colab_type": "code",
    "id": "AzfNiXGcF-5h",
    "outputId": "240eff89-23f9-4b72-ece2-5bfd2a685055"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-4ede8efc2f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCorpus_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_entities_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate_conv' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_conv(args, Corpus_.unique_entities_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LsqGX8eRfje"
   },
   "outputs": [],
   "source": [
    "def train_conv(args):\n",
    "\n",
    "    # Creating convolution model here.\n",
    "    ####################################\n",
    "\n",
    "    print(\"Defining model\")\n",
    "    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "    print(\"Only Conv model trained\")\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "\n",
    "    if CUDA:\n",
    "        model_conv.cuda()\n",
    "        model_gat.cuda()\n",
    "\n",
    "    model_gat.load_state_dict(torch.load(\"gat.pth\"), strict=False)\n",
    "    model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "    model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "    Corpus_.batch_size = args.batch_size_conv\n",
    "    Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "    margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "    epoch_losses = []   # losses of all epochs\n",
    "    print(\"Number of epochs {}\".format(args.epochs_conv))\n",
    "\n",
    "    for epoch in range(args.epochs_conv):\n",
    "        print(\"\\nepoch-> \", epoch)\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_conv.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_conv == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_conv\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_conv) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            preds = model_conv(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = margin_loss(preds.view(-1), train_values.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "        scheduler.step()\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "\n",
    "        if (epoch == 49 or epoch == 99 or epoch == 149 or epoch == 199):\n",
    "            save_model(model_conv, args.data, epoch,\n",
    "                      args.output_folder + \"conv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "T1JaayJcRsTO",
    "outputId": "0b321ee1-3d50-453b-9ce1-4f5c2516569e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "Only Conv model trained\n",
      "Number of epochs 200\n",
      "\n",
      "epoch->  0\n",
      "\n",
      "epoch->  1\n",
      "\n",
      "epoch->  2\n",
      "\n",
      "epoch->  3\n",
      "\n",
      "epoch->  4\n",
      "\n",
      "epoch->  5\n",
      "\n",
      "epoch->  6\n",
      "\n",
      "epoch->  7\n",
      "\n",
      "epoch->  8\n",
      "\n",
      "epoch->  9\n",
      "\n",
      "epoch->  10\n",
      "\n",
      "epoch->  11\n",
      "\n",
      "epoch->  12\n",
      "\n",
      "epoch->  13\n",
      "\n",
      "epoch->  14\n",
      "\n",
      "epoch->  15\n",
      "\n",
      "epoch->  16\n",
      "\n",
      "epoch->  17\n",
      "\n",
      "epoch->  18\n",
      "\n",
      "epoch->  19\n",
      "\n",
      "epoch->  20\n",
      "\n",
      "epoch->  21\n",
      "\n",
      "epoch->  22\n",
      "\n",
      "epoch->  23\n",
      "\n",
      "epoch->  24\n",
      "\n",
      "epoch->  25\n",
      "\n",
      "epoch->  26\n",
      "\n",
      "epoch->  27\n",
      "\n",
      "epoch->  28\n",
      "\n",
      "epoch->  29\n",
      "\n",
      "epoch->  30\n",
      "\n",
      "epoch->  31\n",
      "\n",
      "epoch->  32\n",
      "\n",
      "epoch->  33\n",
      "\n",
      "epoch->  34\n",
      "\n",
      "epoch->  35\n",
      "\n",
      "epoch->  36\n",
      "\n",
      "epoch->  37\n",
      "\n",
      "epoch->  38\n",
      "\n",
      "epoch->  39\n",
      "\n",
      "epoch->  40\n",
      "\n",
      "epoch->  41\n",
      "\n",
      "epoch->  42\n",
      "\n",
      "epoch->  43\n",
      "\n",
      "epoch->  44\n",
      "\n",
      "epoch->  45\n",
      "\n",
      "epoch->  46\n",
      "\n",
      "epoch->  47\n",
      "\n",
      "epoch->  48\n",
      "\n",
      "epoch->  49\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  50\n",
      "\n",
      "epoch->  51\n",
      "\n",
      "epoch->  52\n",
      "\n",
      "epoch->  53\n",
      "\n",
      "epoch->  54\n",
      "\n",
      "epoch->  55\n",
      "\n",
      "epoch->  56\n",
      "\n",
      "epoch->  57\n",
      "\n",
      "epoch->  58\n",
      "\n",
      "epoch->  59\n",
      "\n",
      "epoch->  60\n",
      "\n",
      "epoch->  61\n",
      "\n",
      "epoch->  62\n",
      "\n",
      "epoch->  63\n",
      "\n",
      "epoch->  64\n",
      "\n",
      "epoch->  65\n",
      "\n",
      "epoch->  66\n",
      "\n",
      "epoch->  67\n",
      "\n",
      "epoch->  68\n",
      "\n",
      "epoch->  69\n",
      "\n",
      "epoch->  70\n",
      "\n",
      "epoch->  71\n",
      "\n",
      "epoch->  72\n",
      "\n",
      "epoch->  73\n",
      "\n",
      "epoch->  74\n",
      "\n",
      "epoch->  75\n",
      "\n",
      "epoch->  76\n",
      "\n",
      "epoch->  77\n",
      "\n",
      "epoch->  78\n",
      "\n",
      "epoch->  79\n",
      "\n",
      "epoch->  80\n",
      "\n",
      "epoch->  81\n",
      "\n",
      "epoch->  82\n",
      "\n",
      "epoch->  83\n",
      "\n",
      "epoch->  84\n"
     ]
    }
   ],
   "source": [
    "train_conv(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhmLex6-SVTE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svEWQ6mgRCl5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.random.randn(10, 10)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_TVFncS2w5j"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCVi58UB2xzx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "_yz1pZuiC_aG"
   ],
   "machine_shape": "hm",
   "name": "KBAT_Embedded_Chính thức.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
