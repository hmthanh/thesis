{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mục tiêu : Đọc và xây dựng chuyển thành input đầu vào\n",
    "\n",
    "* Fb15k\n",
    "* Fb15k-237\n",
    "* WN18\n",
    "* WN18RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_id(filename='../dataset/WN18RR/entity2id.txt'):\n",
    "    entity2id = {}\n",
    "    id2entity = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                tmp = line.strip().split()\n",
    "                entity2id[tmp[0]] = int(tmp[1])\n",
    "                id2entity[int(tmp[1])] = tmp[0]\n",
    "    return entity2id, id2entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_norm_Vector(relinit, entinit, embedding_size):\n",
    "    lstent = []\n",
    "    lstrel = []\n",
    "    with open(relinit) as f:\n",
    "        for line in f:\n",
    "            tmp = [float(val) for val in line.strip().split()]\n",
    "            # if np.linalg.norm(tmp) > 1:\n",
    "            #     tmp = tmp / np.linalg.norm(tmp)\n",
    "            lstrel.append(tmp)\n",
    "    with open(entinit) as f:\n",
    "        for line in f:\n",
    "            tmp = [float(val) for val in line.strip().split()]\n",
    "            # if np.linalg.norm(tmp) > 1:\n",
    "            #     tmp = tmp / np.linalg.norm(tmp)\n",
    "            lstent.append(tmp)\n",
    "    assert embedding_size % len(lstent[0]) == 0\n",
    "    return np.array(lstent, dtype=np.float32), np.array(lstrel, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getID(folder='data/WN18RR/'):\n",
    "    lstEnts = {}\n",
    "    lstRels = {}\n",
    "    with open(folder + 'train.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "\n",
    "    with open(folder + 'valid.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "\n",
    "    with open(folder + 'test.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "\n",
    "    wri = open(folder + 'entity2id.txt', 'w')\n",
    "    for entity in lstEnts:\n",
    "        wri.write(entity + '\\t' + str(lstEnts[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()\n",
    "\n",
    "    wri = open(folder + 'relation2id.txt', 'w')\n",
    "    for entity in lstRels:\n",
    "        wri.write(entity + '\\t' + str(lstRels[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    line = line.strip().split()\n",
    "    sub = line[0]\n",
    "    rel = line[1]\n",
    "    obj = line[2]\n",
    "    val = [1]\n",
    "    if len(line) > 3:\n",
    "        if line[3] == '-1':\n",
    "            val = [-1]\n",
    "    return sub, obj, rel, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triples_from_txt(filename, words_indexes=None, parse_line=parse_line):\n",
    "    \"\"\"\n",
    "    Take a list of file names and build the corresponding dictionnary of triples\n",
    "    \"\"\"\n",
    "    if words_indexes == None:\n",
    "        words_indexes = dict()\n",
    "        entities = set()\n",
    "        next_ent = 0\n",
    "    else:\n",
    "        entities = set(words_indexes)\n",
    "        next_ent = max(words_indexes.values()) + 1\n",
    "\n",
    "    data = dict()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for _, line in enumerate(lines):\n",
    "        sub, obj, rel, val = parse_line(line)\n",
    "\n",
    "        if sub in entities:\n",
    "            sub_ind = words_indexes[sub]\n",
    "        else:\n",
    "            sub_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[sub] = sub_ind\n",
    "            entities.add(sub)\n",
    "\n",
    "        if rel in entities:\n",
    "            rel_ind = words_indexes[rel]\n",
    "        else:\n",
    "            rel_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[rel] = rel_ind\n",
    "            entities.add(rel)\n",
    "\n",
    "        if obj in entities:\n",
    "            obj_ind = words_indexes[obj]\n",
    "        else:\n",
    "            obj_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[obj] = obj_ind\n",
    "            entities.add(obj)\n",
    "\n",
    "        data[(sub_ind, rel_ind, obj_ind)] = val\n",
    "\n",
    "    indexes_words = {}\n",
    "    for tmpkey in words_indexes:\n",
    "        indexes_words[words_indexes[tmpkey]] = tmpkey\n",
    "\n",
    "    return data, words_indexes, indexes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triples_from_txt(filename, words_indexes=None, parse_line=parse_line):\n",
    "    \"\"\"\n",
    "    Take a list of file names and build the corresponding dictionnary of triples\n",
    "    \"\"\"\n",
    "    if words_indexes == None:\n",
    "        words_indexes = dict()\n",
    "        entities = set()\n",
    "        next_ent = 0\n",
    "    else:\n",
    "        entities = set(words_indexes)\n",
    "        next_ent = max(words_indexes.values()) + 1\n",
    "\n",
    "    data = dict()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for _, line in enumerate(lines):\n",
    "        sub, obj, rel, val = parse_line(line)\n",
    "\n",
    "        if sub in entities:\n",
    "            sub_ind = words_indexes[sub]\n",
    "        else:\n",
    "            sub_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[sub] = sub_ind\n",
    "            entities.add(sub)\n",
    "\n",
    "        if rel in entities:\n",
    "            rel_ind = words_indexes[rel]\n",
    "        else:\n",
    "            rel_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[rel] = rel_ind\n",
    "            entities.add(rel)\n",
    "\n",
    "        if obj in entities:\n",
    "            obj_ind = words_indexes[obj]\n",
    "        else:\n",
    "            obj_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[obj] = obj_ind\n",
    "            entities.add(obj)\n",
    "\n",
    "        data[(sub_ind, rel_ind, obj_ind)] = val\n",
    "\n",
    "    indexes_words = {}\n",
    "    for tmpkey in words_indexes:\n",
    "        indexes_words[words_indexes[tmpkey]] = tmpkey\n",
    "\n",
    "    return data, words_indexes, indexes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_of_chars(words_indexes):\n",
    "    lstChars = {}\n",
    "    for word in words_indexes:\n",
    "        for char in word:\n",
    "            if char not in lstChars:\n",
    "                lstChars[char] = len(lstChars)\n",
    "    lstChars['unk'] = len(lstChars)\n",
    "    return lstChars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seq_chars(x_batch, lstChars, indexes_words):\n",
    "    lst = []\n",
    "    for [tmpH, tmpR, tmpT] in x_batch:\n",
    "        wH = [lstChars[tmp] for tmp in indexes_words[tmpH]]\n",
    "        wR = [lstChars[tmp] for tmp in indexes_words[tmpR]]\n",
    "        wT = [lstChars[tmp] for tmp in indexes_words[tmpT]]\n",
    "        lst.append([wH, wR, wT])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    sequence_padded, sequence_length = [], []\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok] * max(max_length - len(seq), 0)\n",
    "        sequence_padded += [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_tok):\n",
    "    sequence_padded, sequence_length = [], []\n",
    "    max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                           for seq in sequences])\n",
    "    for seq in sequences:\n",
    "        # all words are same length now\n",
    "        sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "        sequence_padded += [sp]\n",
    "        sequence_length += [sl]\n",
    "\n",
    "    max_length_sentence = max(map(lambda x: len(x), sequences))\n",
    "    sequence_padded, _ = _pad_sequences(sequence_padded, [pad_tok] * max_length_word, max_length_sentence)\n",
    "    sequence_length, _ = _pad_sequences(sequence_length, 0, max_length_sentence)\n",
    "\n",
    "    return np.array(sequence_padded).astype(np.int32), np.array(sequence_length).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(name='WN18', path='../data'):\n",
    "    folder = path + '/' + name + '/'\n",
    "\n",
    "    train_triples, words_indexes, _ = load_triples_from_txt(folder + 'train.txt', parse_line=parse_line)\n",
    "\n",
    "    valid_triples, words_indexes, _ = load_triples_from_txt(folder + 'valid.txt',\n",
    "                                                            words_indexes=words_indexes, parse_line=parse_line)\n",
    "\n",
    "    test_triples, words_indexes, indexes_words = load_triples_from_txt(folder + 'test.txt',\n",
    "                                                                       words_indexes=words_indexes,\n",
    "                                                                       parse_line=parse_line)\n",
    "\n",
    "    entity2id, id2entity = read_from_id(folder + '/entity2id.txt')\n",
    "    relation2id, id2relation = read_from_id(folder + '/relation2id.txt')\n",
    "    left_entity = {}\n",
    "    right_entity = {}\n",
    "\n",
    "    with open(folder + 'train.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    for _, line in enumerate(lines):\n",
    "        head, tail, rel, val = parse_line(line)\n",
    "        # count the number of occurrences for each (heal, rel)\n",
    "        if relation2id[rel] not in left_entity:\n",
    "            left_entity[relation2id[rel]] = {}\n",
    "        if entity2id[head] not in left_entity[relation2id[rel]]:\n",
    "            left_entity[relation2id[rel]][entity2id[head]] = 0\n",
    "        left_entity[relation2id[rel]][entity2id[head]] += 1\n",
    "        # count the number of occurrences for each (rel, tail)\n",
    "        if relation2id[rel] not in right_entity:\n",
    "            right_entity[relation2id[rel]] = {}\n",
    "        if entity2id[tail] not in right_entity[relation2id[rel]]:\n",
    "            right_entity[relation2id[rel]][entity2id[tail]] = 0\n",
    "        right_entity[relation2id[rel]][entity2id[tail]] += 1\n",
    "\n",
    "    left_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        left_avg[i] = sum(left_entity[i].values()) * 1.0 / len(left_entity[i])\n",
    "\n",
    "    right_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        right_avg[i] = sum(right_entity[i].values()) * 1.0 / len(right_entity[i])\n",
    "\n",
    "    headTailSelector = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        headTailSelector[i] = 1000 * right_avg[i] / (right_avg[i] + left_avg[i])\n",
    "\n",
    "    return train_triples, valid_triples, test_triples, words_indexes, indexes_words, headTailSelector, entity2id, id2entity, relation2id, id2relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-657a3d4e20c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbuild_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'build_data' is not defined"
     ]
    }
   ],
   "source": [
    "build_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1],\n",
       "         [2],\n",
       "         [3]],\n",
       "\n",
       "        [[4],\n",
       "         [5],\n",
       "         [6]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.unsqueeze(-1)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-f2d4c0d9b846>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-18-f2d4c0d9b846>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    [[434, 6, 0, 20151], [434, 6, 1, 27022], [434, 1, 3, 6756], [434, 1, 1, 9701], [434, 1, 3, 25215], [434, 1, 3, 14514], [434, 1, 1, 17355], [434, 1, 1, 4678], [434, 1, 0, 22969], [434, 0, 0, 8093], [102, 1, 1, 8198], [102, 1, 9, 3513], [102, 1, 9, 6835], [102, 0, 0, 28374], [1015, 1, 1, 9837], [1015, 1, 1, 8193], [1015, 9, 1, 9709], [1015, 9, 1, 24451], [1015, 9, 0, 30263], [1015, 5, 5, 3276], [1015, 5, 0, 24555], [1015, 5, 1, 18987], [1015, 5, 1, 12226], [1015, 0, 0, 1856], [1334, 0, 0, 21207], [1170, 1, 0, 2021], [1170, 0, 1, 5746], [1170, 0, 1, 3407], [1170, 0, 1, 15941], [1170, 0, 3, 17561], [1170, 9, 0, 13432], [1170, 5, 0, 2340], [1170, 5, 1, 432], [551, 1, 1, 1750], [551, 1, 1, 1650], [551, 1, 1, 57], [593, 1, 1, 14724], [593, 1, 1, 15194], [593, 1, 0, 3538], [593, 1, 5, 212], [593, 0, 0, 20441], [878, 1, 10, 15612], [878, 1, 6, 12894], [878, 1, 1, 21993], [878, 1, 1, 7965], [878, 1, 6, 25573], [878, 1, 1, 1353], [878, 1, 0, 267], [878, 1, 1, 1354], [878, 1, 0, 15983], [878, 1, 1, 12156], [878, 1, 1, 4130], [878, 1, 1, 3621], [878, 1, 1, 16831], [878, 1, 0, 6991], [878, 1, 1, 4331], [878, 1, 1, 23169], [878, 0, 1, 5550], [878, 0, 1, 18401], [878, 0, 1, 9048], [878, 0, 1, 6526], [878, 0, 1, 411], [878, 0, 1, 23994], [878, 0, 1, 26005], [878, 1, 0, 5224], [1745, 0, 0, 9550], [1745, 0, 0, 425], [68, 1, 0, 25065], [68, 3, 1, 16639], [68, 3, 1, 2060], [68, 3, 3, 4087], [68, 3, 3, 16968], [68, 3, 3, 2432], [68, 3, 3, 4128], [68, 3, 0, 17329], [68, 3, 1, 31988], [68, 3, 1, 2185], [68, 3, 3, 12153], [392, 3, 1, 8768], [392, 3, 3, 14552], [392, 3, 1, 3825], [392, 3, 10, 5770], [392, 3, 1, 25461], [392, 3, 3, 28623], [392, 3, 1, 15110], [392, 3, 3, 13343], [392, 1, 0, 33642], [392, 1, 1, 10644], [392, 3, 3, 1388], [392, 3, 3, 10720], [392, 3, 3, 14121], [392, 3, 1, 3901], [689, 1, 1, 6041], [689, 1, 4, 608], [689, 1, 0, 11666], [689, 1, 4, 24498], [689, 0, 1, 12211], [689, 0, 1, 838], [689, 0, 3, 25215], [689, 0, 0, 12252...\u001b[0m\n\u001b[1;37m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[[434, 6, 0, 20151], [434, 6, 1, 27022], [434, 1, 3, 6756], [434, 1, 1, 9701], [434, 1, 3, 25215], [434, 1, 3, 14514], [434, 1, 1, 17355], [434, 1, 1, 4678], [434, 1, 0, 22969], [434, 0, 0, 8093], [102, 1, 1, 8198], [102, 1, 9, 3513], [102, 1, 9, 6835], [102, 0, 0, 28374], [1015, 1, 1, 9837], [1015, 1, 1, 8193], [1015, 9, 1, 9709], [1015, 9, 1, 24451], [1015, 9, 0, 30263], [1015, 5, 5, 3276], [1015, 5, 0, 24555], [1015, 5, 1, 18987], [1015, 5, 1, 12226], [1015, 0, 0, 1856], [1334, 0, 0, 21207], [1170, 1, 0, 2021], [1170, 0, 1, 5746], [1170, 0, 1, 3407], [1170, 0, 1, 15941], [1170, 0, 3, 17561], [1170, 9, 0, 13432], [1170, 5, 0, 2340], [1170, 5, 1, 432], [551, 1, 1, 1750], [551, 1, 1, 1650], [551, 1, 1, 57], [593, 1, 1, 14724], [593, 1, 1, 15194], [593, 1, 0, 3538], [593, 1, 5, 212], [593, 0, 0, 20441], [878, 1, 10, 15612], [878, 1, 6, 12894], [878, 1, 1, 21993], [878, 1, 1, 7965], [878, 1, 6, 25573], [878, 1, 1, 1353], [878, 1, 0, 267], [878, 1, 1, 1354], [878, 1, 0, 15983], [878, 1, 1, 12156], [878, 1, 1, 4130], [878, 1, 1, 3621], [878, 1, 1, 16831], [878, 1, 0, 6991], [878, 1, 1, 4331], [878, 1, 1, 23169], [878, 0, 1, 5550], [878, 0, 1, 18401], [878, 0, 1, 9048], [878, 0, 1, 6526], [878, 0, 1, 411], [878, 0, 1, 23994], [878, 0, 1, 26005], [878, 1, 0, 5224], [1745, 0, 0, 9550], [1745, 0, 0, 425], [68, 1, 0, 25065], [68, 3, 1, 16639], [68, 3, 1, 2060], [68, 3, 3, 4087], [68, 3, 3, 16968], [68, 3, 3, 2432], [68, 3, 3, 4128], [68, 3, 0, 17329], [68, 3, 1, 31988], [68, 3, 1, 2185], [68, 3, 3, 12153], [392, 3, 1, 8768], [392, 3, 3, 14552], [392, 3, 1, 3825], [392, 3, 10, 5770], [392, 3, 1, 25461], [392, 3, 3, 28623], [392, 3, 1, 15110], [392, 3, 3, 13343], [392, 1, 0, 33642], [392, 1, 1, 10644], [392, 3, 3, 1388], [392, 3, 3, 10720], [392, 3, 3, 14121], [392, 3, 1, 3901], [689, 1, 1, 6041], [689, 1, 4, 608], [689, 1, 0, 11666], [689, 1, 4, 24498], [689, 0, 1, 12211], [689, 0, 1, 838], [689, 0, 3, 25215], [689, 0, 0, 12252..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.tensor([1,2,3])\n",
    "b = torch.tensor([2,3,4])\n",
    "\n",
    "class A:\n",
    "    def funcA(a, b):\n",
    "        c = torch.tensor([0,0,0])\n",
    "        c = a + b\n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "funcA(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = funcA(a,b)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
