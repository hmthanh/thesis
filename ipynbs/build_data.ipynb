{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mục tiêu : Đọc và xây dựng chuyển thành input đầu vào\n",
    "\n",
    "* Fb15k\n",
    "* Fb15k-237\n",
    "* WN18\n",
    "* WN18RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_from_id(filename='../dataset/WN18RR/entity2id.txt'):\n",
    "    entity2id = {}\n",
    "    id2entity = {}\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                tmp = line.strip().split()\n",
    "                entity2id[tmp[0]] = int(tmp[1])\n",
    "                id2entity[int(tmp[1])] = tmp[0]\n",
    "    return entity2id, id2entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_norm_Vector(relinit, entinit, embedding_size):\n",
    "    lstent = []\n",
    "    lstrel = []\n",
    "    with open(relinit) as f:\n",
    "        for line in f:\n",
    "            tmp = [float(val) for val in line.strip().split()]\n",
    "            # if np.linalg.norm(tmp) > 1:\n",
    "            #     tmp = tmp / np.linalg.norm(tmp)\n",
    "            lstrel.append(tmp)\n",
    "    with open(entinit) as f:\n",
    "        for line in f:\n",
    "            tmp = [float(val) for val in line.strip().split()]\n",
    "            # if np.linalg.norm(tmp) > 1:\n",
    "            #     tmp = tmp / np.linalg.norm(tmp)\n",
    "            lstent.append(tmp)\n",
    "    assert embedding_size % len(lstent[0]) == 0\n",
    "    return np.array(lstent, dtype=np.float32), np.array(lstrel, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getID(folder='data/WN18RR/'):\n",
    "    lstEnts = {}\n",
    "    lstRels = {}\n",
    "    with open(folder + 'train.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "\n",
    "    with open(folder + 'valid.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "\n",
    "    with open(folder + 'test.txt') as f:\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "\n",
    "    wri = open(folder + 'entity2id.txt', 'w')\n",
    "    for entity in lstEnts:\n",
    "        wri.write(entity + '\\t' + str(lstEnts[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()\n",
    "\n",
    "    wri = open(folder + 'relation2id.txt', 'w')\n",
    "    for entity in lstRels:\n",
    "        wri.write(entity + '\\t' + str(lstRels[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line):\n",
    "    line = line.strip().split()\n",
    "    sub = line[0]\n",
    "    rel = line[1]\n",
    "    obj = line[2]\n",
    "    val = [1]\n",
    "    if len(line) > 3:\n",
    "        if line[3] == '-1':\n",
    "            val = [-1]\n",
    "    return sub, obj, rel, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triples_from_txt(filename, words_indexes=None, parse_line=parse_line):\n",
    "    \"\"\"\n",
    "    Take a list of file names and build the corresponding dictionnary of triples\n",
    "    \"\"\"\n",
    "    if words_indexes == None:\n",
    "        words_indexes = dict()\n",
    "        entities = set()\n",
    "        next_ent = 0\n",
    "    else:\n",
    "        entities = set(words_indexes)\n",
    "        next_ent = max(words_indexes.values()) + 1\n",
    "\n",
    "    data = dict()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for _, line in enumerate(lines):\n",
    "        sub, obj, rel, val = parse_line(line)\n",
    "\n",
    "        if sub in entities:\n",
    "            sub_ind = words_indexes[sub]\n",
    "        else:\n",
    "            sub_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[sub] = sub_ind\n",
    "            entities.add(sub)\n",
    "\n",
    "        if rel in entities:\n",
    "            rel_ind = words_indexes[rel]\n",
    "        else:\n",
    "            rel_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[rel] = rel_ind\n",
    "            entities.add(rel)\n",
    "\n",
    "        if obj in entities:\n",
    "            obj_ind = words_indexes[obj]\n",
    "        else:\n",
    "            obj_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[obj] = obj_ind\n",
    "            entities.add(obj)\n",
    "\n",
    "        data[(sub_ind, rel_ind, obj_ind)] = val\n",
    "\n",
    "    indexes_words = {}\n",
    "    for tmpkey in words_indexes:\n",
    "        indexes_words[words_indexes[tmpkey]] = tmpkey\n",
    "\n",
    "    return data, words_indexes, indexes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_triples_from_txt(filename, words_indexes=None, parse_line=parse_line):\n",
    "    \"\"\"\n",
    "    Take a list of file names and build the corresponding dictionnary of triples\n",
    "    \"\"\"\n",
    "    if words_indexes == None:\n",
    "        words_indexes = dict()\n",
    "        entities = set()\n",
    "        next_ent = 0\n",
    "    else:\n",
    "        entities = set(words_indexes)\n",
    "        next_ent = max(words_indexes.values()) + 1\n",
    "\n",
    "    data = dict()\n",
    "\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for _, line in enumerate(lines):\n",
    "        sub, obj, rel, val = parse_line(line)\n",
    "\n",
    "        if sub in entities:\n",
    "            sub_ind = words_indexes[sub]\n",
    "        else:\n",
    "            sub_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[sub] = sub_ind\n",
    "            entities.add(sub)\n",
    "\n",
    "        if rel in entities:\n",
    "            rel_ind = words_indexes[rel]\n",
    "        else:\n",
    "            rel_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[rel] = rel_ind\n",
    "            entities.add(rel)\n",
    "\n",
    "        if obj in entities:\n",
    "            obj_ind = words_indexes[obj]\n",
    "        else:\n",
    "            obj_ind = next_ent\n",
    "            next_ent += 1\n",
    "            words_indexes[obj] = obj_ind\n",
    "            entities.add(obj)\n",
    "\n",
    "        data[(sub_ind, rel_ind, obj_ind)] = val\n",
    "\n",
    "    indexes_words = {}\n",
    "    for tmpkey in words_indexes:\n",
    "        indexes_words[words_indexes[tmpkey]] = tmpkey\n",
    "\n",
    "    return data, words_indexes, indexes_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dic_of_chars(words_indexes):\n",
    "    lstChars = {}\n",
    "    for word in words_indexes:\n",
    "        for char in word:\n",
    "            if char not in lstChars:\n",
    "                lstChars[char] = len(lstChars)\n",
    "    lstChars['unk'] = len(lstChars)\n",
    "    return lstChars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_seq_chars(x_batch, lstChars, indexes_words):\n",
    "    lst = []\n",
    "    for [tmpH, tmpR, tmpT] in x_batch:\n",
    "        wH = [lstChars[tmp] for tmp in indexes_words[tmpH]]\n",
    "        wR = [lstChars[tmp] for tmp in indexes_words[tmpR]]\n",
    "        wT = [lstChars[tmp] for tmp in indexes_words[tmpT]]\n",
    "        lst.append([wH, wR, wT])\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_sequences(sequences, pad_tok, max_length):\n",
    "    sequence_padded, sequence_length = [], []\n",
    "    for seq in sequences:\n",
    "        seq = list(seq)\n",
    "        seq_ = seq[:max_length] + [pad_tok] * max(max_length - len(seq), 0)\n",
    "        sequence_padded += [seq_]\n",
    "        sequence_length += [min(len(seq), max_length)]\n",
    "\n",
    "    return sequence_padded, sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequences(sequences, pad_tok):\n",
    "    sequence_padded, sequence_length = [], []\n",
    "    max_length_word = max([max(map(lambda x: len(x), seq))\n",
    "                           for seq in sequences])\n",
    "    for seq in sequences:\n",
    "        # all words are same length now\n",
    "        sp, sl = _pad_sequences(seq, pad_tok, max_length_word)\n",
    "        sequence_padded += [sp]\n",
    "        sequence_length += [sl]\n",
    "\n",
    "    max_length_sentence = max(map(lambda x: len(x), sequences))\n",
    "    sequence_padded, _ = _pad_sequences(sequence_padded, [pad_tok] * max_length_word, max_length_sentence)\n",
    "    sequence_length, _ = _pad_sequences(sequence_length, 0, max_length_sentence)\n",
    "\n",
    "    return np.array(sequence_padded).astype(np.int32), np.array(sequence_length).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(name='WN18', path='../data'):\n",
    "    folder = path + '/' + name + '/'\n",
    "\n",
    "    train_triples, words_indexes, _ = load_triples_from_txt(folder + 'train.txt', parse_line=parse_line)\n",
    "\n",
    "    valid_triples, words_indexes, _ = load_triples_from_txt(folder + 'valid.txt',\n",
    "                                                            words_indexes=words_indexes, parse_line=parse_line)\n",
    "\n",
    "    test_triples, words_indexes, indexes_words = load_triples_from_txt(folder + 'test.txt',\n",
    "                                                                       words_indexes=words_indexes,\n",
    "                                                                       parse_line=parse_line)\n",
    "\n",
    "    entity2id, id2entity = read_from_id(folder + '/entity2id.txt')\n",
    "    relation2id, id2relation = read_from_id(folder + '/relation2id.txt')\n",
    "    left_entity = {}\n",
    "    right_entity = {}\n",
    "\n",
    "    with open(folder + 'train.txt') as f:\n",
    "        lines = f.readlines()\n",
    "    for _, line in enumerate(lines):\n",
    "        head, tail, rel, val = parse_line(line)\n",
    "        # count the number of occurrences for each (heal, rel)\n",
    "        if relation2id[rel] not in left_entity:\n",
    "            left_entity[relation2id[rel]] = {}\n",
    "        if entity2id[head] not in left_entity[relation2id[rel]]:\n",
    "            left_entity[relation2id[rel]][entity2id[head]] = 0\n",
    "        left_entity[relation2id[rel]][entity2id[head]] += 1\n",
    "        # count the number of occurrences for each (rel, tail)\n",
    "        if relation2id[rel] not in right_entity:\n",
    "            right_entity[relation2id[rel]] = {}\n",
    "        if entity2id[tail] not in right_entity[relation2id[rel]]:\n",
    "            right_entity[relation2id[rel]][entity2id[tail]] = 0\n",
    "        right_entity[relation2id[rel]][entity2id[tail]] += 1\n",
    "\n",
    "    left_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        left_avg[i] = sum(left_entity[i].values()) * 1.0 / len(left_entity[i])\n",
    "\n",
    "    right_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        right_avg[i] = sum(right_entity[i].values()) * 1.0 / len(right_entity[i])\n",
    "\n",
    "    headTailSelector = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        headTailSelector[i] = 1000 * right_avg[i] / (right_avg[i] + left_avg[i])\n",
    "\n",
    "    return train_triples, valid_triples, test_triples, words_indexes, indexes_words, headTailSelector, entity2id, id2entity, relation2id, id2relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
