{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dVe3gXdlAKiX"
   },
   "source": [
    "# Learning Attention-based Embeddings for Relation Prediction in Knowledge Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUf0F7XsVXxi"
   },
   "source": [
    "# Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PRm8BTekEnGi"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "from copy import deepcopy\n",
    "from torchsummary import summary\n",
    "\n",
    "import random\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from collections import defaultdict\n",
    "import queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8t0zLh2cEjuy"
   },
   "outputs": [],
   "source": [
    "class Args:\n",
    "     # network arguments\n",
    "    data = \"./data/WN18RR/\"\n",
    "    epochs_gat = 3600\n",
    "    epochs_conv = 200\n",
    "    weight_decay_gat = float(5e-6)\n",
    "    weight_decay_conv = float(1e-5)\n",
    "    pretrained_emb = True\n",
    "    embedding_size = 50\n",
    "    lr = float(1e-3)\n",
    "    get_2hop = True\n",
    "    use_2hop = True\n",
    "    partial_2hop = False\n",
    "    output_folder = \"./checkpoints/wn/out/\"\n",
    "\n",
    "    # arguments for GAT\n",
    "    batch_size_gat = 86835\n",
    "    # Tỷ lệ của tập valid so với tập invalid trong khi training GAT\n",
    "    valid_invalid_ratio_gat = 2\n",
    "    drop_GAT = 0.3  # Tỷ lệ dropout của lớp SpGAT\n",
    "    alpha = 0.2  # LeakyRelu alphs for SpGAT layer\n",
    "    entity_out_dim = [100, 200]  # Miền nhúng của đầu ra output\n",
    "    nheads_GAT = [2, 2]  # Multihead attention SpGAT\n",
    "    # Margin used in hinge loss ( Sử dụng margin trong hinge (khớp nối))\n",
    "    margin = 5\n",
    "\n",
    "    # arguments for convolution network\n",
    "    batch_size_conv = 128  # Batch size for conv\n",
    "    alpha_conv = 0.2  # LeakyRelu alphas for conv layer\n",
    "    # Ratio of valid to invalid triples for convolution training\n",
    "    valid_invalid_ratio_conv = 40\n",
    "    out_channels = 500  # Số lượng output channels trong lớp conv\n",
    "    drop_conv = 0.0  # Xắc xuất dropout cho lớp convolution\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "KaJDECMUbnL_",
    "outputId": "12c2a559-3588-42ca-8720-0be0e7771e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2019 NVIDIA Corporation\n",
      "Built on Sun_Jul_28_19:07:16_PDT_2019\n",
      "Cuda compilation tools, release 10.1, V10.1.243\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4J9EZBfOblZC",
    "outputId": "57c7f083-24fa-46ac-9be5-fcf1dec9033f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zVnr2GHjAGGK",
    "outputId": "28167413-00a5-480c-f179-914b2edc272c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'GCATs'...\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "rm -r /content/*\n",
    "git clone https://github.com/hmthanh/GCATs.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uY4U03W3AT_0"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mv /content/GCATs/* /content/\n",
    "rm -r GCATs/\n",
    "rm /content/README.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "U--J2OdaA5DG",
    "outputId": "aade4e66-01ce-4c54-b085-78f31c7ecd45"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘gat/’: File exists\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sh \"/content/prepare.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlUuYNjCLSy4"
   },
   "source": [
    "# File : create_dataset_files.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "8oaT3a91LJjT",
    "outputId": "4e017663-a167-437d-ef87-a30925f8acb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of train_marked set set  86835\n",
      "Size of VALID_marked set set  3034\n",
      "Size of test_marked set set  3134\n"
     ]
    }
   ],
   "source": [
    "def getID(folder='data/WN18RR/'):\n",
    "    lstEnts = {}\n",
    "    lstRels = {}\n",
    "    with open(folder + 'train.txt') as f, open(folder + 'train_marked.txt', 'w') as f2:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            line = [i.strip() for i in line]\n",
    "            # print(line[0], line[1], line[2])\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            count += 1\n",
    "            f2.write(str(line[0]) + '\\t' + str(line[1]) +\n",
    "                     '\\t' + str(line[2]) + '\\n')\n",
    "        print(\"Size of train_marked set set \", count)\n",
    "\n",
    "    with open(folder + 'valid.txt') as f, open(folder + 'valid_marked.txt', 'w') as f2:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            line = [i.strip() for i in line]\n",
    "            # print(line[0], line[1], line[2])\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            count += 1\n",
    "            f2.write(str(line[0]) + '\\t' + str(line[1]) +\n",
    "                     '\\t' + str(line[2]) + '\\n')\n",
    "        print(\"Size of VALID_marked set set \", count)\n",
    "\n",
    "    with open(folder + 'test.txt') as f, open(folder + 'test_marked.txt', 'w') as f2:\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            line = [i.strip() for i in line]\n",
    "            # print(line[0], line[1], line[2])\n",
    "            if line[0] not in lstEnts:\n",
    "                lstEnts[line[0]] = len(lstEnts)\n",
    "            if line[1] not in lstRels:\n",
    "                lstRels[line[1]] = len(lstRels)\n",
    "            if line[2] not in lstEnts:\n",
    "                lstEnts[line[2]] = len(lstEnts)\n",
    "            count += 1\n",
    "            f2.write(str(line[0]) + '\\t' + str(line[1]) +\n",
    "                     '\\t' + str(line[2]) + '\\n')\n",
    "        print(\"Size of test_marked set set \", count)\n",
    "\n",
    "    wri = open(folder + 'entity2id.txt', 'w')\n",
    "    for entity in lstEnts:\n",
    "        wri.write(entity + '\\t' + str(lstEnts[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()\n",
    "\n",
    "    wri = open(folder + 'relation2id.txt', 'w')\n",
    "    for entity in lstRels:\n",
    "        wri.write(entity + '\\t' + str(lstRels[entity]))\n",
    "        wri.write('\\n')\n",
    "    wri.close()\n",
    "\n",
    "\n",
    "getID()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zBdtg0VELehu"
   },
   "source": [
    "# File : preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1U6EnE9lBtk3"
   },
   "outputs": [],
   "source": [
    "def read_entity_from_id(filename='./data/WN18RR/entity2id.txt'):\n",
    "    entity2id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                entity, entity_id = line.strip().split(\n",
    "                )[0].strip(), line.strip().split()[1].strip()\n",
    "                entity2id[entity] = int(entity_id)\n",
    "    return entity2id\n",
    "\n",
    "\n",
    "def read_relation_from_id(filename='./data/WN18RR/relation2id.txt'):\n",
    "    relation2id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                relation, relation_id = line.strip().split(\n",
    "                )[0].strip(), line.strip().split()[1].strip()\n",
    "                relation2id[relation] = int(relation_id)\n",
    "    return relation2id\n",
    "\n",
    "def init_embeddings(entity_file, relation_file):\n",
    "    entity_emb, relation_emb = [], []\n",
    "\n",
    "    with open(entity_file) as f:\n",
    "        for line in f:\n",
    "            entity_emb.append([float(val) for val in line.strip().split()])\n",
    "\n",
    "    with open(relation_file) as f:\n",
    "        for line in f:\n",
    "            relation_emb.append([float(val) for val in line.strip().split()])\n",
    "\n",
    "    return np.array(entity_emb, dtype=np.float32), np.array(relation_emb, dtype=np.float32)\n",
    "\n",
    "def parse_line(line):\n",
    "    line = line.strip().split()\n",
    "    e1, relation, e2 = line[0].strip(), line[1].strip(), line[2].strip()\n",
    "    return e1, relation, e2\n",
    "\n",
    "def load_data(filename, entity2id, relation2id, is_unweigted=False, directed=True):\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # this is list for relation triples\n",
    "    triples_data = []\n",
    "\n",
    "    # for sparse tensor, rows list contains corresponding row of sparse tensor, cols list contains corresponding\n",
    "    # columnn of sparse tensor, data contains the type of relation\n",
    "    # Adjacecny matrix of entities is undirected, as the source and tail entities should know, the relation\n",
    "    # type they are connected with\n",
    "    rows, cols, data = [], [], []\n",
    "    unique_entities = set()\n",
    "    for line in lines:\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "        unique_entities.add(e1)\n",
    "        unique_entities.add(e2)\n",
    "        triples_data.append(\n",
    "            (entity2id[e1], relation2id[relation], entity2id[e2]))\n",
    "        if not directed:\n",
    "                # Connecting source and tail entity\n",
    "            rows.append(entity2id[e1])\n",
    "            cols.append(entity2id[e2])\n",
    "            if is_unweigted:\n",
    "                data.append(1)\n",
    "            else:\n",
    "                data.append(relation2id[relation])\n",
    "\n",
    "        # Connecting tail and source entity\n",
    "        rows.append(entity2id[e2])\n",
    "        cols.append(entity2id[e1])\n",
    "        if is_unweigted:\n",
    "            data.append(1)\n",
    "        else:\n",
    "            data.append(relation2id[relation])\n",
    "\n",
    "    print(\"number of unique_entities ->\", len(unique_entities))\n",
    "    return triples_data, (rows, cols, data), list(unique_entities)\n",
    "\n",
    "def build_data(path='./data/WN18RR/', is_unweigted=False, directed=True):\n",
    "    entity2id = read_entity_from_id(path + 'entity2id.txt')\n",
    "    relation2id = read_relation_from_id(path + 'relation2id.txt')\n",
    "\n",
    "    # Adjacency matrix only required for training phase\n",
    "    # Currenlty creating as unweighted, undirected\n",
    "    train_triples, train_adjacency_mat, unique_entities_train = load_data(os.path.join(\n",
    "        path, 'train.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    validation_triples, valid_adjacency_mat, unique_entities_validation = load_data(\n",
    "        os.path.join(path, 'valid.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    test_triples, test_adjacency_mat, unique_entities_test = load_data(os.path.join(\n",
    "        path, 'test.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "\n",
    "    id2entity = {v: k for k, v in entity2id.items()}\n",
    "    id2relation = {v: k for k, v in relation2id.items()}\n",
    "    left_entity, right_entity = {}, {}\n",
    "\n",
    "    with open(os.path.join(path, 'train.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "\n",
    "        # Count number of occurences for each (e1, relation)\n",
    "        if relation2id[relation] not in left_entity:\n",
    "            left_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e1] not in left_entity[relation2id[relation]]:\n",
    "            left_entity[relation2id[relation]][entity2id[e1]] = 0\n",
    "        left_entity[relation2id[relation]][entity2id[e1]] += 1\n",
    "\n",
    "        # Count number of occurences for each (relation, e2)\n",
    "        if relation2id[relation] not in right_entity:\n",
    "            right_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e2] not in right_entity[relation2id[relation]]:\n",
    "            right_entity[relation2id[relation]][entity2id[e2]] = 0\n",
    "        right_entity[relation2id[relation]][entity2id[e2]] += 1\n",
    "\n",
    "    left_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        left_entity_avg[i] = sum(\n",
    "            left_entity[i].values()) * 1.0 / len(left_entity[i])\n",
    "\n",
    "    right_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        right_entity_avg[i] = sum(\n",
    "            right_entity[i].values()) * 1.0 / len(right_entity[i])\n",
    "\n",
    "    headTailSelector = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        headTailSelector[i] = 1000 * right_entity_avg[i] / \\\n",
    "            (right_entity_avg[i] + left_entity_avg[i])\n",
    "\n",
    "    return (train_triples, train_adjacency_mat), (validation_triples, valid_adjacency_mat), (test_triples, test_adjacency_mat), \\\n",
    "        entity2id, relation2id, headTailSelector, unique_entities_train\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjIwefuvL1kC"
   },
   "source": [
    "# File : utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBgCEZXBLz2A"
   },
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "\n",
    "\n",
    "def save_model(model, name, epoch, folder_name):\n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(),\n",
    "               (folder_name + \"trained_{}.pth\").format(epoch))\n",
    "    print(\"Done saving Model\")\n",
    "\n",
    "\n",
    "gat_loss_func = nn.MarginRankingLoss(margin=0.5)\n",
    "\n",
    "def GAT_Loss(train_indices, valid_invalid_ratio):\n",
    "    len_pos_triples = train_indices.shape[0] // (int(valid_invalid_ratio) + 1)\n",
    "\n",
    "    pos_triples = train_indices[:len_pos_triples]\n",
    "    neg_triples = train_indices[len_pos_triples:]\n",
    "\n",
    "    pos_triples = pos_triples.repeat(int(valid_invalid_ratio), 1)\n",
    "\n",
    "    source_embeds = entity_embed[pos_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[pos_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[pos_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    pos_norm = torch.norm(x, p=2, dim=1)\n",
    "\n",
    "    source_embeds = entity_embed[neg_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[neg_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[neg_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    neg_norm = torch.norm(x, p=2, dim=1)\n",
    "\n",
    "    y = torch.ones(int(args.valid_invalid_ratio)\n",
    "                   * len_pos_triples).cuda()\n",
    "    loss = gat_loss_func(pos_norm, neg_norm, y)\n",
    "    return loss\n",
    "\n",
    "def render_model_graph(model, Corpus_, train_indices, relation_adj, averaged_entity_vectors):\n",
    "    graph = make_dot(model(Corpus_.train_adj_matrix, train_indices, relation_adj, averaged_entity_vectors),\n",
    "                     params=dict(model.named_parameters()))\n",
    "    graph.render()\n",
    "\n",
    "\n",
    "def print_grads(model):\n",
    "    print(model.relation_embed.weight.grad)\n",
    "    print(model.relation_gat_1.attention_0.a.grad)\n",
    "    print(model.convKB.fc_layer.weight.grad)\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.grad)\n",
    "\n",
    "\n",
    "def clip_gradients(model, gradient_clip_norm):\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip_norm)\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(name, \"norm before clipping is -> \", param.grad.norm())\n",
    "            torch.nn.utils.clip_grad_norm_(param, args.gradient_clip_norm)\n",
    "            print(name, \"norm beafterfore clipping is -> \", param.grad.norm())\n",
    "\n",
    "def plot_grad_flow(named_parameters, parameters):\n",
    "    '''Plots the gradients flowing through different layers in the net during training.\n",
    "    Can be used for checking for possible gradient vanishing / exploding problems.\n",
    "\n",
    "    Usage: Plug this function in Trainer class after loss.backwards() as\n",
    "    \"plot_grad_flow(self.model.named_parameters())\" to visualize the gradient flow'''\n",
    "    ave_grads = []\n",
    "    max_grads = []\n",
    "    layers = []\n",
    "\n",
    "    for n, p in zip(named_parameters, parameters):\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "            max_grads.append(p.grad.abs().max())\n",
    "    plt.bar(np.arange(len(max_grads)), max_grads, alpha=0.1, lw=1, color=\"r\")\n",
    "    plt.bar(np.arange(len(max_grads)), ave_grads, alpha=0.1, lw=1, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, lw=2, color=\"g\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(left=0, right=len(ave_grads))\n",
    "    plt.ylim(bottom=-0.001, top=0.02)  # zoom in on the lower gradient regions\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.legend([Line2D([0], [0], color=\"r\", lw=4),\n",
    "                Line2D([0], [0], color=\"b\", lw=4),\n",
    "                Line2D([0], [0], color=\"g\", lw=4)], ['max-gradient', 'mean-gradient', 'zero-gradient'])\n",
    "    plt.savefig('initial.png')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_grad_flow_low(named_parameters, parameters):\n",
    "    ave_grads = []\n",
    "    layers = []\n",
    "    for n, p in zip(named_parameters, parameters):\n",
    "        # print(n)\n",
    "        if(p.requires_grad) and (\"bias\" not in n):\n",
    "            layers.append(n)\n",
    "            ave_grads.append(p.grad.abs().mean())\n",
    "    plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "    plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n",
    "    plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "    plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "    plt.xlabel(\"Layers\")\n",
    "    plt.ylabel(\"average gradient\")\n",
    "    plt.title(\"Gradient flow\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig('initial.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BOprxDhkMGbQ"
   },
   "source": [
    "# File : models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ohtx6cpoMFSc"
   },
   "outputs": [],
   "source": [
    "class SpGAT(nn.Module):\n",
    "    def __init__(self, num_nodes, nfeat, nhid, relation_dim, dropout, alpha, nheads):\n",
    "        \"\"\"\n",
    "            Sparse version of GAT\n",
    "            nfeat -> Entity Input Embedding dimensions\n",
    "            nhid  -> Entity Output Embedding dimensions\n",
    "            relation_dim -> Relation Embedding dimensions\n",
    "            num_nodes -> number of nodes in the Graph\n",
    "            nheads -> Used for Multihead attention\n",
    "\n",
    "        \"\"\"\n",
    "        super(SpGAT, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.dropout_layer = nn.Dropout(self.dropout)\n",
    "        self.attentions = [SpGraphAttentionLayer(num_nodes, nfeat,\n",
    "                                                 nhid,\n",
    "                                                 relation_dim,\n",
    "                                                 dropout=dropout,\n",
    "                                                 alpha=alpha,\n",
    "                                                 concat=True)\n",
    "                           for _ in range(nheads)]\n",
    "\n",
    "        for i, attention in enumerate(self.attentions):\n",
    "            self.add_module('attention_{}'.format(i), attention)\n",
    "\n",
    "        # W matrix to convert h_input to h_output dimension\n",
    "        self.W = nn.Parameter(torch.zeros(size=(relation_dim, nheads * nhid)))\n",
    "        nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
    "\n",
    "        self.out_att = SpGraphAttentionLayer(num_nodes, nhid * nheads,\n",
    "                                             nheads * nhid, nheads * nhid,\n",
    "                                             dropout=dropout,\n",
    "                                             alpha=alpha,\n",
    "                                             concat=False\n",
    "                                             )\n",
    "\n",
    "    def forward(self, Corpus_, batch_inputs, entity_embeddings, relation_embed,\n",
    "                edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop):\n",
    "        x = entity_embeddings\n",
    "\n",
    "        edge_embed_nhop = relation_embed[\n",
    "            edge_type_nhop[:, 0]] + relation_embed[edge_type_nhop[:, 1]]\n",
    "\n",
    "        x = torch.cat([att(x, edge_list, edge_embed, edge_list_nhop, edge_embed_nhop)\n",
    "                       for att in self.attentions], dim=1)\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        out_relation_1 = relation_embed.mm(self.W)\n",
    "\n",
    "        edge_embed = out_relation_1[edge_type]\n",
    "        edge_embed_nhop = out_relation_1[\n",
    "            edge_type_nhop[:, 0]] + out_relation_1[edge_type_nhop[:, 1]]\n",
    "\n",
    "        x = F.elu(self.out_att(x, edge_list, edge_embed,\n",
    "                               edge_list_nhop, edge_embed_nhop))\n",
    "        return x, out_relation_1\n",
    "\n",
    "class SpKBGATModified(nn.Module):\n",
    "    def __init__(self, initial_entity_emb, initial_relation_emb, entity_out_dim, relation_out_dim,\n",
    "                 drop_GAT, alpha, nheads_GAT):\n",
    "        '''Sparse version of KBGAT\n",
    "        entity_in_dim -> Entity Input Embedding dimensions\n",
    "        entity_out_dim  -> Entity Output Embedding dimensions, passed as a list\n",
    "        num_relation -> number of unique relations\n",
    "        relation_dim -> Relation Embedding dimensions\n",
    "        num_nodes -> number of nodes in the Graph\n",
    "        nheads_GAT -> Used for Multihead attention, passed as a list '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = initial_entity_emb.shape[0]\n",
    "        self.entity_in_dim = initial_entity_emb.shape[1]\n",
    "        self.entity_out_dim_1 = entity_out_dim[0]\n",
    "        self.nheads_GAT_1 = nheads_GAT[0]\n",
    "        self.entity_out_dim_2 = entity_out_dim[1]\n",
    "        self.nheads_GAT_2 = nheads_GAT[1]\n",
    "\n",
    "        # Properties of Relations\n",
    "        self.num_relation = initial_relation_emb.shape[0]\n",
    "        self.relation_dim = initial_relation_emb.shape[1]\n",
    "        self.relation_out_dim_1 = relation_out_dim[0]\n",
    "\n",
    "        self.drop_GAT = drop_GAT\n",
    "        self.alpha = alpha      # For leaky relu\n",
    "\n",
    "        self.final_entity_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_nodes, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.final_relation_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_relation, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.entity_embeddings = nn.Parameter(initial_entity_emb)\n",
    "        self.relation_embeddings = nn.Parameter(initial_relation_emb)\n",
    "\n",
    "        self.sparse_gat_1 = SpGAT(self.num_nodes, self.entity_in_dim, self.entity_out_dim_1, self.relation_dim,\n",
    "                                  self.drop_GAT, self.alpha, self.nheads_GAT_1)\n",
    "\n",
    "        self.W_entities = nn.Parameter(torch.zeros(\n",
    "            size=(self.entity_in_dim, self.entity_out_dim_1 * self.nheads_GAT_1)))\n",
    "        nn.init.xavier_uniform_(self.W_entities.data, gain=1.414)\n",
    "\n",
    "    def forward(self, Corpus_, adj, batch_inputs, train_indices_nhop):\n",
    "        # getting edge list\n",
    "        edge_list = adj[0]\n",
    "        edge_type = adj[1]\n",
    "\n",
    "        edge_list_nhop = torch.cat(\n",
    "            (train_indices_nhop[:, 3].unsqueeze(-1), train_indices_nhop[:, 0].unsqueeze(-1)), dim=1).t()\n",
    "        edge_type_nhop = torch.cat(\n",
    "            [train_indices_nhop[:, 1].unsqueeze(-1), train_indices_nhop[:, 2].unsqueeze(-1)], dim=1)\n",
    "\n",
    "        if(CUDA):\n",
    "            edge_list = edge_list.cuda()\n",
    "            edge_type = edge_type.cuda()\n",
    "            edge_list_nhop = edge_list_nhop.cuda()\n",
    "            edge_type_nhop = edge_type_nhop.cuda()\n",
    "\n",
    "        edge_embed = self.relation_embeddings[edge_type]\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        self.entity_embeddings.data = F.normalize(\n",
    "            self.entity_embeddings.data, p=2, dim=1).detach()\n",
    "\n",
    "        # self.relation_embeddings.data = F.normalize(\n",
    "        #     self.relation_embeddings.data, p=2, dim=1)\n",
    "\n",
    "        out_entity_1, out_relation_1 = self.sparse_gat_1(\n",
    "            Corpus_, batch_inputs, self.entity_embeddings, self.relation_embeddings,\n",
    "            edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop)\n",
    "\n",
    "        mask_indices = torch.unique(batch_inputs[:, 2]).cuda()\n",
    "        mask = torch.zeros(self.entity_embeddings.shape[0]).cuda()\n",
    "        mask[mask_indices] = 1.0\n",
    "\n",
    "        entities_upgraded = self.entity_embeddings.mm(self.W_entities)\n",
    "        out_entity_1 = entities_upgraded + \\\n",
    "            mask.unsqueeze(-1).expand_as(out_entity_1) * out_entity_1\n",
    "\n",
    "        out_entity_1 = F.normalize(out_entity_1, p=2, dim=1)\n",
    "\n",
    "        self.final_entity_embeddings.data = out_entity_1.data\n",
    "        self.final_relation_embeddings.data = out_relation_1.data\n",
    "\n",
    "        return out_entity_1, out_relation_1\n",
    "\n",
    "class SpKBGATConvOnly(nn.Module):\n",
    "    def __init__(self, initial_entity_emb, initial_relation_emb, entity_out_dim, relation_out_dim,\n",
    "                 drop_GAT, drop_conv, alpha, alpha_conv, nheads_GAT, conv_out_channels):\n",
    "        '''Sparse version of KBGAT\n",
    "        entity_in_dim -> Entity Input Embedding dimensions\n",
    "        entity_out_dim  -> Entity Output Embedding dimensions, passed as a list\n",
    "        num_relation -> number of unique relations\n",
    "        relation_dim -> Relation Embedding dimensions\n",
    "        num_nodes -> number of nodes in the Graph\n",
    "        nheads_GAT -> Used for Multihead attention, passed as a list '''\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_nodes = initial_entity_emb.shape[0]\n",
    "        self.entity_in_dim = initial_entity_emb.shape[1]\n",
    "        self.entity_out_dim_1 = entity_out_dim[0]\n",
    "        self.nheads_GAT_1 = nheads_GAT[0]\n",
    "        self.entity_out_dim_2 = entity_out_dim[1]\n",
    "        self.nheads_GAT_2 = nheads_GAT[1]\n",
    "\n",
    "        # Properties of Relations\n",
    "        self.num_relation = initial_relation_emb.shape[0]\n",
    "        self.relation_dim = initial_relation_emb.shape[1]\n",
    "        self.relation_out_dim_1 = relation_out_dim[0]\n",
    "\n",
    "        self.drop_GAT = drop_GAT\n",
    "        self.drop_conv = drop_conv\n",
    "        self.alpha = alpha      # For leaky relu\n",
    "        self.alpha_conv = alpha_conv\n",
    "        self.conv_out_channels = conv_out_channels\n",
    "\n",
    "        self.final_entity_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_nodes, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.final_relation_embeddings = nn.Parameter(\n",
    "            torch.randn(self.num_relation, self.entity_out_dim_1 * self.nheads_GAT_1))\n",
    "\n",
    "        self.convKB = ConvKB(self.entity_out_dim_1 * self.nheads_GAT_1, 3, 1,\n",
    "                             self.conv_out_channels, self.drop_conv, self.alpha_conv)\n",
    "\n",
    "    def forward(self, Corpus_, adj, batch_inputs):\n",
    "        conv_input = torch.cat((self.final_entity_embeddings[batch_inputs[:, 0], :].unsqueeze(1), self.final_relation_embeddings[\n",
    "            batch_inputs[:, 1]].unsqueeze(1), self.final_entity_embeddings[batch_inputs[:, 2], :].unsqueeze(1)), dim=1)\n",
    "        out_conv = self.convKB(conv_input)\n",
    "        return out_conv\n",
    "\n",
    "    def batch_test(self, batch_inputs):\n",
    "        conv_input = torch.cat((self.final_entity_embeddings[batch_inputs[:, 0], :].unsqueeze(1), self.final_relation_embeddings[\n",
    "            batch_inputs[:, 1]].unsqueeze(1), self.final_entity_embeddings[batch_inputs[:, 2], :].unsqueeze(1)), dim=1)\n",
    "        out_conv = self.convKB(conv_input)\n",
    "        return out_conv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXAQD6XVMTLJ"
   },
   "source": [
    "# File : layers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5d_SG2XyMSyV"
   },
   "outputs": [],
   "source": [
    "class ConvKB(nn.Module):\n",
    "    def __init__(self, input_dim, input_seq_len, in_channels, out_channels, drop_prob, alpha_leaky):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv_layer = nn.Conv2d(\n",
    "            in_channels, out_channels, (1, input_seq_len))  # kernel size -> 1*input_seq_length(i.e. 2)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.non_linearity = nn.ReLU()\n",
    "        self.fc_layer = nn.Linear((input_dim) * out_channels, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc_layer.weight, gain=1.414)\n",
    "        nn.init.xavier_uniform_(self.conv_layer.weight, gain=1.414)\n",
    "\n",
    "    def forward(self, conv_input):\n",
    "\n",
    "        batch_size, length, dim = conv_input.size()\n",
    "        # assuming inputs are of the form ->\n",
    "        conv_input = conv_input.transpose(1, 2)\n",
    "        # batch * length(which is 3 here -> entity,relation,entity) * dim\n",
    "        # To make tensor of size 4, where second dim is for input channels\n",
    "        conv_input = conv_input.unsqueeze(1)\n",
    "\n",
    "        out_conv = self.dropout(\n",
    "            self.non_linearity(self.conv_layer(conv_input)))\n",
    "\n",
    "        input_fc = out_conv.squeeze(-1).view(batch_size, -1)\n",
    "        output = self.fc_layer(input_fc)\n",
    "        return output\n",
    "\n",
    "class SpecialSpmmFunctionFinal(torch.autograd.Function):\n",
    "    \"\"\"Special function for only sparse region backpropataion layer.\"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, edge, edge_w, N, E, out_features):\n",
    "        # assert indices.requires_grad == False\n",
    "        a = torch.sparse_coo_tensor(\n",
    "            edge, edge_w, torch.Size([N, N, out_features]))\n",
    "        b = torch.sparse.sum(a, dim=1)\n",
    "        ctx.N = b.shape[0]\n",
    "        ctx.outfeat = b.shape[1]\n",
    "        ctx.E = E\n",
    "        ctx.indices = a._indices()[0, :]\n",
    "\n",
    "        return b.to_dense()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad_values = None\n",
    "        if ctx.needs_input_grad[1]:\n",
    "            edge_sources = ctx.indices\n",
    "\n",
    "            if(CUDA):\n",
    "                edge_sources = edge_sources.cuda()\n",
    "\n",
    "            grad_values = grad_output[edge_sources]\n",
    "            # grad_values = grad_values.view(ctx.E, ctx.outfeat)\n",
    "            # print(\"Grad Outputs-> \", grad_output)\n",
    "            # print(\"Grad values-> \", grad_values)\n",
    "        return None, grad_values, None, None, None\n",
    "\n",
    "class SpecialSpmmFinal(nn.Module):\n",
    "    def forward(self, edge, edge_w, N, E, out_features):\n",
    "        return SpecialSpmmFunctionFinal.apply(edge, edge_w, N, E, out_features)\n",
    "\n",
    "\n",
    "class SpGraphAttentionLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Sparse version GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, in_features, out_features, nrela_dim, dropout, alpha, concat=True):\n",
    "        super(SpGraphAttentionLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.num_nodes = num_nodes\n",
    "        self.alpha = alpha\n",
    "        self.concat = concat\n",
    "        self.nrela_dim = nrela_dim\n",
    "\n",
    "        self.a = nn.Parameter(torch.zeros(\n",
    "            size=(out_features, 2 * in_features + nrela_dim)))\n",
    "        nn.init.xavier_normal_(self.a.data, gain=1.414)\n",
    "        self.a_2 = nn.Parameter(torch.zeros(size=(1, out_features)))\n",
    "        nn.init.xavier_normal_(self.a_2.data, gain=1.414)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
    "        self.special_spmm_final = SpecialSpmmFinal()\n",
    "\n",
    "    def forward(self, input, edge, edge_embed, edge_list_nhop, edge_embed_nhop):\n",
    "        N = input.size()[0]\n",
    "\n",
    "        # Self-attention on the nodes - Shared attention mechanism\n",
    "        edge = torch.cat((edge[:, :], edge_list_nhop[:, :]), dim=1)\n",
    "        edge_embed = torch.cat(\n",
    "            (edge_embed[:, :], edge_embed_nhop[:, :]), dim=0)\n",
    "\n",
    "        edge_h = torch.cat(\n",
    "            (input[edge[0, :], :], input[edge[1, :], :], edge_embed[:, :]), dim=1).t()\n",
    "        # edge_h: (2*in_dim + nrela_dim) x E\n",
    "\n",
    "        edge_m = self.a.mm(edge_h)\n",
    "        # edge_m: D * E\n",
    "\n",
    "        # to be checked later\n",
    "        powers = -self.leakyrelu(self.a_2.mm(edge_m).squeeze())\n",
    "        edge_e = torch.exp(powers).unsqueeze(1)\n",
    "        assert not torch.isnan(edge_e).any()\n",
    "        # edge_e: E\n",
    "\n",
    "        e_rowsum = self.special_spmm_final(\n",
    "            edge, edge_e, N, edge_e.shape[0], 1)\n",
    "        e_rowsum[e_rowsum == 0.0] = 1e-12\n",
    "\n",
    "        e_rowsum = e_rowsum\n",
    "        # e_rowsum: N x 1\n",
    "        edge_e = edge_e.squeeze(1)\n",
    "\n",
    "        edge_e = self.dropout(edge_e)\n",
    "        # edge_e: E\n",
    "\n",
    "        edge_w = (edge_e * edge_m).t()\n",
    "        # edge_w: E * D\n",
    "\n",
    "        h_prime = self.special_spmm_final(\n",
    "            edge, edge_w, N, edge_w.shape[0], self.out_features)\n",
    "\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        # h_prime: N x out\n",
    "        h_prime = h_prime.div(e_rowsum)\n",
    "        # h_prime: N x out\n",
    "\n",
    "        assert not torch.isnan(h_prime).any()\n",
    "        if self.concat:\n",
    "            # if this layer is not last layer,\n",
    "            return F.elu(h_prime)\n",
    "        else:\n",
    "            # if this layer is last layer,\n",
    "            return h_prime\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0ZfxfQfMp21"
   },
   "source": [
    "# File : create_batch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1nxeYUF7MsV5"
   },
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, args, train_data, validation_data, test_data, entity2id,\n",
    "                 relation2id, headTailSelector, batch_size, valid_to_invalid_samples_ratio, unique_entities_train, get_2hop=False):\n",
    "        self.train_triples = train_data[0]\n",
    "\n",
    "        # Converting to sparse tensor\n",
    "        adj_indices = torch.LongTensor(\n",
    "            [train_data[1][0], train_data[1][1]])  # rows and columns\n",
    "        adj_values = torch.LongTensor(train_data[1][2])\n",
    "        self.train_adj_matrix = (adj_indices, adj_values)\n",
    "\n",
    "        # adjacency matrix is needed for train_data only, as GAT is trained for\n",
    "        # training data\n",
    "        self.validation_triples = validation_data[0]\n",
    "        self.test_triples = test_data[0]\n",
    "\n",
    "        self.headTailSelector = headTailSelector  # for selecting random entities\n",
    "        self.entity2id = entity2id\n",
    "        self.id2entity = {v: k for k, v in self.entity2id.items()}\n",
    "        self.relation2id = relation2id\n",
    "        self.id2relation = {v: k for k, v in self.relation2id.items()}\n",
    "        self.batch_size = batch_size\n",
    "        # ratio of valid to invalid samples per batch for training ConvKB Model\n",
    "        self.invalid_valid_ratio = int(valid_to_invalid_samples_ratio)\n",
    "\n",
    "        if(get_2hop):\n",
    "            self.graph = self.get_graph()\n",
    "            self.node_neighbors_2hop = self.get_further_neighbors()\n",
    "\n",
    "        self.unique_entities_train = [self.entity2id[i]\n",
    "                                      for i in unique_entities_train]\n",
    "\n",
    "        self.train_indices = np.array(\n",
    "            list(self.train_triples)).astype(np.int32)\n",
    "        # These are valid triples, hence all have value 1\n",
    "        self.train_values = np.array(\n",
    "            [[1]] * len(self.train_triples)).astype(np.float32)\n",
    "\n",
    "        self.validation_indices = np.array(\n",
    "            list(self.validation_triples)).astype(np.int32)\n",
    "        self.validation_values = np.array(\n",
    "            [[1]] * len(self.validation_triples)).astype(np.float32)\n",
    "\n",
    "        self.test_indices = np.array(list(self.test_triples)).astype(np.int32)\n",
    "        self.test_values = np.array(\n",
    "            [[1]] * len(self.test_triples)).astype(np.float32)\n",
    "\n",
    "        self.valid_triples_dict = {j: i for i, j in enumerate(\n",
    "            self.train_triples + self.validation_triples + self.test_triples)}\n",
    "        print(\"Total triples count {}, training triples {}, validation_triples {}, test_triples {}\".format(len(self.valid_triples_dict), len(self.train_indices),\n",
    "                                                                                                           len(self.validation_indices), len(self.test_indices)))\n",
    "\n",
    "        # For training purpose\n",
    "        self.batch_indices = np.empty(\n",
    "            (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "        self.batch_values = np.empty(\n",
    "            (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "    def get_iteration_batch(self, iter_num):\n",
    "        if (iter_num + 1) * self.batch_size <= len(self.train_indices):\n",
    "            self.batch_indices = np.empty(\n",
    "                (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "            self.batch_values = np.empty(\n",
    "                (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "            indices = range(self.batch_size * iter_num,\n",
    "                            self.batch_size * (iter_num + 1))\n",
    "\n",
    "            self.batch_indices[:self.batch_size,\n",
    "                               :] = self.train_indices[indices, :]\n",
    "            self.batch_values[:self.batch_size,\n",
    "                              :] = self.train_values[indices, :]\n",
    "\n",
    "            last_index = self.batch_size\n",
    "\n",
    "            if self.invalid_valid_ratio > 0:\n",
    "                random_entities = np.random.randint(\n",
    "                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "                # Precopying the same valid indices from 0 to batch_size to rest\n",
    "                # of the indices\n",
    "                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "                for i in range(last_index):\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\n",
    "                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           0] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = last_index * \\\n",
    "                            (self.invalid_valid_ratio // 2) + \\\n",
    "                            (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\n",
    "                               random_entities[current_index]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           2] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                return self.batch_indices, self.batch_values\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "        else:\n",
    "            last_iter_size = len(self.train_indices) - \\\n",
    "                self.batch_size * iter_num\n",
    "            self.batch_indices = np.empty(\n",
    "                (last_iter_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "            self.batch_values = np.empty(\n",
    "                (last_iter_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "            indices = range(self.batch_size * iter_num,\n",
    "                            len(self.train_indices))\n",
    "            self.batch_indices[:last_iter_size,\n",
    "                               :] = self.train_indices[indices, :]\n",
    "            self.batch_values[:last_iter_size,\n",
    "                              :] = self.train_values[indices, :]\n",
    "\n",
    "            last_index = last_iter_size\n",
    "\n",
    "            if self.invalid_valid_ratio > 0:\n",
    "                random_entities = np.random.randint(\n",
    "                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "                # Precopying the same valid indices from 0 to batch_size to rest\n",
    "                # of the indices\n",
    "                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "                for i in range(last_index):\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\n",
    "                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           0] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = last_index * \\\n",
    "                            (self.invalid_valid_ratio // 2) + \\\n",
    "                            (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\n",
    "                               random_entities[current_index]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           2] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                return self.batch_indices, self.batch_values\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "    def get_iteration_batch_nhop(self, current_batch_indices, node_neighbors, batch_size):\n",
    "\n",
    "        self.batch_indices = np.empty(\n",
    "            (batch_size * (self.invalid_valid_ratio + 1), 4)).astype(np.int32)\n",
    "        self.batch_values = np.empty(\n",
    "            (batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "        indices = random.sample(range(len(current_batch_indices)), batch_size)\n",
    "\n",
    "        self.batch_indices[:batch_size,\n",
    "                           :] = current_batch_indices[indices, :]\n",
    "        self.batch_values[:batch_size,\n",
    "                          :] = np.ones((batch_size, 1))\n",
    "\n",
    "        last_index = batch_size\n",
    "\n",
    "        if self.invalid_valid_ratio > 0:\n",
    "            random_entities = np.random.randint(\n",
    "                0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "            # Precopying the same valid indices from 0 to batch_size to rest\n",
    "            # of the indices\n",
    "            self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "            self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "            for i in range(last_index):\n",
    "                for j in range(self.invalid_valid_ratio // 2):\n",
    "                    current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                    self.batch_indices[last_index + current_index,\n",
    "                                       0] = random_entities[current_index]\n",
    "                    self.batch_values[last_index + current_index, :] = [0]\n",
    "\n",
    "                for j in range(self.invalid_valid_ratio // 2):\n",
    "                    current_index = last_index * \\\n",
    "                        (self.invalid_valid_ratio // 2) + \\\n",
    "                        (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                    self.batch_indices[last_index + current_index,\n",
    "                                       3] = random_entities[current_index]\n",
    "                    self.batch_values[last_index + current_index, :] = [0]\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "        return self.batch_indices, self.batch_values\n",
    "\n",
    "    def get_graph(self):\n",
    "        graph = {}\n",
    "        all_tiples = torch.cat([self.train_adj_matrix[0].transpose(\n",
    "            0, 1), self.train_adj_matrix[1].unsqueeze(1)], dim=1)\n",
    "\n",
    "        for data in all_tiples:\n",
    "            source = data[1].data.item()\n",
    "            target = data[0].data.item()\n",
    "            value = data[2].data.item()\n",
    "\n",
    "            if(source not in graph.keys()):\n",
    "                graph[source] = {}\n",
    "                graph[source][target] = value\n",
    "            else:\n",
    "                graph[source][target] = value\n",
    "        print(\"Graph created\")\n",
    "        return graph\n",
    "\n",
    "    def bfs(self, graph, source, nbd_size=2):\n",
    "        visit = {}\n",
    "        distance = {}\n",
    "        parent = {}\n",
    "        distance_lengths = {}\n",
    "\n",
    "        visit[source] = 1\n",
    "        distance[source] = 0\n",
    "        parent[source] = (-1, -1)\n",
    "\n",
    "        q = queue.Queue()\n",
    "        q.put((source, -1))\n",
    "\n",
    "        while(not q.empty()):\n",
    "            top = q.get()\n",
    "            if top[0] in graph.keys():\n",
    "                for target in graph[top[0]].keys():\n",
    "                    if(target in visit.keys()):\n",
    "                        continue\n",
    "                    else:\n",
    "                        q.put((target, graph[top[0]][target]))\n",
    "\n",
    "                        distance[target] = distance[top[0]] + 1\n",
    "\n",
    "                        visit[target] = 1\n",
    "                        if distance[target] > 2:\n",
    "                            continue\n",
    "                        parent[target] = (top[0], graph[top[0]][target])\n",
    "\n",
    "                        if distance[target] not in distance_lengths.keys():\n",
    "                            distance_lengths[distance[target]] = 1\n",
    "\n",
    "        neighbors = {}\n",
    "        for target in visit.keys():\n",
    "            if(distance[target] != nbd_size):\n",
    "                continue\n",
    "            edges = [-1, parent[target][1]]\n",
    "            relations = []\n",
    "            entities = [target]\n",
    "            temp = target\n",
    "            while(parent[temp] != (-1, -1)):\n",
    "                relations.append(parent[temp][1])\n",
    "                entities.append(parent[temp][0])\n",
    "                temp = parent[temp][0]\n",
    "\n",
    "            if(distance[target] in neighbors.keys()):\n",
    "                neighbors[distance[target]].append(\n",
    "                    (tuple(relations), tuple(entities[:-1])))\n",
    "            else:\n",
    "                neighbors[distance[target]] = [\n",
    "                    (tuple(relations), tuple(entities[:-1]))]\n",
    "\n",
    "        return neighbors\n",
    "\n",
    "    def get_further_neighbors(self, nbd_size=2):\n",
    "        neighbors = {}\n",
    "        start_time = time.time()\n",
    "        print(\"length of graph keys is \", len(self.graph.keys()))\n",
    "        for source in self.graph.keys():\n",
    "            # st_time = time.time()\n",
    "            temp_neighbors = self.bfs(self.graph, source, nbd_size)\n",
    "            for distance in temp_neighbors.keys():\n",
    "                if(source in neighbors.keys()):\n",
    "                    if(distance in neighbors[source].keys()):\n",
    "                        neighbors[source][distance].append(\n",
    "                            temp_neighbors[distance])\n",
    "                    else:\n",
    "                        neighbors[source][distance] = temp_neighbors[distance]\n",
    "                else:\n",
    "                    neighbors[source] = {}\n",
    "                    neighbors[source][distance] = temp_neighbors[distance]\n",
    "\n",
    "        print(\"time taken \", time.time() - start_time)\n",
    "\n",
    "        print(\"length of neighbors dict is \", len(neighbors))\n",
    "        return neighbors\n",
    "\n",
    "    def get_batch_nhop_neighbors_all(self, args, batch_sources, node_neighbors, nbd_size=2):\n",
    "        batch_source_triples = []\n",
    "        print(\"length of unique_entities \", len(batch_sources))\n",
    "        count = 0\n",
    "        for source in batch_sources:\n",
    "            # randomly select from the list of neighbors\n",
    "            if source in node_neighbors.keys():\n",
    "                nhop_list = node_neighbors[source][nbd_size]\n",
    "\n",
    "                for i, tup in enumerate(nhop_list):\n",
    "                    if(args.partial_2hop and i >= 2):\n",
    "                        break\n",
    "\n",
    "                    count += 1\n",
    "                    batch_source_triples.append([source, nhop_list[i][0][-1], nhop_list[i][0][0],\n",
    "                                                 nhop_list[i][1][0]])\n",
    "\n",
    "        return np.array(batch_source_triples).astype(np.int32)\n",
    "\n",
    "    def transe_scoring(self, batch_inputs, entity_embeddings, relation_embeddings):\n",
    "        source_embeds = entity_embeddings[batch_inputs[:, 0]]\n",
    "        relation_embeds = relation_embeddings[batch_inputs[:, 1]]\n",
    "        tail_embeds = entity_embeddings[batch_inputs[:, 2]]\n",
    "        x = source_embeds + relation_embed - tail_embeds\n",
    "        x = torch.norm(x, p=1, dim=1)\n",
    "        return x\n",
    "\n",
    "    def get_validation_pred(self, args, model, unique_entities):\n",
    "        average_hits_at_100_head, average_hits_at_100_tail = [], []\n",
    "        average_hits_at_ten_head, average_hits_at_ten_tail = [], []\n",
    "        average_hits_at_three_head, average_hits_at_three_tail = [], []\n",
    "        average_hits_at_one_head, average_hits_at_one_tail = [], []\n",
    "        average_mean_rank_head, average_mean_rank_tail = [], []\n",
    "        average_mean_recip_rank_head, average_mean_recip_rank_tail = [], []\n",
    "\n",
    "        for iters in range(1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            indices = [i for i in range(len(self.test_indices))]\n",
    "            batch_indices = self.test_indices[indices, :]\n",
    "            print(\"Sampled indices\")\n",
    "            print(\"test set length \", len(self.test_indices))\n",
    "            entity_list = [j for i, j in self.entity2id.items()]\n",
    "\n",
    "            ranks_head, ranks_tail = [], []\n",
    "            reciprocal_ranks_head, reciprocal_ranks_tail = [], []\n",
    "            hits_at_100_head, hits_at_100_tail = 0, 0\n",
    "            hits_at_ten_head, hits_at_ten_tail = 0, 0\n",
    "            hits_at_three_head, hits_at_three_tail = 0, 0\n",
    "            hits_at_one_head, hits_at_one_tail = 0, 0\n",
    "\n",
    "            for i in range(batch_indices.shape[0]):\n",
    "                print(len(ranks_head))\n",
    "                start_time_it = time.time()\n",
    "                new_x_batch_head = np.tile(\n",
    "                    batch_indices[i, :], (len(self.entity2id), 1))\n",
    "                new_x_batch_tail = np.tile(\n",
    "                    batch_indices[i, :], (len(self.entity2id), 1))\n",
    "\n",
    "                if(batch_indices[i, 0] not in unique_entities or batch_indices[i, 2] not in unique_entities):\n",
    "                    continue\n",
    "\n",
    "                new_x_batch_head[:, 0] = entity_list\n",
    "                new_x_batch_tail[:, 2] = entity_list\n",
    "\n",
    "                last_index_head = []  # array of already existing triples\n",
    "                last_index_tail = []\n",
    "                for tmp_index in range(len(new_x_batch_head)):\n",
    "                    temp_triple_head = (new_x_batch_head[tmp_index][0], new_x_batch_head[tmp_index][1],\n",
    "                                        new_x_batch_head[tmp_index][2])\n",
    "                    if temp_triple_head in self.valid_triples_dict.keys():\n",
    "                        last_index_head.append(tmp_index)\n",
    "\n",
    "                    temp_triple_tail = (new_x_batch_tail[tmp_index][0], new_x_batch_tail[tmp_index][1],\n",
    "                                        new_x_batch_tail[tmp_index][2])\n",
    "                    if temp_triple_tail in self.valid_triples_dict.keys():\n",
    "                        last_index_tail.append(tmp_index)\n",
    "\n",
    "                # Deleting already existing triples, leftover triples are invalid, according\n",
    "                # to train, validation and test data\n",
    "                # Note, all of them maynot be actually invalid\n",
    "                new_x_batch_head = np.delete(\n",
    "                    new_x_batch_head, last_index_head, axis=0)\n",
    "                new_x_batch_tail = np.delete(\n",
    "                    new_x_batch_tail, last_index_tail, axis=0)\n",
    "\n",
    "                # adding the current valid triples to the top, i.e, index 0\n",
    "                new_x_batch_head = np.insert(\n",
    "                    new_x_batch_head, 0, batch_indices[i], axis=0)\n",
    "                new_x_batch_tail = np.insert(\n",
    "                    new_x_batch_tail, 0, batch_indices[i], axis=0)\n",
    "\n",
    "                import math\n",
    "                # Have to do this, because it doesn't fit in memory\n",
    "\n",
    "                if 'WN' in args.data:\n",
    "                    num_triples_each_shot = int(\n",
    "                        math.ceil(new_x_batch_head.shape[0] / 4))\n",
    "\n",
    "                    scores1_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[:num_triples_each_shot, :]).cuda())\n",
    "                    scores2_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())\n",
    "                    scores3_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())\n",
    "                    scores4_head = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_head[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores5_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores6_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[5 * num_triples_each_shot: 6 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores7_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[6 * num_triples_each_shot: 7 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores8_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[7 * num_triples_each_shot: 8 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores9_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[8 * num_triples_each_shot: 9 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores10_head = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_head[9 * num_triples_each_shot:, :]).cuda())\n",
    "\n",
    "                    scores_head = torch.cat(\n",
    "                        [scores1_head, scores2_head, scores3_head, scores4_head], dim=0)\n",
    "                    #scores5_head, scores6_head, scores7_head, scores8_head,\n",
    "                    # cores9_head, scores10_head], dim=0)\n",
    "                else:\n",
    "                    scores_head = model.batch_test(new_x_batch_head)\n",
    "\n",
    "                sorted_scores_head, sorted_indices_head = torch.sort(\n",
    "                    scores_head.view(-1), dim=-1, descending=True)\n",
    "                # Just search for zeroth index in the sorted scores, we appended valid triple at top\n",
    "                ranks_head.append(\n",
    "                    np.where(sorted_indices_head.cpu().numpy() == 0)[0][0] + 1)\n",
    "                reciprocal_ranks_head.append(1.0 / ranks_head[-1])\n",
    "\n",
    "                # Tail part here\n",
    "\n",
    "                if 'WN' in args.data:\n",
    "                    num_triples_each_shot = int(\n",
    "                        math.ceil(new_x_batch_tail.shape[0] / 4))\n",
    "\n",
    "                    scores1_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[:num_triples_each_shot, :]).cuda())\n",
    "                    scores2_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[num_triples_each_shot: 2 * num_triples_each_shot, :]).cuda())\n",
    "                    scores3_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[2 * num_triples_each_shot: 3 * num_triples_each_shot, :]).cuda())\n",
    "                    scores4_tail = model.batch_test(torch.LongTensor(\n",
    "                        new_x_batch_tail[3 * num_triples_each_shot: 4 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores5_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[4 * num_triples_each_shot: 5 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores6_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[5 * num_triples_each_shot: 6 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores7_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[6 * num_triples_each_shot: 7 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores8_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[7 * num_triples_each_shot: 8 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores9_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[8 * num_triples_each_shot: 9 * num_triples_each_shot, :]).cuda())\n",
    "                    # scores10_tail = model.batch_test(torch.LongTensor(\n",
    "                    #     new_x_batch_tail[9 * num_triples_each_shot:, :]).cuda())\n",
    "\n",
    "                    scores_tail = torch.cat(\n",
    "                        [scores1_tail, scores2_tail, scores3_tail, scores4_tail], dim=0)\n",
    "                    #     scores5_tail, scores6_tail, scores7_tail, scores8_tail,\n",
    "                    #     scores9_tail, scores10_tail], dim=0)\n",
    "\n",
    "                else:\n",
    "                    scores_tail = model.batch_test(new_x_batch_tail)\n",
    "\n",
    "                sorted_scores_tail, sorted_indices_tail = torch.sort(\n",
    "                    scores_tail.view(-1), dim=-1, descending=True)\n",
    "\n",
    "                # Just search for zeroth index in the sorted scores, we appended valid triple at top\n",
    "                ranks_tail.append(\n",
    "                    np.where(sorted_indices_tail.cpu().numpy() == 0)[0][0] + 1)\n",
    "                reciprocal_ranks_tail.append(1.0 / ranks_tail[-1])\n",
    "                print(\"sample - \", ranks_head[-1], ranks_tail[-1])\n",
    "\n",
    "            for i in range(len(ranks_head)):\n",
    "                if ranks_head[i] <= 100:\n",
    "                    hits_at_100_head = hits_at_100_head + 1\n",
    "                if ranks_head[i] <= 10:\n",
    "                    hits_at_ten_head = hits_at_ten_head + 1\n",
    "                if ranks_head[i] <= 3:\n",
    "                    hits_at_three_head = hits_at_three_head + 1\n",
    "                if ranks_head[i] == 1:\n",
    "                    hits_at_one_head = hits_at_one_head + 1\n",
    "\n",
    "            for i in range(len(ranks_tail)):\n",
    "                if ranks_tail[i] <= 100:\n",
    "                    hits_at_100_tail = hits_at_100_tail + 1\n",
    "                if ranks_tail[i] <= 10:\n",
    "                    hits_at_ten_tail = hits_at_ten_tail + 1\n",
    "                if ranks_tail[i] <= 3:\n",
    "                    hits_at_three_tail = hits_at_three_tail + 1\n",
    "                if ranks_tail[i] == 1:\n",
    "                    hits_at_one_tail = hits_at_one_tail + 1\n",
    "\n",
    "            assert len(ranks_head) == len(reciprocal_ranks_head)\n",
    "            assert len(ranks_tail) == len(reciprocal_ranks_tail)\n",
    "            print(\"here {}\".format(len(ranks_head)))\n",
    "            print(\"\\nCurrent iteration time {}\".format(time.time() - start_time))\n",
    "            print(\"Stats for replacing head are -> \")\n",
    "            print(\"Current iteration Hits@100 are {}\".format(\n",
    "                hits_at_100_head / float(len(ranks_head))))\n",
    "            print(\"Current iteration Hits@10 are {}\".format(\n",
    "                hits_at_ten_head / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@3 are {}\".format(\n",
    "                hits_at_three_head / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@1 are {}\".format(\n",
    "                hits_at_one_head / len(ranks_head)))\n",
    "            print(\"Current iteration Mean rank {}\".format(\n",
    "                sum(ranks_head) / len(ranks_head)))\n",
    "            print(\"Current iteration Mean Reciprocal Rank {}\".format(\n",
    "                sum(reciprocal_ranks_head) / len(reciprocal_ranks_head)))\n",
    "\n",
    "            print(\"\\nStats for replacing tail are -> \")\n",
    "            print(\"Current iteration Hits@100 are {}\".format(\n",
    "                hits_at_100_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@10 are {}\".format(\n",
    "                hits_at_ten_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@3 are {}\".format(\n",
    "                hits_at_three_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Hits@1 are {}\".format(\n",
    "                hits_at_one_tail / len(ranks_head)))\n",
    "            print(\"Current iteration Mean rank {}\".format(\n",
    "                sum(ranks_tail) / len(ranks_tail)))\n",
    "            print(\"Current iteration Mean Reciprocal Rank {}\".format(\n",
    "                sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail)))\n",
    "\n",
    "            average_hits_at_100_head.append(\n",
    "                hits_at_100_head / len(ranks_head))\n",
    "            average_hits_at_ten_head.append(\n",
    "                hits_at_ten_head / len(ranks_head))\n",
    "            average_hits_at_three_head.append(\n",
    "                hits_at_three_head / len(ranks_head))\n",
    "            average_hits_at_one_head.append(\n",
    "                hits_at_one_head / len(ranks_head))\n",
    "            average_mean_rank_head.append(sum(ranks_head) / len(ranks_head))\n",
    "            average_mean_recip_rank_head.append(\n",
    "                sum(reciprocal_ranks_head) / len(reciprocal_ranks_head))\n",
    "\n",
    "            average_hits_at_100_tail.append(\n",
    "                hits_at_100_tail / len(ranks_head))\n",
    "            average_hits_at_ten_tail.append(\n",
    "                hits_at_ten_tail / len(ranks_head))\n",
    "            average_hits_at_three_tail.append(\n",
    "                hits_at_three_tail / len(ranks_head))\n",
    "            average_hits_at_one_tail.append(\n",
    "                hits_at_one_tail / len(ranks_head))\n",
    "            average_mean_rank_tail.append(sum(ranks_tail) / len(ranks_tail))\n",
    "            average_mean_recip_rank_tail.append(\n",
    "                sum(reciprocal_ranks_tail) / len(reciprocal_ranks_tail))\n",
    "\n",
    "        print(\"\\nAveraged stats for replacing head are -> \")\n",
    "        print(\"Hits@100 are {}\".format(\n",
    "            sum(average_hits_at_100_head) / len(average_hits_at_100_head)))\n",
    "        print(\"Hits@10 are {}\".format(\n",
    "            sum(average_hits_at_ten_head) / len(average_hits_at_ten_head)))\n",
    "        print(\"Hits@3 are {}\".format(\n",
    "            sum(average_hits_at_three_head) / len(average_hits_at_three_head)))\n",
    "        print(\"Hits@1 are {}\".format(\n",
    "            sum(average_hits_at_one_head) / len(average_hits_at_one_head)))\n",
    "        print(\"Mean rank {}\".format(\n",
    "            sum(average_mean_rank_head) / len(average_mean_rank_head)))\n",
    "        print(\"Mean Reciprocal Rank {}\".format(\n",
    "            sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head)))\n",
    "\n",
    "        print(\"\\nAveraged stats for replacing tail are -> \")\n",
    "        print(\"Hits@100 are {}\".format(\n",
    "            sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)))\n",
    "        print(\"Hits@10 are {}\".format(\n",
    "            sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)))\n",
    "        print(\"Hits@3 are {}\".format(\n",
    "            sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)))\n",
    "        print(\"Hits@1 are {}\".format(\n",
    "            sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)))\n",
    "        print(\"Mean rank {}\".format(\n",
    "            sum(average_mean_rank_tail) / len(average_mean_rank_tail)))\n",
    "        print(\"Mean Reciprocal Rank {}\".format(\n",
    "            sum(average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)))\n",
    "\n",
    "        cumulative_hits_100 = (sum(average_hits_at_100_head) / len(average_hits_at_100_head)\n",
    "                               + sum(average_hits_at_100_tail) / len(average_hits_at_100_tail)) / 2\n",
    "        cumulative_hits_ten = (sum(average_hits_at_ten_head) / len(average_hits_at_ten_head)\n",
    "                               + sum(average_hits_at_ten_tail) / len(average_hits_at_ten_tail)) / 2\n",
    "        cumulative_hits_three = (sum(average_hits_at_three_head) / len(average_hits_at_three_head)\n",
    "                                 + sum(average_hits_at_three_tail) / len(average_hits_at_three_tail)) / 2\n",
    "        cumulative_hits_one = (sum(average_hits_at_one_head) / len(average_hits_at_one_head)\n",
    "                               + sum(average_hits_at_one_tail) / len(average_hits_at_one_tail)) / 2\n",
    "        cumulative_mean_rank = (sum(average_mean_rank_head) / len(average_mean_rank_head)\n",
    "                                + sum(average_mean_rank_tail) / len(average_mean_rank_tail)) / 2\n",
    "        cumulative_mean_recip_rank = (sum(average_mean_recip_rank_head) / len(average_mean_recip_rank_head) + sum(\n",
    "            average_mean_recip_rank_tail) / len(average_mean_recip_rank_tail)) / 2\n",
    "\n",
    "        print(\"\\nCumulative stats are -> \")\n",
    "        print(\"Hits@100 are {}\".format(cumulative_hits_100))\n",
    "        print(\"Hits@10 are {}\".format(cumulative_hits_ten))\n",
    "        print(\"Hits@3 are {}\".format(cumulative_hits_three))\n",
    "        print(\"Hits@1 are {}\".format(cumulative_hits_one))\n",
    "        print(\"Mean rank {}\".format(cumulative_mean_rank))\n",
    "        print(\"Mean Reciprocal Rank {}\".format(cumulative_mean_recip_rank))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JJjp1V-rF0Bk"
   },
   "outputs": [],
   "source": [
    "def main_load_data(args):\n",
    "    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\n",
    "                                                                 os.path.join(args.data, 'relation2vec.txt'))\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        entity_embeddings = np.random.randn(\n",
    "            len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(\n",
    "            len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\n",
    "\n",
    "    return corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "gO6BWCO9GPxN",
    "outputId": "04ea8f6d-89dd-4aaa-b8ef-adfb9083df0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique_entities -> 40559\n",
      "number of unique_entities -> 5173\n",
      "number of unique_entities -> 5323\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        \"./data/WN18RR/\", is_unweigted=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "dVDLIBLTHSHp",
    "outputId": "3eafe0de-28a3-4e4f-80cc-81de771c30c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_triples (3134, 3)\n",
      "test_adjacency_mat (3, 3134)\n",
      "entity2id 40943\n",
      "relation2id 11\n",
      "headTailSelector 11\n",
      "unique_entities_train (40559,)\n"
     ]
    }
   ],
   "source": [
    "#(train_triples, train_adjacency_mat) = train_data\n",
    "(test_triples, test_adjacency_mat) = test_data\n",
    "print(\"test_triples\", np.array(test_triples).shape)\n",
    "print(\"test_adjacency_mat\", np.array(test_adjacency_mat).shape)\n",
    "print(\"entity2id\", len(entity2id))\n",
    "print(\"relation2id\", len(relation2id))\n",
    "print(\"headTailSelector\", len(headTailSelector))\n",
    "print(\"unique_entities_train\", np.array(unique_entities_train).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xy-rjzy9fluF"
   },
   "source": [
    "# Không chạy cái này, chạy dòng dưới để load dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "colab_type": "code",
    "id": "o1tn_TuhHs27",
    "outputId": "0e9fc2b7-486a-47ae-ad4e-87b71e390dc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique_entities -> 40559\n",
      "number of unique_entities -> 5173\n",
      "number of unique_entities -> 5323\n",
      "Initialised relations and entities from TransE\n",
      "Graph created\n",
      "length of graph keys is  39610\n",
      "time taken  3225.77739071846\n",
      "length of neighbors dict is  39115\n",
      "Total triples count 93003, training triples 86835, validation_triples 3034, test_triples 3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "def load_data_main(args):\n",
    "    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\n",
    "                                                                 os.path.join(args.data, 'relation2vec.txt'))\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        entity_embeddings = np.random.randn(\n",
    "            len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(\n",
    "            len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\n",
    "\n",
    "    return corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)\n",
    "\n",
    "\n",
    "Corpus_, entity_embeddings, relation_embeddings = load_data_main(args)\n",
    "\n",
    "with open(\"Corpus.pk\", 'wb') as Corpus_pk:\n",
    "  pickle.dump(Corpus_, Corpus_pk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PHjGer0jM-kb"
   },
   "outputs": [],
   "source": [
    "print(entity_embeddings.type(n), relation_embeddings.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bIBEq0go0DmH"
   },
   "outputs": [],
   "source": [
    "torch.save(Corpus_, \"Corpus_torch.pt\")\n",
    "torch.save(entity_embeddings, \"./data/WN18RR/entity_embeddings.pt\")\n",
    "torch.save(relation_embeddings, \"./data/WN18RR/relation_embeddings.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zKwcZLp_Cg5a"
   },
   "source": [
    "### Load entity và relation trained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VAaeubw9RMzd"
   },
   "source": [
    "Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pb97zZQZZwZ8"
   },
   "outputs": [],
   "source": [
    "entity_embeddings = torch.load(\"./data/WN18RR/entity_embeddings.pt\")\n",
    "relation_embeddings = torch.load(\"./data/WN18RR/relation_embeddings.pt\")\n",
    "Corpus_ = torch.load(\"/content/Corpus_torch.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "cV2uiNpKwHnX",
    "outputId": "2d38d547-4fb5-44f3-85d8-eb7545fa9811"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening node_neighbors pickle object\n",
      "Initial entity dimensions torch.Size([40943, 50]) , relation dimensions torch.Size([11, 50])\n"
     ]
    }
   ],
   "source": [
    "if(args.use_2hop):\n",
    "    print(\"Opening node_neighbors pickle object\")\n",
    "    file = \"/content/2hop.pickle\"\n",
    "    with open(file, 'rb') as handle:\n",
    "        node_neighbors_2hop = pickle.load(handle)\n",
    "\n",
    "entity_embeddings_copied = deepcopy(entity_embeddings)\n",
    "relation_embeddings_copied = deepcopy(relation_embeddings)\n",
    "\n",
    "print(\"Initial entity dimensions {} , relation dimensions {}\".format(\n",
    "    entity_embeddings.size(), relation_embeddings.size()))\n",
    "# %%\n",
    "\n",
    "CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OHFOSgYsgUAG"
   },
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RzuVdPsjV0-0"
   },
   "source": [
    "# File : main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YW9JVaWzWIRC"
   },
   "outputs": [],
   "source": [
    "def save_model(model, name, epoch, model_name):\n",
    "    print(\"Saving Model\")\n",
    "    torch.save(model.state_dict(), \"/content/{0}/trained_{1}.pth\".format(model_name, epoch))\n",
    "    print(\"Done saving Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IpU2kcraV0EJ"
   },
   "outputs": [],
   "source": [
    "def batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):\n",
    "    len_pos_triples = int(\n",
    "        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))\n",
    "\n",
    "    pos_triples = train_indices[:len_pos_triples]\n",
    "    neg_triples = train_indices[len_pos_triples:]\n",
    "\n",
    "    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)\n",
    "\n",
    "    source_embeds = entity_embed[pos_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[pos_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[pos_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    pos_norm = torch.norm(x, p=1, dim=1)\n",
    "\n",
    "    source_embeds = entity_embed[neg_triples[:, 0]]\n",
    "    relation_embeds = relation_embed[neg_triples[:, 1]]\n",
    "    tail_embeds = entity_embed[neg_triples[:, 2]]\n",
    "\n",
    "    x = source_embeds + relation_embeds - tail_embeds\n",
    "    neg_norm = torch.norm(x, p=1, dim=1)\n",
    "\n",
    "    y = torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()\n",
    "\n",
    "    loss = gat_loss_func(pos_norm, neg_norm, y)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def train_gat(args):\n",
    "\n",
    "    # Creating the gat model here.\n",
    "    ####################################\n",
    "\n",
    "    print(\"Defining model\")\n",
    "\n",
    "    print(\n",
    "        \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\n",
    "    # SpKBGATModified : lớp GAT chính\n",
    "    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "\n",
    "    if CUDA:\n",
    "        model_gat.cuda()\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=500, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "    gat_loss_func = nn.MarginRankingLoss(margin=args.margin)\n",
    "\n",
    "    current_batch_2hop_indices = torch.tensor([])\n",
    "    if(args.use_2hop):\n",
    "        current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\n",
    "                                                                          Corpus_.unique_entities_train, node_neighbors_2hop)\n",
    "\n",
    "    if CUDA:\n",
    "        current_batch_2hop_indices = Variable(\n",
    "            torch.LongTensor(current_batch_2hop_indices)).cuda()\n",
    "    else:\n",
    "        current_batch_2hop_indices = Variable(\n",
    "            torch.LongTensor(current_batch_2hop_indices))\n",
    "\n",
    "    epoch_losses = []   # losses of all epochs\n",
    "    print(\"Number of epochs {}\".format(args.epochs_gat))\n",
    "\n",
    "    for epoch in range(args.epochs_gat):\n",
    "        print(\"\\nepoch-> \", epoch)\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_gat.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_gat == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_gat\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_gat) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            # forward pass\n",
    "            entity_embed, relation_embed = model_gat(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\n",
    "            \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = batch_gat_loss(\n",
    "                gat_loss_func, train_indices, entity_embed, relation_embed)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "            print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\n",
    "                iters, end_time_iter - start_time_iter, loss.data.item()))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\"Epoch {} , average loss {} , epoch_time {}\".format(\n",
    "            epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "\n",
    "        save_model(model_gat, args.data, epoch,\n",
    "                   args.output_folder)\n",
    "\n",
    "\n",
    "def train_conv(args):\n",
    "\n",
    "    # Creating convolution model here.\n",
    "    ####################################\n",
    "\n",
    "    print(\"Defining model\")\n",
    "    model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "    print(\"Only Conv model trained\")\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "\n",
    "    if CUDA:\n",
    "        model_conv.cuda()\n",
    "        model_gat.cuda()\n",
    "\n",
    "    model_gat.load_state_dict(torch.load(\n",
    "        '{}/trained_{}.pth'.format(args.output_folder, args.epochs_gat - 1)), strict=False)\n",
    "    model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "    model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "    Corpus_.batch_size = args.batch_size_conv\n",
    "    Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
    "\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "        optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "    margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "    epoch_losses = []   # losses of all epochs\n",
    "    print(\"Number of epochs {}\".format(args.epochs_conv))\n",
    "\n",
    "    for epoch in range(args.epochs_conv):\n",
    "        print(\"\\nepoch-> \", epoch)\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_conv.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_conv == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_conv\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_conv) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            preds = model_conv(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = margin_loss(preds.view(-1), train_values.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "            print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\n",
    "                iters, end_time_iter - start_time_iter, loss.data.item()))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\"Epoch {} , average loss {} , epoch_time {}\".format(\n",
    "            epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "\n",
    "        save_model(model_conv, args.data, epoch,\n",
    "                   args.output_folder + \"conv/\")\n",
    "\n",
    "\n",
    "def evaluate_conv(args, unique_entities):\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "    model_conv.load_state_dict(torch.load(\n",
    "        '{0}conv/trained_{1}.pth'.format(args.output_folder, args.epochs_conv - 1)), strict=False)\n",
    "\n",
    "    model_conv.cuda()\n",
    "    model_conv.eval()\n",
    "    with torch.no_grad():\n",
    "        Corpus_.get_validation_pred(args, model_conv, unique_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mxejLRbqaMVo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UdIoKzwogXDf"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K3doI8Ydgj5k"
   },
   "source": [
    "Traning mô hình GAT trước rồi mới traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 254
    },
    "colab_type": "code",
    "id": "XO7FxwFJadXt",
    "outputId": "7eded5d4-26bc-4c7d-c8c0-0d99c33e4931"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "\n",
      "Model type -> GAT layer with 2 heads used , Initital Embeddings training\n",
      "length of unique_entities  40559\n",
      "Number of epochs 3600\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n"
     ]
    }
   ],
   "source": [
    "train_gat(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "-4ghLUbV3u4y",
    "outputId": "6ee60d4c-0c34-4042-8ffa-97ed2335914e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "Only Conv model trained\n",
      "Number of epochs 200\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n",
      "Saving Model\n",
      "Done saving Model\n"
     ]
    }
   ],
   "source": [
    "train_conv(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8kRuQBb34-O"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "-D2GaRUQ34bx",
    "outputId": "6bc694de-b35f-4936-961e-e892b40b2199"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "sample -  50 4\n",
      "531\n",
      "sample -  22 53\n",
      "532\n",
      "sample -  432 77\n",
      "533\n",
      "sample -  23 21\n",
      "534\n",
      "sample -  1 1\n",
      "535\n",
      "sample -  125 6\n",
      "536\n",
      "sample -  5 3\n",
      "537\n",
      "sample -  11393 12982\n",
      "538\n",
      "sample -  1 1\n",
      "539\n",
      "sample -  5356 904\n",
      "540\n",
      "sample -  1071 904\n",
      "541\n",
      "sample -  49 58\n",
      "542\n",
      "sample -  1 2\n",
      "543\n",
      "sample -  9643 1624\n",
      "544\n",
      "sample -  105 41\n",
      "545\n",
      "sample -  17381 4589\n",
      "546\n",
      "sample -  5 1\n",
      "547\n",
      "sample -  27 1088\n",
      "548\n",
      "sample -  2 1\n",
      "549\n",
      "549\n",
      "sample -  90 35\n",
      "550\n",
      "sample -  80 4\n",
      "551\n",
      "sample -  7278 1360\n",
      "552\n",
      "sample -  1 1\n",
      "553\n",
      "sample -  2 3\n",
      "554\n",
      "sample -  1 1\n",
      "555\n",
      "sample -  1 1\n",
      "556\n",
      "sample -  1 1\n",
      "557\n",
      "sample -  1 1\n",
      "558\n",
      "sample -  159 25\n",
      "559\n",
      "sample -  3 3\n",
      "560\n",
      "sample -  1 1\n",
      "561\n",
      "sample -  36983 14634\n",
      "562\n",
      "sample -  1 4\n",
      "563\n",
      "sample -  416 19\n",
      "564\n",
      "sample -  1 1\n",
      "565\n",
      "sample -  584 5\n",
      "566\n",
      "sample -  1256 500\n",
      "567\n",
      "sample -  19 10\n",
      "568\n",
      "sample -  19 135\n",
      "569\n",
      "sample -  1466 197\n",
      "570\n",
      "sample -  14 5\n",
      "571\n",
      "sample -  1 1\n",
      "572\n",
      "sample -  1 1\n",
      "573\n",
      "sample -  1 1\n",
      "574\n",
      "sample -  1 3\n",
      "575\n",
      "sample -  1 1\n",
      "576\n",
      "sample -  21964 5908\n",
      "577\n",
      "sample -  1 2\n",
      "578\n",
      "sample -  5 3\n",
      "579\n",
      "sample -  1 3\n",
      "580\n",
      "sample -  1 1\n",
      "581\n",
      "sample -  1 1\n",
      "582\n",
      "sample -  8 1\n",
      "583\n",
      "sample -  1 2\n",
      "584\n",
      "sample -  806 119\n",
      "585\n",
      "sample -  3 2\n",
      "586\n",
      "sample -  21 7\n",
      "587\n",
      "587\n",
      "sample -  1 1\n",
      "588\n",
      "sample -  2 5\n",
      "589\n",
      "sample -  1 2\n",
      "590\n",
      "sample -  2 2\n",
      "591\n",
      "sample -  3 8\n",
      "592\n",
      "sample -  1 1\n",
      "593\n",
      "sample -  1 1\n",
      "594\n",
      "sample -  46 20\n",
      "595\n",
      "sample -  989 885\n",
      "596\n",
      "sample -  1 1\n",
      "597\n",
      "sample -  2 3\n",
      "598\n",
      "sample -  3 30\n",
      "599\n",
      "sample -  939 95\n",
      "600\n",
      "600\n",
      "sample -  5 2\n",
      "601\n",
      "sample -  1 2\n",
      "602\n",
      "sample -  1 1\n",
      "603\n",
      "sample -  1 1\n",
      "604\n",
      "sample -  1 1\n",
      "605\n",
      "sample -  1 1\n",
      "606\n",
      "sample -  3824 346\n",
      "607\n",
      "sample -  2 2\n",
      "608\n",
      "608\n",
      "sample -  104 103\n",
      "609\n",
      "sample -  3 3\n",
      "610\n",
      "sample -  19922 28618\n",
      "611\n",
      "sample -  3648 3360\n",
      "612\n",
      "sample -  6 1\n",
      "613\n",
      "sample -  994 361\n",
      "614\n",
      "sample -  1 1\n",
      "615\n",
      "sample -  3 4\n",
      "616\n",
      "sample -  1 1\n",
      "617\n",
      "617\n",
      "sample -  2 1\n",
      "618\n",
      "sample -  1 1\n",
      "619\n",
      "sample -  1 1\n",
      "620\n",
      "sample -  1 1\n",
      "621\n",
      "621\n",
      "sample -  2 2\n",
      "622\n",
      "sample -  15879 6453\n",
      "623\n",
      "sample -  3252 26\n",
      "624\n",
      "sample -  28 16\n",
      "625\n",
      "sample -  12 31\n",
      "626\n",
      "sample -  233 2241\n",
      "627\n",
      "sample -  19 8\n",
      "628\n",
      "sample -  1 1\n",
      "629\n",
      "sample -  28 8\n",
      "630\n",
      "sample -  5 3\n",
      "631\n",
      "sample -  11439 476\n",
      "632\n",
      "sample -  2 1\n",
      "633\n",
      "sample -  2 5\n",
      "634\n",
      "sample -  1 1\n",
      "635\n",
      "sample -  2 1\n",
      "636\n",
      "sample -  2 2\n",
      "637\n",
      "637\n",
      "sample -  10383 7591\n",
      "638\n",
      "sample -  17308 710\n",
      "639\n",
      "sample -  377 10\n",
      "640\n",
      "sample -  7 32\n",
      "641\n",
      "sample -  10 1\n",
      "642\n",
      "sample -  1 1\n",
      "643\n",
      "sample -  3191 2153\n",
      "644\n",
      "sample -  160 21\n",
      "645\n",
      "645\n",
      "sample -  6 3\n",
      "646\n",
      "sample -  3 1\n",
      "647\n",
      "sample -  1 1\n",
      "648\n",
      "sample -  2 1\n",
      "649\n",
      "sample -  13701 2413\n",
      "650\n",
      "sample -  5520 804\n",
      "651\n",
      "sample -  2 1\n",
      "652\n",
      "sample -  1 1\n",
      "653\n",
      "sample -  1 2\n",
      "654\n",
      "sample -  11 4\n",
      "655\n",
      "sample -  1 1\n",
      "656\n",
      "sample -  327 21\n",
      "657\n",
      "sample -  430 20067\n",
      "658\n",
      "sample -  18248 8326\n",
      "659\n",
      "sample -  2 2\n",
      "660\n",
      "sample -  2 2\n",
      "661\n",
      "sample -  2 1\n",
      "662\n",
      "sample -  16 25\n",
      "663\n",
      "sample -  1 1\n",
      "664\n",
      "sample -  5844 7717\n",
      "665\n",
      "sample -  14393 7192\n",
      "666\n",
      "666\n",
      "sample -  567 302\n",
      "667\n",
      "sample -  1 2\n",
      "668\n",
      "sample -  158 62\n",
      "669\n",
      "sample -  356 1064\n",
      "670\n",
      "sample -  1 1\n",
      "671\n",
      "sample -  18 2\n",
      "672\n",
      "sample -  2153 36\n",
      "673\n",
      "673\n",
      "sample -  1 1\n",
      "674\n",
      "674\n",
      "sample -  1 1\n",
      "675\n",
      "sample -  118 98\n",
      "676\n",
      "sample -  1 1\n",
      "677\n",
      "sample -  190 23\n",
      "678\n",
      "sample -  3 2\n",
      "679\n",
      "sample -  218 99\n",
      "680\n",
      "sample -  8 3\n",
      "681\n",
      "681\n",
      "sample -  1 1\n",
      "682\n",
      "sample -  6309 1172\n",
      "683\n",
      "sample -  2397 312\n",
      "684\n",
      "sample -  105 490\n",
      "685\n",
      "sample -  1 1\n",
      "686\n",
      "sample -  1 1\n",
      "687\n",
      "sample -  1 1\n",
      "688\n",
      "sample -  1 1\n",
      "689\n",
      "sample -  262 12\n",
      "690\n",
      "sample -  3348 1539\n",
      "691\n",
      "sample -  1 1\n",
      "692\n",
      "sample -  1 1\n",
      "693\n",
      "sample -  1 1\n",
      "694\n",
      "sample -  3802 923\n",
      "695\n",
      "695\n",
      "sample -  7245 25233\n",
      "696\n",
      "sample -  25 167\n",
      "697\n",
      "sample -  1555 268\n",
      "698\n",
      "sample -  42 186\n",
      "699\n",
      "sample -  2 9\n",
      "700\n",
      "700\n",
      "sample -  6 17\n",
      "701\n",
      "701\n",
      "sample -  4411 292\n",
      "702\n",
      "sample -  1 1\n",
      "703\n",
      "sample -  25623 25670\n",
      "704\n",
      "sample -  1 1\n",
      "705\n",
      "sample -  5 1153\n",
      "706\n",
      "sample -  24326 16293\n",
      "707\n",
      "sample -  1 1\n",
      "708\n",
      "sample -  12 6\n",
      "709\n",
      "sample -  1 1\n",
      "710\n",
      "sample -  18022 7459\n",
      "711\n",
      "sample -  1 1\n",
      "712\n",
      "sample -  2 1\n",
      "713\n",
      "sample -  23282 6527\n",
      "714\n",
      "sample -  1 1\n",
      "715\n",
      "sample -  12867 3061\n",
      "716\n",
      "sample -  1 2\n",
      "717\n",
      "sample -  4 4\n",
      "718\n",
      "sample -  22080 27768\n",
      "719\n",
      "sample -  1075 34\n",
      "720\n",
      "sample -  1 1\n",
      "721\n",
      "sample -  476 32\n",
      "722\n",
      "sample -  1 1\n",
      "723\n",
      "sample -  4 18\n",
      "724\n",
      "724\n",
      "sample -  61 8\n",
      "725\n",
      "sample -  192 56\n",
      "726\n",
      "sample -  1 4\n",
      "727\n",
      "sample -  1 4\n",
      "728\n",
      "sample -  2 2\n",
      "729\n",
      "sample -  24390 9437\n",
      "730\n",
      "sample -  3 2\n",
      "731\n",
      "sample -  1 1\n",
      "732\n",
      "sample -  1 2\n",
      "733\n",
      "sample -  1 1\n",
      "734\n",
      "sample -  66 21\n",
      "735\n",
      "sample -  1335 124\n",
      "736\n",
      "sample -  8 6\n",
      "737\n",
      "sample -  4060 2394\n",
      "738\n",
      "sample -  3857 488\n",
      "739\n",
      "sample -  1212 132\n",
      "740\n",
      "sample -  3 1\n",
      "741\n",
      "sample -  1 1\n",
      "742\n",
      "sample -  4 2\n",
      "743\n",
      "sample -  1 4\n",
      "744\n",
      "sample -  14 2\n",
      "745\n",
      "sample -  1 1\n",
      "746\n",
      "sample -  1 1\n",
      "747\n",
      "sample -  1 1\n",
      "748\n",
      "748\n",
      "sample -  11581 2090\n",
      "749\n",
      "sample -  30273 16959\n",
      "750\n",
      "sample -  1 1\n",
      "751\n",
      "sample -  1 1\n",
      "752\n",
      "sample -  1 1\n",
      "753\n",
      "sample -  24 152\n",
      "754\n",
      "sample -  12064 1710\n",
      "755\n",
      "sample -  3 13\n",
      "756\n",
      "sample -  5705 4959\n",
      "757\n",
      "sample -  1 1\n",
      "758\n",
      "sample -  1 1\n",
      "759\n",
      "sample -  1 2\n",
      "760\n",
      "760\n",
      "sample -  14 29\n",
      "761\n",
      "sample -  1 1\n",
      "762\n",
      "sample -  1 1\n",
      "763\n",
      "sample -  21439 4820\n",
      "764\n",
      "sample -  384 129\n",
      "765\n",
      "sample -  1 3\n",
      "766\n",
      "sample -  1 1\n",
      "767\n",
      "sample -  141 697\n",
      "768\n",
      "sample -  147 30\n",
      "769\n",
      "sample -  701 270\n",
      "770\n",
      "sample -  3 1\n",
      "771\n",
      "sample -  2552 30\n",
      "772\n",
      "sample -  26263 13536\n",
      "773\n",
      "sample -  1699 533\n",
      "774\n",
      "sample -  5763 306\n",
      "775\n",
      "sample -  249 45\n",
      "776\n",
      "sample -  1 1\n",
      "777\n",
      "sample -  108 8\n",
      "778\n",
      "sample -  19114 2117\n",
      "779\n",
      "sample -  1 1\n",
      "780\n",
      "sample -  482 99\n",
      "781\n",
      "sample -  968 32\n",
      "782\n",
      "sample -  3 28\n",
      "783\n",
      "sample -  1 2\n",
      "784\n",
      "sample -  2 2\n",
      "785\n",
      "sample -  1334 1574\n",
      "786\n",
      "sample -  17857 3776\n",
      "787\n",
      "sample -  20133 6031\n",
      "788\n",
      "sample -  38 66\n",
      "789\n",
      "sample -  1 1\n",
      "790\n",
      "sample -  1 1\n",
      "791\n",
      "sample -  1 1\n",
      "792\n",
      "sample -  20251 12048\n",
      "793\n",
      "sample -  1 4\n",
      "794\n",
      "sample -  1 2\n",
      "795\n",
      "sample -  1 4\n",
      "796\n",
      "sample -  25644 28239\n",
      "797\n",
      "sample -  971 678\n",
      "798\n",
      "sample -  1 2\n",
      "799\n",
      "sample -  1071 38\n",
      "800\n",
      "sample -  85 69\n",
      "801\n",
      "801\n",
      "sample -  39115 15577\n",
      "802\n",
      "sample -  117 5\n",
      "803\n",
      "sample -  1 3\n",
      "804\n",
      "sample -  35 8\n",
      "805\n",
      "805\n",
      "sample -  1 1\n",
      "806\n",
      "sample -  1 1\n",
      "807\n",
      "sample -  23598 9297\n",
      "808\n",
      "sample -  1 1\n",
      "809\n",
      "sample -  2 2\n",
      "810\n",
      "sample -  208 459\n",
      "811\n",
      "sample -  1 1\n",
      "812\n",
      "sample -  1 1\n",
      "813\n",
      "sample -  3 7\n",
      "814\n",
      "814\n",
      "sample -  42 219\n",
      "815\n",
      "sample -  3362 10584\n",
      "816\n",
      "sample -  313 507\n",
      "817\n",
      "sample -  755 2382\n",
      "818\n",
      "818\n",
      "sample -  1 1\n",
      "819\n",
      "sample -  1 1\n",
      "820\n",
      "sample -  6487 2520\n",
      "821\n",
      "sample -  4 2\n",
      "822\n",
      "sample -  1 1\n",
      "823\n",
      "sample -  1 1\n",
      "824\n",
      "sample -  1 1\n",
      "825\n",
      "sample -  294 222\n",
      "826\n",
      "sample -  1 1\n",
      "827\n",
      "sample -  15 87\n",
      "828\n",
      "sample -  27911 22701\n",
      "829\n",
      "sample -  1 1\n",
      "830\n",
      "sample -  1 1\n",
      "831\n",
      "sample -  163 18\n",
      "832\n",
      "sample -  1 1\n",
      "833\n",
      "sample -  9852 16392\n",
      "834\n",
      "sample -  23418 22561\n",
      "835\n",
      "sample -  1 1\n",
      "836\n",
      "sample -  5124 7861\n",
      "837\n",
      "sample -  1 1\n",
      "838\n",
      "sample -  889 38\n",
      "839\n",
      "sample -  1 1\n",
      "840\n",
      "sample -  826 260\n",
      "841\n",
      "sample -  1 1\n",
      "842\n",
      "sample -  25296 2849\n",
      "843\n",
      "843\n",
      "sample -  188 41\n",
      "844\n",
      "sample -  1 1\n",
      "845\n",
      "sample -  8316 1884\n",
      "846\n",
      "sample -  4 11\n",
      "847\n",
      "sample -  1 3\n",
      "848\n",
      "sample -  2452 591\n",
      "849\n",
      "sample -  12328 4171\n",
      "850\n",
      "sample -  3 1\n",
      "851\n",
      "sample -  1 1\n",
      "852\n",
      "sample -  2 1\n",
      "853\n",
      "sample -  1 1\n",
      "854\n",
      "sample -  1412 2778\n",
      "855\n",
      "sample -  20167 20296\n",
      "856\n",
      "sample -  1 1\n",
      "857\n",
      "sample -  1 1\n",
      "858\n",
      "sample -  123 70\n",
      "859\n",
      "sample -  1 1\n",
      "860\n",
      "sample -  1 1\n",
      "861\n",
      "sample -  2 1\n",
      "862\n",
      "sample -  22454 18947\n",
      "863\n",
      "sample -  1 1\n",
      "864\n",
      "sample -  90 49\n",
      "865\n",
      "sample -  1 1\n",
      "866\n",
      "sample -  1072 74\n",
      "867\n",
      "sample -  2 1\n",
      "868\n",
      "sample -  2 1\n",
      "869\n",
      "sample -  13832 2250\n",
      "870\n",
      "sample -  4378 3484\n",
      "871\n",
      "sample -  1 1\n",
      "872\n",
      "sample -  2 1\n",
      "873\n",
      "sample -  1 1\n",
      "874\n",
      "sample -  4 2\n",
      "875\n",
      "sample -  1 3\n",
      "876\n",
      "sample -  1 1\n",
      "877\n",
      "sample -  39522 15511\n",
      "878\n",
      "878\n",
      "sample -  1 1\n",
      "879\n",
      "sample -  386 44\n",
      "880\n",
      "sample -  1 1\n",
      "881\n",
      "sample -  1 1\n",
      "882\n",
      "882\n",
      "sample -  1 1\n",
      "883\n",
      "sample -  9631 10107\n",
      "884\n",
      "sample -  8784 14217\n",
      "885\n",
      "sample -  1 1\n",
      "886\n",
      "sample -  1 1\n",
      "887\n",
      "sample -  22653 13645\n",
      "888\n",
      "sample -  1 1\n",
      "889\n",
      "sample -  2681 92\n",
      "890\n",
      "sample -  37 15\n",
      "891\n",
      "sample -  2 1\n",
      "892\n",
      "sample -  3 12\n",
      "893\n",
      "sample -  1 1\n",
      "894\n",
      "sample -  1 1\n",
      "895\n",
      "sample -  844 179\n",
      "896\n",
      "sample -  14 4\n",
      "897\n",
      "sample -  10 3\n",
      "898\n",
      "sample -  5909 3161\n",
      "899\n",
      "899\n",
      "sample -  6417 1939\n",
      "900\n",
      "sample -  1 1\n",
      "901\n",
      "sample -  408 3565\n",
      "902\n",
      "sample -  1 1\n",
      "903\n",
      "sample -  2 12\n",
      "904\n",
      "sample -  2 1\n",
      "905\n",
      "sample -  2 3\n",
      "906\n",
      "sample -  9 2\n",
      "907\n",
      "sample -  2 1\n",
      "908\n",
      "sample -  1 2\n",
      "909\n",
      "sample -  1 1\n",
      "910\n",
      "sample -  1 1\n",
      "911\n",
      "sample -  1 1\n",
      "912\n",
      "sample -  2 1\n",
      "913\n",
      "sample -  1 1\n",
      "914\n",
      "sample -  2 3\n",
      "915\n",
      "sample -  5873 10475\n",
      "916\n",
      "sample -  16773 5941\n",
      "917\n",
      "917\n",
      "sample -  10303 6720\n",
      "918\n",
      "sample -  1 3\n",
      "919\n",
      "sample -  6279 4168\n",
      "920\n",
      "sample -  2040 130\n",
      "921\n",
      "sample -  33868 20076\n",
      "922\n",
      "sample -  18 18\n",
      "923\n",
      "sample -  38253 28554\n",
      "924\n",
      "sample -  3 1\n",
      "925\n",
      "sample -  1405 133\n",
      "926\n",
      "sample -  4 15\n",
      "927\n",
      "sample -  1 1\n",
      "928\n",
      "sample -  1 1\n",
      "929\n",
      "sample -  265 44\n",
      "930\n",
      "sample -  4 14\n",
      "931\n",
      "sample -  47 14\n",
      "932\n",
      "sample -  6 8\n",
      "933\n",
      "sample -  16 11\n",
      "934\n",
      "sample -  4699 2919\n",
      "935\n",
      "sample -  12 20\n",
      "936\n",
      "sample -  7969 2034\n",
      "937\n",
      "sample -  1 1\n",
      "938\n",
      "sample -  1 1\n",
      "939\n",
      "sample -  222 1240\n",
      "940\n",
      "sample -  17 16\n",
      "941\n",
      "sample -  7 6\n",
      "942\n",
      "sample -  77 24\n",
      "943\n",
      "sample -  20 2\n",
      "944\n",
      "sample -  2 9\n",
      "945\n",
      "sample -  31326 35640\n",
      "946\n",
      "sample -  4 6\n",
      "947\n",
      "sample -  543 15\n",
      "948\n",
      "sample -  290 27\n",
      "949\n",
      "sample -  1 1\n",
      "950\n",
      "sample -  1 1\n",
      "951\n",
      "sample -  1 1\n",
      "952\n",
      "sample -  1 1\n",
      "953\n",
      "sample -  57 8\n",
      "954\n",
      "sample -  2 3\n",
      "955\n",
      "sample -  1 1\n",
      "956\n",
      "sample -  1797 258\n",
      "957\n",
      "957\n",
      "sample -  1 1\n",
      "958\n",
      "sample -  7667 6182\n",
      "959\n",
      "sample -  17 18\n",
      "960\n",
      "sample -  2 2\n",
      "961\n",
      "sample -  1 1\n",
      "962\n",
      "962\n",
      "sample -  1 1\n",
      "963\n",
      "sample -  1 1\n",
      "964\n",
      "sample -  1 1\n",
      "965\n",
      "sample -  1062 1524\n",
      "966\n",
      "sample -  3 3\n",
      "967\n",
      "sample -  28310 4314\n",
      "968\n",
      "sample -  17464 14077\n",
      "969\n",
      "sample -  1 1\n",
      "970\n",
      "sample -  1 1\n",
      "971\n",
      "sample -  1 1\n",
      "972\n",
      "sample -  420 76\n",
      "973\n",
      "sample -  1 1\n",
      "974\n",
      "sample -  26250 17652\n",
      "975\n",
      "975\n",
      "sample -  1 1\n",
      "976\n",
      "sample -  1 1\n",
      "977\n",
      "977\n",
      "sample -  6212 1710\n",
      "978\n",
      "978\n",
      "sample -  452 50\n",
      "979\n",
      "sample -  3 2\n",
      "980\n",
      "sample -  9688 2544\n",
      "981\n",
      "sample -  6 10\n",
      "982\n",
      "sample -  1 1\n",
      "983\n",
      "sample -  4367 73\n",
      "984\n",
      "sample -  2 1\n",
      "985\n",
      "sample -  1 1\n",
      "986\n",
      "sample -  3169 1882\n",
      "987\n",
      "987\n",
      "sample -  761 342\n",
      "988\n",
      "988\n",
      "sample -  2734 3506\n",
      "989\n",
      "989\n",
      "sample -  1 1\n",
      "990\n",
      "sample -  1 1\n",
      "991\n",
      "sample -  1 2\n",
      "992\n",
      "sample -  15772 5552\n",
      "993\n",
      "sample -  1 1\n",
      "994\n",
      "sample -  1328 1960\n",
      "995\n",
      "sample -  1 1\n",
      "996\n",
      "sample -  33 11\n",
      "997\n",
      "sample -  239 3\n",
      "998\n",
      "sample -  1 3\n",
      "999\n",
      "sample -  55 82\n",
      "1000\n",
      "sample -  1 1\n",
      "1001\n",
      "sample -  160 11\n",
      "1002\n",
      "sample -  2543 547\n",
      "1003\n",
      "sample -  139 29\n",
      "1004\n",
      "sample -  100 718\n",
      "1005\n",
      "sample -  1 1\n",
      "1006\n",
      "sample -  4 6\n",
      "1007\n",
      "sample -  3 11\n",
      "1008\n",
      "sample -  1 1\n",
      "1009\n",
      "sample -  1 1\n",
      "1010\n",
      "sample -  1 1\n",
      "1011\n",
      "sample -  1428 46\n",
      "1012\n",
      "sample -  34 7\n",
      "1013\n",
      "sample -  1 4\n",
      "1014\n",
      "sample -  1 1\n",
      "1015\n",
      "sample -  56 495\n",
      "1016\n",
      "sample -  1 2\n",
      "1017\n",
      "sample -  42 10\n",
      "1018\n",
      "1018\n",
      "sample -  1310 204\n",
      "1019\n",
      "sample -  16738 3889\n",
      "1020\n",
      "sample -  1 1\n",
      "1021\n",
      "sample -  949 59\n",
      "1022\n",
      "sample -  2854 346\n",
      "1023\n",
      "sample -  17582 8798\n",
      "1024\n",
      "sample -  1 3\n",
      "1025\n",
      "sample -  9 6\n",
      "1026\n",
      "sample -  1 3\n",
      "1027\n",
      "sample -  7112 667\n",
      "1028\n",
      "sample -  5646 4859\n",
      "1029\n",
      "sample -  1 1\n",
      "1030\n",
      "1030\n",
      "sample -  17 69\n",
      "1031\n",
      "sample -  1 1\n",
      "1032\n",
      "sample -  1 2\n",
      "1033\n",
      "sample -  1 1\n",
      "1034\n",
      "sample -  34 5\n",
      "1035\n",
      "sample -  1 1\n",
      "1036\n",
      "sample -  1150 1130\n",
      "1037\n",
      "sample -  222 12\n",
      "1038\n",
      "sample -  15 7\n",
      "1039\n",
      "sample -  1 1\n",
      "1040\n",
      "1040\n",
      "sample -  5 4\n",
      "1041\n",
      "1041\n",
      "sample -  1262 1009\n",
      "1042\n",
      "sample -  2 1\n",
      "1043\n",
      "sample -  1 1\n",
      "1044\n",
      "sample -  2923 5947\n",
      "1045\n",
      "sample -  13705 716\n",
      "1046\n",
      "sample -  8518 10531\n",
      "1047\n",
      "sample -  1983 547\n",
      "1048\n",
      "sample -  1 1\n",
      "1049\n",
      "sample -  48 8\n",
      "1050\n",
      "sample -  1 1\n",
      "1051\n",
      "sample -  1 1\n",
      "1052\n",
      "sample -  11 4\n",
      "1053\n",
      "sample -  1 1\n",
      "1054\n",
      "sample -  1 1\n",
      "1055\n",
      "sample -  84 281\n",
      "1056\n",
      "sample -  2 1\n",
      "1057\n",
      "sample -  28 173\n",
      "1058\n",
      "sample -  66 42\n",
      "1059\n",
      "sample -  2 2\n",
      "1060\n",
      "sample -  1 1\n",
      "1061\n",
      "sample -  5770 9204\n",
      "1062\n",
      "sample -  5 439\n",
      "1063\n",
      "1063\n",
      "sample -  1 2\n",
      "1064\n",
      "1064\n",
      "sample -  1 1\n",
      "1065\n",
      "sample -  1 2\n",
      "1066\n",
      "sample -  1 1\n",
      "1067\n",
      "sample -  1 1\n",
      "1068\n",
      "sample -  2 2\n",
      "1069\n",
      "sample -  19148 652\n",
      "1070\n",
      "sample -  3 1\n",
      "1071\n",
      "sample -  7020 714\n",
      "1072\n",
      "sample -  1 1\n",
      "1073\n",
      "sample -  18 289\n",
      "1074\n",
      "sample -  2144 230\n",
      "1075\n",
      "sample -  49 262\n",
      "1076\n",
      "sample -  573 510\n",
      "1077\n",
      "sample -  1 1\n",
      "1078\n",
      "sample -  1205 34\n",
      "1079\n",
      "sample -  1 1\n",
      "1080\n",
      "sample -  1 1\n",
      "1081\n",
      "sample -  479 81\n",
      "1082\n",
      "1082\n",
      "sample -  5 2\n",
      "1083\n",
      "sample -  102 20\n",
      "1084\n",
      "sample -  10 103\n",
      "1085\n",
      "sample -  1 2\n",
      "1086\n",
      "sample -  269 6\n",
      "1087\n",
      "sample -  626 167\n",
      "1088\n",
      "sample -  1 1\n",
      "1089\n",
      "1089\n",
      "sample -  1 1\n",
      "1090\n",
      "sample -  1 1\n",
      "1091\n",
      "sample -  1 1\n",
      "1092\n",
      "sample -  2 1\n",
      "1093\n",
      "1093\n",
      "sample -  8 7\n",
      "1094\n",
      "sample -  1 1\n",
      "1095\n",
      "sample -  1 1\n",
      "1096\n",
      "1096\n",
      "sample -  1768 864\n",
      "1097\n",
      "sample -  1 1\n",
      "1098\n",
      "sample -  128 23\n",
      "1099\n",
      "sample -  5 5\n",
      "1100\n",
      "sample -  1 1\n",
      "1101\n",
      "sample -  1 2\n",
      "1102\n",
      "1102\n",
      "sample -  1 1\n",
      "1103\n",
      "sample -  3192 451\n",
      "1104\n",
      "sample -  4798 4854\n",
      "1105\n",
      "sample -  13 2\n",
      "1106\n",
      "sample -  2 2\n",
      "1107\n",
      "sample -  7921 5111\n",
      "1108\n",
      "sample -  1 4\n",
      "1109\n",
      "sample -  10387 1726\n",
      "1110\n",
      "sample -  3 1\n",
      "1111\n",
      "sample -  5 15\n",
      "1112\n",
      "sample -  59 5\n",
      "1113\n",
      "sample -  19797 14353\n",
      "1114\n",
      "1114\n",
      "sample -  249 68\n",
      "1115\n",
      "sample -  16 15\n",
      "1116\n",
      "sample -  6953 509\n",
      "1117\n",
      "sample -  58 8\n",
      "1118\n",
      "sample -  22 6\n",
      "1119\n",
      "sample -  1 1\n",
      "1120\n",
      "sample -  1 1\n",
      "1121\n",
      "sample -  7618 1857\n",
      "1122\n",
      "sample -  1 1\n",
      "1123\n",
      "sample -  20620 24963\n",
      "1124\n",
      "sample -  9 13\n",
      "1125\n",
      "1125\n",
      "sample -  2611 5860\n",
      "1126\n",
      "sample -  1 1\n",
      "1127\n",
      "sample -  53 161\n",
      "1128\n",
      "sample -  7 18\n",
      "1129\n",
      "sample -  4639 271\n",
      "1130\n",
      "sample -  2383 2072\n",
      "1131\n",
      "sample -  3 2\n",
      "1132\n",
      "sample -  1 1\n",
      "1133\n",
      "sample -  1 1\n",
      "1134\n",
      "sample -  101 22\n",
      "1135\n",
      "sample -  1 1\n",
      "1136\n",
      "sample -  5092 356\n",
      "1137\n",
      "sample -  5957 791\n",
      "1138\n",
      "sample -  1 2\n",
      "1139\n",
      "sample -  1 1\n",
      "1140\n",
      "sample -  462 68\n",
      "1141\n",
      "sample -  1 1\n",
      "1142\n",
      "sample -  9 80\n",
      "1143\n",
      "sample -  4459 155\n",
      "1144\n",
      "sample -  23 12\n",
      "1145\n",
      "sample -  4 3\n",
      "1146\n",
      "sample -  14 16\n",
      "1147\n",
      "sample -  75 6\n",
      "1148\n",
      "sample -  1 2\n",
      "1149\n",
      "sample -  1 3\n",
      "1150\n",
      "sample -  1 1\n",
      "1151\n",
      "sample -  20 307\n",
      "1152\n",
      "sample -  1 1\n",
      "1153\n",
      "sample -  9685 149\n",
      "1154\n",
      "sample -  1384 267\n",
      "1155\n",
      "sample -  5 1\n",
      "1156\n",
      "sample -  4 1\n",
      "1157\n",
      "sample -  1 1\n",
      "1158\n",
      "sample -  1 1\n",
      "1159\n",
      "sample -  1 5\n",
      "1160\n",
      "sample -  4 5\n",
      "1161\n",
      "sample -  1 2\n",
      "1162\n",
      "sample -  1 1\n",
      "1163\n",
      "sample -  1 2\n",
      "1164\n",
      "sample -  210 14\n",
      "1165\n",
      "sample -  13 11\n",
      "1166\n",
      "sample -  1 1\n",
      "1167\n",
      "sample -  474 21\n",
      "1168\n",
      "sample -  39078 33206\n",
      "1169\n",
      "sample -  31 2\n",
      "1170\n",
      "sample -  1 1\n",
      "1171\n",
      "sample -  1 10\n",
      "1172\n",
      "1172\n",
      "sample -  4 2\n",
      "1173\n",
      "sample -  1 1\n",
      "1174\n",
      "sample -  21162 6770\n",
      "1175\n",
      "sample -  2 1\n",
      "1176\n",
      "sample -  2 1\n",
      "1177\n",
      "sample -  1 1\n",
      "1178\n",
      "1178\n",
      "sample -  364 14\n",
      "1179\n",
      "sample -  1 2\n",
      "1180\n",
      "sample -  1 1\n",
      "1181\n",
      "sample -  1480 2791\n",
      "1182\n",
      "sample -  37 2\n",
      "1183\n",
      "sample -  28460 10521\n",
      "1184\n",
      "sample -  1 1\n",
      "1185\n",
      "sample -  2 11\n",
      "1186\n",
      "sample -  16997 14443\n",
      "1187\n",
      "sample -  2 3\n",
      "1188\n",
      "sample -  20990 10084\n",
      "1189\n",
      "sample -  63 24\n",
      "1190\n",
      "sample -  131 16\n",
      "1191\n",
      "sample -  8228 144\n",
      "1192\n",
      "sample -  4 2\n",
      "1193\n",
      "1193\n",
      "sample -  2 1\n",
      "1194\n",
      "sample -  1 2\n",
      "1195\n",
      "sample -  1 2\n",
      "1196\n",
      "sample -  1 1\n",
      "1197\n",
      "sample -  1 1\n",
      "1198\n",
      "sample -  1 1\n",
      "1199\n",
      "sample -  2 2\n",
      "1200\n",
      "sample -  1 1\n",
      "1201\n",
      "sample -  30 404\n",
      "1202\n",
      "sample -  1 1\n",
      "1203\n",
      "sample -  3207 402\n",
      "1204\n",
      "sample -  1 1\n",
      "1205\n",
      "sample -  1 3\n",
      "1206\n",
      "sample -  1 1\n",
      "1207\n",
      "sample -  1 2\n",
      "1208\n",
      "sample -  1 1\n",
      "1209\n",
      "sample -  58 56\n",
      "1210\n",
      "sample -  217 6222\n",
      "1211\n",
      "sample -  391 546\n",
      "1212\n",
      "sample -  9735 4471\n",
      "1213\n",
      "sample -  24 8\n",
      "1214\n",
      "sample -  25 4\n",
      "1215\n",
      "sample -  17 6\n",
      "1216\n",
      "sample -  1 18\n",
      "1217\n",
      "sample -  12 5\n",
      "1218\n",
      "sample -  3 3\n",
      "1219\n",
      "sample -  4 2\n",
      "1220\n",
      "1220\n",
      "sample -  1 1\n",
      "1221\n",
      "sample -  1 4\n",
      "1222\n",
      "sample -  513 167\n",
      "1223\n",
      "sample -  19 11\n",
      "1224\n",
      "sample -  85 338\n",
      "1225\n",
      "1225\n",
      "sample -  1 2\n",
      "1226\n",
      "sample -  1 1\n",
      "1227\n",
      "1227\n",
      "1227\n",
      "sample -  1859 482\n",
      "1228\n",
      "sample -  2442 157\n",
      "1229\n",
      "sample -  1 1\n",
      "1230\n",
      "sample -  1 3\n",
      "1231\n",
      "sample -  4 10\n",
      "1232\n",
      "sample -  10 37\n",
      "1233\n",
      "1233\n",
      "sample -  5847 547\n",
      "1234\n",
      "sample -  2 9\n",
      "1235\n",
      "sample -  15907 3594\n",
      "1236\n",
      "sample -  2406 1732\n",
      "1237\n",
      "sample -  1 1\n",
      "1238\n",
      "sample -  8527 3718\n",
      "1239\n",
      "sample -  5912 4280\n",
      "1240\n",
      "sample -  36046 30492\n",
      "1241\n",
      "sample -  2541 519\n",
      "1242\n",
      "sample -  1 3\n",
      "1243\n",
      "sample -  18 11\n",
      "1244\n",
      "sample -  1 1\n",
      "1245\n",
      "sample -  3 3\n",
      "1246\n",
      "sample -  85 261\n",
      "1247\n",
      "sample -  79 15\n",
      "1248\n",
      "sample -  1 1\n",
      "1249\n",
      "1249\n",
      "sample -  3 1\n",
      "1250\n",
      "sample -  1 8\n",
      "1251\n",
      "sample -  3 23\n",
      "1252\n",
      "sample -  2 1\n",
      "1253\n",
      "sample -  1890 7350\n",
      "1254\n",
      "sample -  1 1\n",
      "1255\n",
      "sample -  39 71\n",
      "1256\n",
      "sample -  1 1\n",
      "1257\n",
      "sample -  1 1\n",
      "1258\n",
      "1258\n",
      "sample -  1630 2676\n",
      "1259\n",
      "sample -  19611 17920\n",
      "1260\n",
      "sample -  1 1\n",
      "1261\n",
      "sample -  1 4\n",
      "1262\n",
      "sample -  9 2\n",
      "1263\n",
      "sample -  14358 17326\n",
      "1264\n",
      "sample -  946 150\n",
      "1265\n",
      "sample -  5 40\n",
      "1266\n",
      "sample -  1 2\n",
      "1267\n",
      "sample -  2 2\n",
      "1268\n",
      "sample -  3 10\n",
      "1269\n",
      "sample -  90 1\n",
      "1270\n",
      "sample -  1 2\n",
      "1271\n",
      "sample -  1332 1380\n",
      "1272\n",
      "sample -  1 1\n",
      "1273\n",
      "sample -  1 1\n",
      "1274\n",
      "sample -  261 4\n",
      "1275\n",
      "sample -  8239 717\n",
      "1276\n",
      "sample -  2515 203\n",
      "1277\n",
      "sample -  1 1\n",
      "1278\n",
      "1278\n",
      "sample -  1 1\n",
      "1279\n",
      "sample -  8 3\n",
      "1280\n",
      "sample -  1055 409\n",
      "1281\n",
      "1281\n",
      "sample -  1 1\n",
      "1282\n",
      "sample -  37 30\n",
      "1283\n",
      "1283\n",
      "sample -  246 285\n",
      "1284\n",
      "sample -  6594 604\n",
      "1285\n",
      "sample -  7 5\n",
      "1286\n",
      "sample -  1 1\n",
      "1287\n",
      "sample -  1 1\n",
      "1288\n",
      "sample -  1 1\n",
      "1289\n",
      "sample -  186 24\n",
      "1290\n",
      "sample -  7 3\n",
      "1291\n",
      "sample -  1 1\n",
      "1292\n",
      "sample -  9 58\n",
      "1293\n",
      "1293\n",
      "sample -  1 1\n",
      "1294\n",
      "1294\n",
      "sample -  612 20\n",
      "1295\n",
      "1295\n",
      "sample -  28 9\n",
      "1296\n",
      "sample -  28005 12202\n",
      "1297\n",
      "sample -  10 10\n",
      "1298\n",
      "sample -  21 47\n",
      "1299\n",
      "sample -  13 6\n",
      "1300\n",
      "sample -  26974 28859\n",
      "1301\n",
      "sample -  2 2\n",
      "1302\n",
      "sample -  570 162\n",
      "1303\n",
      "sample -  2 1\n",
      "1304\n",
      "sample -  1 1\n",
      "1305\n",
      "sample -  370 79\n",
      "1306\n",
      "sample -  5205 118\n",
      "1307\n",
      "sample -  1 1\n",
      "1308\n",
      "sample -  1 1\n",
      "1309\n",
      "sample -  3826 701\n",
      "1310\n",
      "sample -  1 4\n",
      "1311\n",
      "sample -  1 2\n",
      "1312\n",
      "sample -  22 17\n",
      "1313\n",
      "sample -  1635 607\n",
      "1314\n",
      "sample -  1 1\n",
      "1315\n",
      "sample -  6 2\n",
      "1316\n",
      "sample -  1 1\n",
      "1317\n",
      "sample -  2 11\n",
      "1318\n",
      "sample -  1 1\n",
      "1319\n",
      "sample -  14275 14763\n",
      "1320\n",
      "sample -  1 3\n",
      "1321\n",
      "sample -  1 1\n",
      "1322\n",
      "sample -  461 1942\n",
      "1323\n",
      "sample -  2 1\n",
      "1324\n",
      "1324\n",
      "sample -  1 119\n",
      "1325\n",
      "sample -  10 2\n",
      "1326\n",
      "sample -  826 312\n",
      "1327\n",
      "sample -  1 1\n",
      "1328\n",
      "sample -  8 15\n",
      "1329\n",
      "sample -  1 3\n",
      "1330\n",
      "sample -  1 2\n",
      "1331\n",
      "sample -  293 260\n",
      "1332\n",
      "sample -  1 1\n",
      "1333\n",
      "sample -  1 1\n",
      "1334\n",
      "sample -  13403 2730\n",
      "1335\n",
      "sample -  2 3\n",
      "1336\n",
      "sample -  3 2\n",
      "1337\n",
      "sample -  1 1\n",
      "1338\n",
      "sample -  1 1\n",
      "1339\n",
      "sample -  1 1\n",
      "1340\n",
      "sample -  96 39\n",
      "1341\n",
      "sample -  3322 2248\n",
      "1342\n",
      "sample -  5 1\n",
      "1343\n",
      "sample -  4 2\n",
      "1344\n",
      "sample -  1 1\n",
      "1345\n",
      "sample -  1 2\n",
      "1346\n",
      "sample -  63 9\n",
      "1347\n",
      "sample -  36983 16625\n",
      "1348\n",
      "1348\n",
      "sample -  128 7\n",
      "1349\n",
      "sample -  146 5695\n",
      "1350\n",
      "sample -  1 4\n",
      "1351\n",
      "sample -  1 1\n",
      "1352\n",
      "sample -  1 1\n",
      "1353\n",
      "sample -  1 2\n",
      "1354\n",
      "sample -  1 1\n",
      "1355\n",
      "sample -  2 5\n",
      "1356\n",
      "sample -  157 724\n",
      "1357\n",
      "sample -  2 6\n",
      "1358\n",
      "sample -  2 2\n",
      "1359\n",
      "sample -  1 1\n",
      "1360\n",
      "sample -  1 1\n",
      "1361\n",
      "sample -  73 27\n",
      "1362\n",
      "1362\n",
      "sample -  1 1\n",
      "1363\n",
      "sample -  1 2\n",
      "1364\n",
      "sample -  1 1\n",
      "1365\n",
      "sample -  1 1\n",
      "1366\n",
      "sample -  7 4\n",
      "1367\n",
      "sample -  24359 29913\n",
      "1368\n",
      "1368\n",
      "sample -  5 6\n",
      "1369\n",
      "sample -  1 1\n",
      "1370\n",
      "sample -  78 11\n",
      "1371\n",
      "sample -  1 2\n",
      "1372\n",
      "sample -  8 5\n",
      "1373\n",
      "sample -  1028 42\n",
      "1374\n",
      "sample -  4572 663\n",
      "1375\n",
      "sample -  1 1\n",
      "1376\n",
      "sample -  1 1\n",
      "1377\n",
      "sample -  4 2\n",
      "1378\n",
      "sample -  891 2208\n",
      "1379\n",
      "sample -  35033 22905\n",
      "1380\n",
      "1380\n",
      "sample -  4832 511\n",
      "1381\n",
      "sample -  3 2\n",
      "1382\n",
      "sample -  3 1\n",
      "1383\n",
      "sample -  42 12\n",
      "1384\n",
      "sample -  2 1\n",
      "1385\n",
      "sample -  25 3\n",
      "1386\n",
      "sample -  2 3\n",
      "1387\n",
      "sample -  4 8\n",
      "1388\n",
      "sample -  16768 2055\n",
      "1389\n",
      "sample -  2 1\n",
      "1390\n",
      "sample -  1 1\n",
      "1391\n",
      "sample -  20 2\n",
      "1392\n",
      "sample -  1 1\n",
      "1393\n",
      "sample -  15 18\n",
      "1394\n",
      "sample -  349 1601\n",
      "1395\n",
      "sample -  2 1\n",
      "1396\n",
      "sample -  1 2\n",
      "1397\n",
      "sample -  18 25\n",
      "1398\n",
      "sample -  304 79\n",
      "1399\n",
      "sample -  1 1\n",
      "1400\n",
      "sample -  1 2\n",
      "1401\n",
      "sample -  1 1\n",
      "1402\n",
      "sample -  128 2\n",
      "1403\n",
      "sample -  1 1\n",
      "1404\n",
      "1404\n",
      "sample -  1 1\n",
      "1405\n",
      "sample -  1 1\n",
      "1406\n",
      "sample -  1 1\n",
      "1407\n",
      "sample -  1 1\n",
      "1408\n",
      "sample -  1 2\n",
      "1409\n",
      "sample -  1 2\n",
      "1410\n",
      "sample -  386 43\n",
      "1411\n",
      "sample -  4 1\n",
      "1412\n",
      "sample -  1 1\n",
      "1413\n",
      "sample -  20 3\n",
      "1414\n",
      "sample -  2 10\n",
      "1415\n",
      "sample -  1 1\n",
      "1416\n",
      "sample -  3094 1068\n",
      "1417\n",
      "sample -  5831 282\n",
      "1418\n",
      "1418\n",
      "sample -  14 80\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "1419\n",
      "sample -  34637 36689\n",
      "1420\n",
      "sample -  11 99\n",
      "1421\n",
      "sample -  4307 406\n",
      "1422\n",
      "sample -  63 15\n",
      "1423\n",
      "sample -  570 27\n",
      "1424\n",
      "sample -  1 1\n",
      "1425\n",
      "sample -  3 4\n",
      "1426\n",
      "1426\n",
      "sample -  5 2\n",
      "1427\n",
      "sample -  3 4\n",
      "1428\n",
      "sample -  33 151\n",
      "1429\n",
      "sample -  90 15\n",
      "1430\n",
      "sample -  26381 19014\n",
      "1431\n",
      "sample -  4 9\n",
      "1432\n",
      "sample -  1 1\n",
      "1433\n",
      "sample -  1 3\n",
      "1434\n",
      "sample -  25 9\n",
      "1435\n",
      "sample -  1 1\n",
      "1436\n",
      "sample -  1 1\n",
      "1437\n",
      "sample -  1 1\n",
      "1438\n",
      "sample -  3 3\n",
      "1439\n",
      "sample -  1 1\n",
      "1440\n",
      "sample -  1 1\n",
      "1441\n",
      "sample -  13 4\n",
      "1442\n",
      "sample -  1 2\n",
      "1443\n",
      "sample -  1 1\n",
      "1444\n",
      "sample -  1 1\n",
      "1445\n",
      "sample -  29 150\n",
      "1446\n",
      "sample -  66 536\n",
      "1447\n",
      "sample -  196 180\n",
      "1448\n",
      "sample -  234 62\n",
      "1449\n",
      "sample -  369 125\n",
      "1450\n",
      "sample -  93 15744\n",
      "1451\n",
      "1451\n",
      "sample -  14921 1710\n",
      "1452\n",
      "sample -  2 3\n",
      "1453\n",
      "sample -  9306 1192\n",
      "1454\n",
      "sample -  1 1\n",
      "1455\n",
      "sample -  14 23\n",
      "1456\n",
      "sample -  20640 2041\n",
      "1457\n",
      "sample -  2161 2891\n",
      "1458\n",
      "sample -  1 2\n",
      "1459\n",
      "sample -  3 1\n",
      "1460\n",
      "sample -  14 118\n",
      "1461\n",
      "1461\n",
      "sample -  1 1\n",
      "1462\n",
      "sample -  71 42\n",
      "1463\n",
      "sample -  2 1\n",
      "1464\n",
      "sample -  1 1\n",
      "1465\n",
      "sample -  3174 550\n",
      "1466\n",
      "sample -  35956 18068\n",
      "1467\n",
      "sample -  1 1\n",
      "1468\n",
      "1468\n",
      "sample -  1 1\n",
      "1469\n",
      "sample -  1 1\n",
      "1470\n",
      "sample -  39980 30061\n",
      "1471\n",
      "sample -  1 1\n",
      "1472\n",
      "sample -  1 2\n",
      "1473\n",
      "sample -  1 2\n",
      "1474\n",
      "sample -  1 1\n",
      "1475\n",
      "sample -  2 1\n",
      "1476\n",
      "sample -  781 119\n",
      "1477\n",
      "sample -  1 1\n",
      "1478\n",
      "sample -  5 25\n",
      "1479\n",
      "sample -  24 78\n",
      "1480\n",
      "sample -  2962 300\n",
      "1481\n",
      "sample -  12903 1887\n",
      "1482\n",
      "sample -  2174 1135\n",
      "1483\n",
      "sample -  1 1\n",
      "1484\n",
      "sample -  25055 10877\n",
      "1485\n",
      "sample -  25 5\n",
      "1486\n",
      "sample -  1 1\n",
      "1487\n",
      "sample -  10810 4061\n",
      "1488\n",
      "sample -  1 1\n",
      "1489\n",
      "sample -  2801 28\n",
      "1490\n",
      "sample -  2 2\n",
      "1491\n",
      "sample -  2 2\n",
      "1492\n",
      "sample -  2 7\n",
      "1493\n",
      "sample -  819 3506\n",
      "1494\n",
      "sample -  1 1\n",
      "1495\n",
      "sample -  1 1\n",
      "1496\n",
      "sample -  1 1\n",
      "1497\n",
      "sample -  9 6\n",
      "1498\n",
      "sample -  4 8\n",
      "1499\n",
      "sample -  1 1\n",
      "1500\n",
      "sample -  16 12\n",
      "1501\n",
      "sample -  1 1\n",
      "1502\n",
      "sample -  2676 8452\n",
      "1503\n",
      "sample -  114 197\n",
      "1504\n",
      "sample -  1 1\n",
      "1505\n",
      "sample -  50 19\n",
      "1506\n",
      "sample -  1 1\n",
      "1507\n",
      "sample -  1 1\n",
      "1508\n",
      "sample -  6 1\n",
      "1509\n",
      "1509\n",
      "sample -  121 23\n",
      "1510\n",
      "sample -  15316 1846\n",
      "1511\n",
      "sample -  1 1\n",
      "1512\n",
      "sample -  1 2\n",
      "1513\n",
      "sample -  1 1\n",
      "1514\n",
      "sample -  15570 12012\n",
      "1515\n",
      "sample -  10 1\n",
      "1516\n",
      "sample -  1 1\n",
      "1517\n",
      "sample -  9 26\n",
      "1518\n",
      "sample -  1 1\n",
      "1519\n",
      "sample -  3 1\n",
      "1520\n",
      "1520\n",
      "sample -  1 1\n",
      "1521\n",
      "sample -  21 87\n",
      "1522\n",
      "sample -  1206 20\n",
      "1523\n",
      "sample -  1 1\n",
      "1524\n",
      "sample -  3 2\n",
      "1525\n",
      "sample -  1 1\n",
      "1526\n",
      "sample -  25 7\n",
      "1527\n",
      "sample -  1 1\n",
      "1528\n",
      "sample -  3 3\n",
      "1529\n",
      "sample -  10 7\n",
      "1530\n",
      "sample -  3788 224\n",
      "1531\n",
      "sample -  1 13\n",
      "1532\n",
      "sample -  6470 1976\n",
      "1533\n",
      "sample -  17 26\n",
      "1534\n",
      "sample -  8696 6067\n",
      "1535\n",
      "sample -  1 1\n",
      "1536\n",
      "sample -  1 1\n",
      "1537\n",
      "sample -  31389 13728\n",
      "1538\n",
      "sample -  58 230\n",
      "1539\n",
      "sample -  649 98\n",
      "1540\n",
      "sample -  1 1\n",
      "1541\n",
      "sample -  11150 2235\n",
      "1542\n",
      "sample -  38244 17287\n",
      "1543\n",
      "sample -  2 1\n",
      "1544\n",
      "sample -  35858 29976\n",
      "1545\n",
      "sample -  1 1\n",
      "1546\n",
      "sample -  2 1\n",
      "1547\n",
      "sample -  3 29\n",
      "1548\n",
      "sample -  15721 2920\n",
      "1549\n",
      "sample -  5 27\n",
      "1550\n",
      "sample -  7126 3850\n",
      "1551\n",
      "sample -  25649 39910\n",
      "1552\n",
      "sample -  4997 1679\n",
      "1553\n",
      "sample -  40 3\n",
      "1554\n",
      "sample -  46 13\n",
      "1555\n",
      "sample -  28160 8620\n",
      "1556\n",
      "sample -  88 32\n",
      "1557\n",
      "sample -  1 2\n",
      "1558\n",
      "sample -  11648 6206\n",
      "1559\n",
      "sample -  10 5\n",
      "1560\n",
      "sample -  1 1\n",
      "1561\n",
      "sample -  1 2\n",
      "1562\n",
      "sample -  3 1\n",
      "1563\n",
      "sample -  8684 504\n",
      "1564\n",
      "sample -  1 1\n",
      "1565\n",
      "sample -  12986 21556\n",
      "1566\n",
      "sample -  15190 2969\n",
      "1567\n",
      "sample -  15 4\n",
      "1568\n",
      "sample -  33 3\n",
      "1569\n",
      "sample -  4437 426\n",
      "1570\n",
      "sample -  332 761\n",
      "1571\n",
      "sample -  260 304\n",
      "1572\n",
      "sample -  1 1\n",
      "1573\n",
      "sample -  292 18\n",
      "1574\n",
      "1574\n",
      "sample -  2 1\n",
      "1575\n",
      "sample -  3488 312\n",
      "1576\n",
      "sample -  1 2\n",
      "1577\n",
      "sample -  357 31\n",
      "1578\n",
      "sample -  1 1\n",
      "1579\n",
      "sample -  2 1\n",
      "1580\n",
      "sample -  424 14\n",
      "1581\n",
      "sample -  1 1\n",
      "1582\n",
      "sample -  20 124\n",
      "1583\n",
      "sample -  1 1\n",
      "1584\n",
      "sample -  1 1\n",
      "1585\n",
      "sample -  142 13\n",
      "1586\n",
      "1586\n",
      "sample -  1 1\n",
      "1587\n",
      "sample -  2 1\n",
      "1588\n",
      "sample -  1 2\n",
      "1589\n",
      "sample -  4 1\n",
      "1590\n",
      "sample -  1336 177\n",
      "1591\n",
      "sample -  19 8\n",
      "1592\n",
      "sample -  3 27\n",
      "1593\n",
      "sample -  1 1\n",
      "1594\n",
      "sample -  92 343\n",
      "1595\n",
      "sample -  3 2\n",
      "1596\n",
      "sample -  1 1\n",
      "1597\n",
      "sample -  15 4\n",
      "1598\n",
      "sample -  1914 317\n",
      "1599\n",
      "sample -  84 18\n",
      "1600\n",
      "1600\n",
      "sample -  23682 28560\n",
      "1601\n",
      "sample -  1 1\n",
      "1602\n",
      "sample -  36824 28392\n",
      "1603\n",
      "sample -  10 4\n",
      "1604\n",
      "sample -  438 264\n",
      "1605\n",
      "sample -  135 25\n",
      "1606\n",
      "sample -  4 4\n",
      "1607\n",
      "sample -  3 2\n",
      "1608\n",
      "sample -  7442 372\n",
      "1609\n",
      "sample -  5 8\n",
      "1610\n",
      "sample -  1 6\n",
      "1611\n",
      "sample -  1 1\n",
      "1612\n",
      "sample -  8491 8160\n",
      "1613\n",
      "sample -  15526 4517\n",
      "1614\n",
      "sample -  3 1\n",
      "1615\n",
      "sample -  1474 273\n",
      "1616\n",
      "sample -  1 1\n",
      "1617\n",
      "sample -  13 8\n",
      "1618\n",
      "sample -  1 1\n",
      "1619\n",
      "sample -  3 1\n",
      "1620\n",
      "sample -  1555 165\n",
      "1621\n",
      "sample -  92 11\n",
      "1622\n",
      "1622\n",
      "sample -  1 1\n",
      "1623\n",
      "sample -  1 1\n",
      "1624\n",
      "1624\n",
      "sample -  324 327\n",
      "1625\n",
      "sample -  7744 3613\n",
      "1626\n",
      "sample -  1 1\n",
      "1627\n",
      "sample -  1 1\n",
      "1628\n",
      "sample -  1 21\n",
      "1629\n",
      "sample -  1 1\n",
      "1630\n",
      "sample -  2 1\n",
      "1631\n",
      "sample -  3 1\n",
      "1632\n",
      "sample -  1 1\n",
      "1633\n",
      "sample -  1036 3593\n",
      "1634\n",
      "sample -  128 16102\n",
      "1635\n",
      "sample -  13 53\n",
      "1636\n",
      "sample -  1 103\n",
      "1637\n",
      "sample -  53 85\n",
      "1638\n",
      "sample -  6296 218\n",
      "1639\n",
      "sample -  1 3\n",
      "1640\n",
      "sample -  1 2\n",
      "1641\n",
      "sample -  2 1\n",
      "1642\n",
      "sample -  1 1\n",
      "1643\n",
      "sample -  4 2\n",
      "1644\n",
      "sample -  2628 193\n",
      "1645\n",
      "sample -  1 1\n",
      "1646\n",
      "sample -  1 1\n",
      "1647\n",
      "sample -  729 346\n",
      "1648\n",
      "sample -  1 1\n",
      "1649\n",
      "sample -  1 1\n",
      "1650\n",
      "sample -  5 1\n",
      "1651\n",
      "sample -  11266 2450\n",
      "1652\n",
      "sample -  1 1\n",
      "1653\n",
      "sample -  202 500\n",
      "1654\n",
      "sample -  1 1\n",
      "1655\n",
      "sample -  4 7\n",
      "1656\n",
      "sample -  2 2\n",
      "1657\n",
      "sample -  117 165\n",
      "1658\n",
      "1658\n",
      "sample -  43 6\n",
      "1659\n",
      "sample -  3 1\n",
      "1660\n",
      "sample -  124 169\n",
      "1661\n",
      "1661\n",
      "sample -  534 81\n",
      "1662\n",
      "sample -  2 1\n",
      "1663\n",
      "sample -  10847 3070\n",
      "1664\n",
      "sample -  32749 24415\n",
      "1665\n",
      "sample -  16 13\n",
      "1666\n",
      "sample -  1 1\n",
      "1667\n",
      "1667\n",
      "sample -  5 1\n",
      "1668\n",
      "sample -  17025 3411\n",
      "1669\n",
      "sample -  2 3\n",
      "1670\n",
      "sample -  1 1\n",
      "1671\n",
      "sample -  2 1\n",
      "1672\n",
      "sample -  46 47\n",
      "1673\n",
      "sample -  2 2\n",
      "1674\n",
      "sample -  4 1\n",
      "1675\n",
      "sample -  1 2\n",
      "1676\n",
      "sample -  23 12\n",
      "1677\n",
      "sample -  1 1\n",
      "1678\n",
      "sample -  1 3\n",
      "1679\n",
      "sample -  7 2\n",
      "1680\n",
      "sample -  1807 159\n",
      "1681\n",
      "sample -  1 1\n",
      "1682\n",
      "sample -  3155 1665\n",
      "1683\n",
      "1683\n",
      "sample -  1 1\n",
      "1684\n",
      "sample -  1 2\n",
      "1685\n",
      "sample -  1 1\n",
      "1686\n",
      "sample -  1 1\n",
      "1687\n",
      "sample -  1 1\n",
      "1688\n",
      "sample -  1211 391\n",
      "1689\n",
      "sample -  2 1\n",
      "1690\n",
      "sample -  9 5164\n",
      "1691\n",
      "sample -  16 2\n",
      "1692\n",
      "sample -  1079 114\n",
      "1693\n",
      "sample -  1 1\n",
      "1694\n",
      "sample -  16 5384\n",
      "1695\n",
      "sample -  1 1\n",
      "1696\n",
      "sample -  2 1\n",
      "1697\n",
      "sample -  3087 231\n",
      "1698\n",
      "sample -  5 1\n",
      "1699\n",
      "sample -  1 2\n",
      "1700\n",
      "sample -  1 1\n",
      "1701\n",
      "1701\n",
      "sample -  2200 110\n",
      "1702\n",
      "sample -  1 1\n",
      "1703\n",
      "sample -  1 1\n",
      "1704\n",
      "sample -  2 2\n",
      "1705\n",
      "sample -  2 7\n",
      "1706\n",
      "sample -  2075 369\n",
      "1707\n",
      "sample -  1 1\n",
      "1708\n",
      "sample -  17 5\n",
      "1709\n",
      "sample -  139 19\n",
      "1710\n",
      "sample -  14 83\n",
      "1711\n",
      "sample -  6 27\n",
      "1712\n",
      "sample -  1 1\n",
      "1713\n",
      "sample -  1 7\n",
      "1714\n",
      "sample -  1 1\n",
      "1715\n",
      "sample -  4 2\n",
      "1716\n",
      "sample -  396 21\n",
      "1717\n",
      "sample -  2 1\n",
      "1718\n",
      "sample -  5 5\n",
      "1719\n",
      "sample -  2 1\n",
      "1720\n",
      "sample -  5 3\n",
      "1721\n",
      "sample -  1 1\n",
      "1722\n",
      "sample -  1 1\n",
      "1723\n",
      "sample -  9 4\n",
      "1724\n",
      "1724\n",
      "1724\n",
      "sample -  1 1\n",
      "1725\n",
      "sample -  1 1\n",
      "1726\n",
      "sample -  206 170\n",
      "1727\n",
      "sample -  2 1\n",
      "1728\n",
      "sample -  7653 2054\n",
      "1729\n",
      "sample -  1 1\n",
      "1730\n",
      "sample -  12 4\n",
      "1731\n",
      "sample -  29069 7043\n",
      "1732\n",
      "sample -  8 38\n",
      "1733\n",
      "sample -  3 2\n",
      "1734\n",
      "sample -  1 1\n",
      "1735\n",
      "sample -  1 1\n",
      "1736\n",
      "sample -  1 4\n",
      "1737\n",
      "sample -  18 7\n",
      "1738\n",
      "sample -  1 1\n",
      "1739\n",
      "sample -  1 2\n",
      "1740\n",
      "sample -  1 1\n",
      "1741\n",
      "sample -  3 82\n",
      "1742\n",
      "sample -  1 1\n",
      "1743\n",
      "sample -  1 1\n",
      "1744\n",
      "sample -  1 1\n",
      "1745\n",
      "sample -  4 3\n",
      "1746\n",
      "sample -  11 4\n",
      "1747\n",
      "sample -  3 2\n",
      "1748\n",
      "sample -  23101 6975\n",
      "1749\n",
      "sample -  6429 5216\n",
      "1750\n",
      "sample -  96 29\n",
      "1751\n",
      "sample -  1 1\n",
      "1752\n",
      "sample -  9 3\n",
      "1753\n",
      "sample -  17 1035\n",
      "1754\n",
      "sample -  1 11\n",
      "1755\n",
      "sample -  1 1\n",
      "1756\n",
      "sample -  47 4\n",
      "1757\n",
      "sample -  4 4\n",
      "1758\n",
      "sample -  39 5\n",
      "1759\n",
      "sample -  5 8\n",
      "1760\n",
      "sample -  1 1\n",
      "1761\n",
      "sample -  5 17\n",
      "1762\n",
      "1762\n",
      "sample -  21 5\n",
      "1763\n",
      "sample -  31 3\n",
      "1764\n",
      "sample -  1 1\n",
      "1765\n",
      "sample -  2 2\n",
      "1766\n",
      "sample -  1 1\n",
      "1767\n",
      "sample -  3 3\n",
      "1768\n",
      "sample -  1 1\n",
      "1769\n",
      "sample -  7 29\n",
      "1770\n",
      "sample -  6001 430\n",
      "1771\n",
      "sample -  1 1\n",
      "1772\n",
      "sample -  1827 132\n",
      "1773\n",
      "sample -  1 1\n",
      "1774\n",
      "sample -  1689 40\n",
      "1775\n",
      "sample -  9 2\n",
      "1776\n",
      "sample -  11 1\n",
      "1777\n",
      "1777\n",
      "sample -  2 1\n",
      "1778\n",
      "sample -  2 1\n",
      "1779\n",
      "sample -  1 1\n",
      "1780\n",
      "sample -  1 1\n",
      "1781\n",
      "sample -  4836 108\n",
      "1782\n",
      "sample -  659 239\n",
      "1783\n",
      "sample -  69 290\n",
      "1784\n",
      "sample -  9744 4858\n",
      "1785\n",
      "sample -  6 1\n",
      "1786\n",
      "sample -  252 24\n",
      "1787\n",
      "sample -  681 1437\n",
      "1788\n",
      "sample -  1 1\n",
      "1789\n",
      "1789\n",
      "sample -  13 3\n",
      "1790\n",
      "sample -  3 4\n",
      "1791\n",
      "sample -  1442 869\n",
      "1792\n",
      "sample -  61 37\n",
      "1793\n",
      "1793\n",
      "sample -  2 3\n",
      "1794\n",
      "sample -  334 304\n",
      "1795\n",
      "sample -  303 17\n",
      "1796\n",
      "sample -  182 21\n",
      "1797\n",
      "sample -  3 4\n",
      "1798\n",
      "sample -  2 5\n",
      "1799\n",
      "sample -  1 1\n",
      "1800\n",
      "sample -  1 1\n",
      "1801\n",
      "1801\n",
      "sample -  18 4\n",
      "1802\n",
      "sample -  115 630\n",
      "1803\n",
      "sample -  28 8\n",
      "1804\n",
      "sample -  614 1926\n",
      "1805\n",
      "sample -  26405 15959\n",
      "1806\n",
      "sample -  384 9582\n",
      "1807\n",
      "sample -  32592 37745\n",
      "1808\n",
      "sample -  38 56\n",
      "1809\n",
      "sample -  2 1\n",
      "1810\n",
      "1810\n",
      "sample -  1 1\n",
      "1811\n",
      "sample -  24557 3227\n",
      "1812\n",
      "1812\n",
      "sample -  182 53\n",
      "1813\n",
      "sample -  1 1\n",
      "1814\n",
      "sample -  1 1\n",
      "1815\n",
      "sample -  1 2\n",
      "1816\n",
      "sample -  60 301\n",
      "1817\n",
      "sample -  1 1\n",
      "1818\n",
      "sample -  1 1\n",
      "1819\n",
      "sample -  22719 15343\n",
      "1820\n",
      "sample -  3 1\n",
      "1821\n",
      "sample -  1 1\n",
      "1822\n",
      "sample -  29759 26081\n",
      "1823\n",
      "sample -  1315 1036\n",
      "1824\n",
      "sample -  38053 4343\n",
      "1825\n",
      "sample -  4 2\n",
      "1826\n",
      "sample -  3321 15435\n",
      "1827\n",
      "sample -  2 1\n",
      "1828\n",
      "sample -  1 2\n",
      "1829\n",
      "sample -  19809 25390\n",
      "1830\n",
      "sample -  19229 16938\n",
      "1831\n",
      "sample -  1 1\n",
      "1832\n",
      "sample -  3506 668\n",
      "1833\n",
      "sample -  5 16\n",
      "1834\n",
      "sample -  8 20\n",
      "1835\n",
      "sample -  1 1\n",
      "1836\n",
      "sample -  28 34\n",
      "1837\n",
      "sample -  1 1\n",
      "1838\n",
      "sample -  6841 681\n",
      "1839\n",
      "sample -  4412 23744\n",
      "1840\n",
      "sample -  6 10\n",
      "1841\n",
      "sample -  27 11\n",
      "1842\n",
      "sample -  11554 1411\n",
      "1843\n",
      "sample -  2 2\n",
      "1844\n",
      "1844\n",
      "sample -  1 1\n",
      "1845\n",
      "sample -  592 181\n",
      "1846\n",
      "sample -  1 1\n",
      "1847\n",
      "sample -  341 1138\n",
      "1848\n",
      "sample -  2 1\n",
      "1849\n",
      "sample -  6 5\n",
      "1850\n",
      "1850\n",
      "sample -  2950 533\n",
      "1851\n",
      "sample -  4 1\n",
      "1852\n",
      "sample -  752 618\n",
      "1853\n",
      "sample -  22566 4983\n",
      "1854\n",
      "sample -  4 2\n",
      "1855\n",
      "sample -  2 9\n",
      "1856\n",
      "sample -  1 1\n",
      "1857\n",
      "sample -  378 2224\n",
      "1858\n",
      "sample -  1 1\n",
      "1859\n",
      "sample -  2 7\n",
      "1860\n",
      "sample -  5 1\n",
      "1861\n",
      "sample -  8916 397\n",
      "1862\n",
      "sample -  2 1\n",
      "1863\n",
      "sample -  1108 31\n",
      "1864\n",
      "sample -  5 79\n",
      "1865\n",
      "sample -  1 1\n",
      "1866\n",
      "sample -  1 1\n",
      "1867\n",
      "sample -  56 9\n",
      "1868\n",
      "sample -  2 4\n",
      "1869\n",
      "sample -  1 1\n",
      "1870\n",
      "sample -  55 153\n",
      "1871\n",
      "sample -  1 1\n",
      "1872\n",
      "sample -  25 6\n",
      "1873\n",
      "sample -  24213 18054\n",
      "1874\n",
      "1874\n",
      "sample -  2 1\n",
      "1875\n",
      "sample -  10 5\n",
      "1876\n",
      "sample -  23732 30539\n",
      "1877\n",
      "sample -  1 1\n",
      "1878\n",
      "sample -  50 226\n",
      "1879\n",
      "sample -  282 34\n",
      "1880\n",
      "sample -  1270 55\n",
      "1881\n",
      "sample -  1780 134\n",
      "1882\n",
      "sample -  1 3\n",
      "1883\n",
      "sample -  2550 28\n",
      "1884\n",
      "sample -  9794 496\n",
      "1885\n",
      "sample -  1 1\n",
      "1886\n",
      "sample -  31 104\n",
      "1887\n",
      "sample -  21 38\n",
      "1888\n",
      "sample -  23 3\n",
      "1889\n",
      "sample -  24 3\n",
      "1890\n",
      "sample -  11372 2334\n",
      "1891\n",
      "sample -  965 10\n",
      "1892\n",
      "sample -  13 12\n",
      "1893\n",
      "sample -  24986 5922\n",
      "1894\n",
      "sample -  22513 17028\n",
      "1895\n",
      "sample -  1079 12789\n",
      "1896\n",
      "sample -  31 20\n",
      "1897\n",
      "sample -  9 3\n",
      "1898\n",
      "sample -  13 5\n",
      "1899\n",
      "sample -  26 2\n",
      "1900\n",
      "sample -  5 2\n",
      "1901\n",
      "sample -  1 2\n",
      "1902\n",
      "sample -  6244 726\n",
      "1903\n",
      "sample -  3 2\n",
      "1904\n",
      "sample -  88 3\n",
      "1905\n",
      "sample -  1 2\n",
      "1906\n",
      "sample -  1 1\n",
      "1907\n",
      "sample -  1 1\n",
      "1908\n",
      "sample -  1 1\n",
      "1909\n",
      "1909\n",
      "sample -  1 2\n",
      "1910\n",
      "sample -  1 1\n",
      "1911\n",
      "sample -  1 1\n",
      "1912\n",
      "sample -  1 3\n",
      "1913\n",
      "sample -  62 28\n",
      "1914\n",
      "sample -  651 1725\n",
      "1915\n",
      "sample -  1 1\n",
      "1916\n",
      "1916\n",
      "sample -  1 2\n",
      "1917\n",
      "sample -  1 1\n",
      "1918\n",
      "sample -  1 1\n",
      "1919\n",
      "sample -  95 18\n",
      "1920\n",
      "sample -  91 249\n",
      "1921\n",
      "sample -  20 3\n",
      "1922\n",
      "sample -  55 49\n",
      "1923\n",
      "sample -  1 1\n",
      "1924\n",
      "sample -  76 31\n",
      "1925\n",
      "sample -  27 6\n",
      "1926\n",
      "sample -  1 1\n",
      "1927\n",
      "sample -  1 1\n",
      "1928\n",
      "sample -  38 58\n",
      "1929\n",
      "sample -  1 1\n",
      "1930\n",
      "sample -  4022 5880\n",
      "1931\n",
      "1931\n",
      "sample -  1 1\n",
      "1932\n",
      "sample -  1 1\n",
      "1933\n",
      "sample -  17814 40259\n",
      "1934\n",
      "sample -  1 1\n",
      "1935\n",
      "sample -  29688 4835\n",
      "1936\n",
      "sample -  3357 412\n",
      "1937\n",
      "sample -  27855 3728\n",
      "1938\n",
      "sample -  1 1\n",
      "1939\n",
      "sample -  414 656\n",
      "1940\n",
      "sample -  1 1\n",
      "1941\n",
      "sample -  1 1\n",
      "1942\n",
      "sample -  1 1\n",
      "1943\n",
      "sample -  1 1\n",
      "1944\n",
      "sample -  597 181\n",
      "1945\n",
      "sample -  1 1\n",
      "1946\n",
      "sample -  1 1\n",
      "1947\n",
      "sample -  11 10\n",
      "1948\n",
      "sample -  6 5\n",
      "1949\n",
      "sample -  11 398\n",
      "1950\n",
      "1950\n",
      "sample -  3 1\n",
      "1951\n",
      "sample -  5404 577\n",
      "1952\n",
      "sample -  9852 6539\n",
      "1953\n",
      "sample -  826 2046\n",
      "1954\n",
      "sample -  1 1\n",
      "1955\n",
      "sample -  650 130\n",
      "1956\n",
      "sample -  8 17\n",
      "1957\n",
      "1957\n",
      "sample -  107 11\n",
      "1958\n",
      "sample -  388 11\n",
      "1959\n",
      "sample -  3 2\n",
      "1960\n",
      "sample -  2 1\n",
      "1961\n",
      "sample -  2 3\n",
      "1962\n",
      "sample -  1 1\n",
      "1963\n",
      "sample -  1 1\n",
      "1964\n",
      "sample -  2 1\n",
      "1965\n",
      "sample -  1099 988\n",
      "1966\n",
      "sample -  6 39\n",
      "1967\n",
      "sample -  1 1\n",
      "1968\n",
      "sample -  2 2\n",
      "1969\n",
      "sample -  102 82\n",
      "1970\n",
      "sample -  2 1\n",
      "1971\n",
      "sample -  65 15\n",
      "1972\n",
      "sample -  1 1\n",
      "1973\n",
      "sample -  26 8\n",
      "1974\n",
      "sample -  37 4\n",
      "1975\n",
      "sample -  1793 221\n",
      "1976\n",
      "1976\n",
      "sample -  8075 607\n",
      "1977\n",
      "sample -  1 1\n",
      "1978\n",
      "sample -  1296 65\n",
      "1979\n",
      "sample -  1 1\n",
      "1980\n",
      "sample -  1 1\n",
      "1981\n",
      "1981\n",
      "sample -  48 11\n",
      "1982\n",
      "sample -  3024 441\n",
      "1983\n",
      "sample -  13291 1974\n",
      "1984\n",
      "sample -  116 22\n",
      "1985\n",
      "1985\n",
      "sample -  9732 4015\n",
      "1986\n",
      "sample -  41 148\n",
      "1987\n",
      "sample -  8196 1299\n",
      "1988\n",
      "sample -  1 1\n",
      "1989\n",
      "sample -  3639 3622\n",
      "1990\n",
      "sample -  702 111\n",
      "1991\n",
      "sample -  2 2\n",
      "1992\n",
      "sample -  1469 95\n",
      "1993\n",
      "sample -  2 1\n",
      "1994\n",
      "sample -  237 436\n",
      "1995\n",
      "sample -  2 1\n",
      "1996\n",
      "sample -  59 5\n",
      "1997\n",
      "sample -  1 1\n",
      "1998\n",
      "sample -  16 9\n",
      "1999\n",
      "sample -  1 1\n",
      "2000\n",
      "sample -  3 7\n",
      "2001\n",
      "sample -  59 42\n",
      "2002\n",
      "sample -  6 12\n",
      "2003\n",
      "sample -  1 2\n",
      "2004\n",
      "sample -  17397 11018\n",
      "2005\n",
      "sample -  9 587\n",
      "2006\n",
      "sample -  34632 25098\n",
      "2007\n",
      "sample -  1 1\n",
      "2008\n",
      "2008\n",
      "sample -  3 1\n",
      "2009\n",
      "sample -  194 38\n",
      "2010\n",
      "sample -  155 20\n",
      "2011\n",
      "sample -  30161 18905\n",
      "2012\n",
      "sample -  1 1\n",
      "2013\n",
      "sample -  12 12\n",
      "2014\n",
      "sample -  19264 3848\n",
      "2015\n",
      "sample -  3 38\n",
      "2016\n",
      "sample -  10148 1604\n",
      "2017\n",
      "sample -  1 1\n",
      "2018\n",
      "sample -  4103 1904\n",
      "2019\n",
      "sample -  1 1\n",
      "2020\n",
      "sample -  1 2\n",
      "2021\n",
      "sample -  1 1\n",
      "2022\n",
      "sample -  10 2\n",
      "2023\n",
      "sample -  6 107\n",
      "2024\n",
      "sample -  8 4\n",
      "2025\n",
      "sample -  1 1\n",
      "2026\n",
      "sample -  1971 336\n",
      "2027\n",
      "sample -  359 25\n",
      "2028\n",
      "sample -  12 2\n",
      "2029\n",
      "sample -  1246 132\n",
      "2030\n",
      "sample -  106 4\n",
      "2031\n",
      "2031\n",
      "sample -  22 41\n",
      "2032\n",
      "sample -  23523 5240\n",
      "2033\n",
      "sample -  1 1\n",
      "2034\n",
      "sample -  26 5\n",
      "2035\n",
      "sample -  5 3\n",
      "2036\n",
      "sample -  1 1\n",
      "2037\n",
      "sample -  82 629\n",
      "2038\n",
      "sample -  61 8\n",
      "2039\n",
      "sample -  1 1\n",
      "2040\n",
      "sample -  5868 6663\n",
      "2041\n",
      "sample -  1 2\n",
      "2042\n",
      "sample -  1 1\n",
      "2043\n",
      "sample -  198 1312\n",
      "2044\n",
      "sample -  5 11\n",
      "2045\n",
      "sample -  1 1\n",
      "2046\n",
      "sample -  1 1\n",
      "2047\n",
      "2047\n",
      "sample -  13777 1171\n",
      "2048\n",
      "sample -  27890 31442\n",
      "2049\n",
      "sample -  1 1\n",
      "2050\n",
      "sample -  1 1\n",
      "2051\n",
      "sample -  16853 9061\n",
      "2052\n",
      "sample -  8006 132\n",
      "2053\n",
      "sample -  65 28\n",
      "2054\n",
      "sample -  2968 7581\n",
      "2055\n",
      "sample -  1 2\n",
      "2056\n",
      "sample -  96 1014\n",
      "2057\n",
      "sample -  1 2\n",
      "2058\n",
      "sample -  3 2\n",
      "2059\n",
      "sample -  1 1\n",
      "2060\n",
      "sample -  5 2\n",
      "2061\n",
      "sample -  1 1\n",
      "2062\n",
      "sample -  1 1\n",
      "2063\n",
      "sample -  2 1\n",
      "2064\n",
      "sample -  4118 992\n",
      "2065\n",
      "sample -  279 147\n",
      "2066\n",
      "sample -  15970 1379\n",
      "2067\n",
      "sample -  259 901\n",
      "2068\n",
      "sample -  19224 639\n",
      "2069\n",
      "sample -  7985 2717\n",
      "2070\n",
      "sample -  30194 8668\n",
      "2071\n",
      "sample -  775 74\n",
      "2072\n",
      "2072\n",
      "sample -  15 1406\n",
      "2073\n",
      "sample -  482 243\n",
      "2074\n",
      "sample -  5 25\n",
      "2075\n",
      "sample -  1 1\n",
      "2076\n",
      "2076\n",
      "sample -  4 5\n",
      "2077\n",
      "sample -  1 3\n",
      "2078\n",
      "sample -  1924 94\n",
      "2079\n",
      "sample -  3 1\n",
      "2080\n",
      "sample -  207 111\n",
      "2081\n",
      "sample -  4 1\n",
      "2082\n",
      "sample -  4 1\n",
      "2083\n",
      "sample -  4262 710\n",
      "2084\n",
      "sample -  1252 6218\n",
      "2085\n",
      "sample -  1 1\n",
      "2086\n",
      "sample -  14834 5714\n",
      "2087\n",
      "sample -  3 1\n",
      "2088\n",
      "sample -  2 1\n",
      "2089\n",
      "sample -  13210 22124\n",
      "2090\n",
      "sample -  7456 7806\n",
      "2091\n",
      "2091\n",
      "sample -  3782 676\n",
      "2092\n",
      "sample -  2 5\n",
      "2093\n",
      "sample -  1 1\n",
      "2094\n",
      "sample -  5 14\n",
      "2095\n",
      "sample -  90 4\n",
      "2096\n",
      "sample -  4655 5361\n",
      "2097\n",
      "sample -  1 1\n",
      "2098\n",
      "sample -  1 1\n",
      "2099\n",
      "sample -  4 45\n",
      "2100\n",
      "sample -  2 1\n",
      "2101\n",
      "sample -  18352 27889\n",
      "2102\n",
      "sample -  21027 5509\n",
      "2103\n",
      "sample -  1982 955\n",
      "2104\n",
      "sample -  1 3\n",
      "2105\n",
      "sample -  1 1\n",
      "2106\n",
      "sample -  2462 332\n",
      "2107\n",
      "sample -  1078 133\n",
      "2108\n",
      "sample -  12751 3363\n",
      "2109\n",
      "sample -  12 2\n",
      "2110\n",
      "sample -  1 1\n",
      "2111\n",
      "sample -  5179 3335\n",
      "2112\n",
      "2112\n",
      "sample -  40095 34628\n",
      "2113\n",
      "sample -  150 8\n",
      "2114\n",
      "sample -  4 1\n",
      "2115\n",
      "sample -  1 1\n",
      "2116\n",
      "sample -  1 1\n",
      "2117\n",
      "sample -  10 1\n",
      "2118\n",
      "sample -  8 45\n",
      "2119\n",
      "sample -  147 22\n",
      "2120\n",
      "sample -  12514 594\n",
      "2121\n",
      "sample -  16 1\n",
      "2122\n",
      "sample -  1569 168\n",
      "2123\n",
      "sample -  1 2\n",
      "2124\n",
      "sample -  1 1\n",
      "2125\n",
      "sample -  101 341\n",
      "2126\n",
      "sample -  3 3\n",
      "2127\n",
      "sample -  4701 378\n",
      "2128\n",
      "sample -  2 1\n",
      "2129\n",
      "sample -  2 2\n",
      "2130\n",
      "sample -  1 2\n",
      "2131\n",
      "sample -  1 1\n",
      "2132\n",
      "sample -  1249 13864\n",
      "2133\n",
      "sample -  1 1\n",
      "2134\n",
      "sample -  65 1\n",
      "2135\n",
      "sample -  12 3\n",
      "2136\n",
      "sample -  8 72\n",
      "2137\n",
      "sample -  108 39\n",
      "2138\n",
      "sample -  799 33\n",
      "2139\n",
      "sample -  43 11\n",
      "2140\n",
      "sample -  1 1\n",
      "2141\n",
      "sample -  1 2\n",
      "2142\n",
      "sample -  1 2\n",
      "2143\n",
      "sample -  1 1\n",
      "2144\n",
      "sample -  1 1\n",
      "2145\n",
      "sample -  419 3779\n",
      "2146\n",
      "sample -  7844 259\n",
      "2147\n",
      "sample -  1 2\n",
      "2148\n",
      "sample -  2 3\n",
      "2149\n",
      "sample -  4405 12014\n",
      "2150\n",
      "sample -  782 103\n",
      "2151\n",
      "sample -  2 1\n",
      "2152\n",
      "sample -  1 1\n",
      "2153\n",
      "sample -  1 1\n",
      "2154\n",
      "sample -  11 41\n",
      "2155\n",
      "sample -  23811 8029\n",
      "2156\n",
      "sample -  4 3\n",
      "2157\n",
      "sample -  1 1\n",
      "2158\n",
      "sample -  1 1\n",
      "2159\n",
      "sample -  5 2\n",
      "2160\n",
      "sample -  1 1\n",
      "2161\n",
      "sample -  40 10\n",
      "2162\n",
      "sample -  3 2\n",
      "2163\n",
      "sample -  1380 1074\n",
      "2164\n",
      "sample -  1 1\n",
      "2165\n",
      "sample -  2 34\n",
      "2166\n",
      "sample -  3125 3289\n",
      "2167\n",
      "sample -  1 1\n",
      "2168\n",
      "sample -  2 1\n",
      "2169\n",
      "sample -  14 9\n",
      "2170\n",
      "sample -  14 26\n",
      "2171\n",
      "sample -  85 86\n",
      "2172\n",
      "sample -  12815 3077\n",
      "2173\n",
      "sample -  1 1\n",
      "2174\n",
      "sample -  1 1\n",
      "2175\n",
      "sample -  970 62\n",
      "2176\n",
      "sample -  20902 11404\n",
      "2177\n",
      "sample -  1 1\n",
      "2178\n",
      "2178\n",
      "sample -  7848 1476\n",
      "2179\n",
      "sample -  71 174\n",
      "2180\n",
      "sample -  2 2\n",
      "2181\n",
      "sample -  1 1\n",
      "2182\n",
      "sample -  1 2\n",
      "2183\n",
      "sample -  16 9\n",
      "2184\n",
      "sample -  1 1\n",
      "2185\n",
      "sample -  30 48\n",
      "2186\n",
      "sample -  1 2\n",
      "2187\n",
      "sample -  37396 16737\n",
      "2188\n",
      "sample -  372 21490\n",
      "2189\n",
      "sample -  2 1\n",
      "2190\n",
      "2190\n",
      "sample -  2 9\n",
      "2191\n",
      "sample -  7 2\n",
      "2192\n",
      "sample -  1 1\n",
      "2193\n",
      "2193\n",
      "sample -  4 22\n",
      "2194\n",
      "sample -  43 85\n",
      "2195\n",
      "sample -  1 1\n",
      "2196\n",
      "sample -  28269 36226\n",
      "2197\n",
      "sample -  1 1\n",
      "2198\n",
      "sample -  4 2\n",
      "2199\n",
      "sample -  1 1\n",
      "2200\n",
      "sample -  8 4\n",
      "2201\n",
      "sample -  1986 1463\n",
      "2202\n",
      "sample -  3 2\n",
      "2203\n",
      "sample -  36 10\n",
      "2204\n",
      "sample -  142 165\n",
      "2205\n",
      "sample -  26 4\n",
      "2206\n",
      "sample -  1 1\n",
      "2207\n",
      "sample -  1 1\n",
      "2208\n",
      "sample -  1 1\n",
      "2209\n",
      "sample -  1 1\n",
      "2210\n",
      "sample -  9 3\n",
      "2211\n",
      "sample -  20 22\n",
      "2212\n",
      "sample -  102 16\n",
      "2213\n",
      "sample -  1 1\n",
      "2214\n",
      "sample -  96 69\n",
      "2215\n",
      "sample -  3 28\n",
      "2216\n",
      "sample -  401 52\n",
      "2217\n",
      "sample -  2 1\n",
      "2218\n",
      "sample -  1 1\n",
      "2219\n",
      "sample -  30 7\n",
      "2220\n",
      "sample -  2 2\n",
      "2221\n",
      "sample -  61 7\n",
      "2222\n",
      "sample -  948 701\n",
      "2223\n",
      "sample -  1 1\n",
      "2224\n",
      "sample -  16442 4600\n",
      "2225\n",
      "sample -  1 1\n",
      "2226\n",
      "sample -  368 105\n",
      "2227\n",
      "sample -  12 6\n",
      "2228\n",
      "sample -  12721 1617\n",
      "2229\n",
      "sample -  2 2\n",
      "2230\n",
      "sample -  3473 17143\n",
      "2231\n",
      "sample -  14306 12918\n",
      "2232\n",
      "sample -  52 9\n",
      "2233\n",
      "sample -  1 1\n",
      "2234\n",
      "sample -  1122 19\n",
      "2235\n",
      "sample -  10 3\n",
      "2236\n",
      "2236\n",
      "sample -  1 1\n",
      "2237\n",
      "sample -  1 1\n",
      "2238\n",
      "sample -  9 119\n",
      "2239\n",
      "sample -  1708 263\n",
      "2240\n",
      "sample -  1 1\n",
      "2241\n",
      "sample -  2100 527\n",
      "2242\n",
      "sample -  1 6\n",
      "2243\n",
      "2243\n",
      "sample -  1 1\n",
      "2244\n",
      "sample -  135 254\n",
      "2245\n",
      "sample -  1 2\n",
      "2246\n",
      "sample -  3 2\n",
      "2247\n",
      "sample -  1 1\n",
      "2248\n",
      "sample -  7 47\n",
      "2249\n",
      "sample -  24 19\n",
      "2250\n",
      "sample -  6418 777\n",
      "2251\n",
      "sample -  23033 5540\n",
      "2252\n",
      "sample -  1054 3366\n",
      "2253\n",
      "sample -  1 2\n",
      "2254\n",
      "sample -  26 110\n",
      "2255\n",
      "sample -  19425 7185\n",
      "2256\n",
      "sample -  1 2\n",
      "2257\n",
      "sample -  468 19\n",
      "2258\n",
      "sample -  12582 5982\n",
      "2259\n",
      "sample -  1 1\n",
      "2260\n",
      "sample -  216 34\n",
      "2261\n",
      "sample -  112 8\n",
      "2262\n",
      "2262\n",
      "sample -  59 689\n",
      "2263\n",
      "sample -  87 36\n",
      "2264\n",
      "sample -  1 1\n",
      "2265\n",
      "sample -  12939 462\n",
      "2266\n",
      "sample -  1 2\n",
      "2267\n",
      "sample -  14 4\n",
      "2268\n",
      "sample -  16 2\n",
      "2269\n",
      "sample -  1 2\n",
      "2270\n",
      "sample -  6654 766\n",
      "2271\n",
      "sample -  2435 748\n",
      "2272\n",
      "sample -  12 17\n",
      "2273\n",
      "sample -  1 1\n",
      "2274\n",
      "sample -  1 1\n",
      "2275\n",
      "2275\n",
      "sample -  1 1\n",
      "2276\n",
      "sample -  87 46\n",
      "2277\n",
      "sample -  1 1\n",
      "2278\n",
      "sample -  1 1\n",
      "2279\n",
      "sample -  6 21\n",
      "2280\n",
      "sample -  19 2\n",
      "2281\n",
      "sample -  1 1\n",
      "2282\n",
      "sample -  31 3\n",
      "2283\n",
      "sample -  8 12\n",
      "2284\n",
      "sample -  4927 300\n",
      "2285\n",
      "sample -  1 1\n",
      "2286\n",
      "sample -  29427 31332\n",
      "2287\n",
      "sample -  1 1\n",
      "2288\n",
      "sample -  209 29\n",
      "2289\n",
      "sample -  1 2\n",
      "2290\n",
      "sample -  3 1\n",
      "2291\n",
      "sample -  2 1\n",
      "2292\n",
      "sample -  146 41\n",
      "2293\n",
      "sample -  50 14\n",
      "2294\n",
      "sample -  1 1\n",
      "2295\n",
      "sample -  3 1\n",
      "2296\n",
      "sample -  1 1\n",
      "2297\n",
      "sample -  18467 16914\n",
      "2298\n",
      "sample -  184 9\n",
      "2299\n",
      "sample -  1 1\n",
      "2300\n",
      "sample -  1 1\n",
      "2301\n",
      "sample -  2 1\n",
      "2302\n",
      "sample -  1 2\n",
      "2303\n",
      "sample -  102 28\n",
      "2304\n",
      "sample -  1 25\n",
      "2305\n",
      "sample -  1 1\n",
      "2306\n",
      "sample -  256 5\n",
      "2307\n",
      "sample -  2 1\n",
      "2308\n",
      "sample -  1 2\n",
      "2309\n",
      "sample -  1 1\n",
      "2310\n",
      "2310\n",
      "sample -  1 2\n",
      "2311\n",
      "2311\n",
      "sample -  2 1\n",
      "2312\n",
      "sample -  9313 15871\n",
      "2313\n",
      "sample -  1 2\n",
      "2314\n",
      "sample -  33 31\n",
      "2315\n",
      "sample -  230 20\n",
      "2316\n",
      "sample -  2 2\n",
      "2317\n",
      "sample -  2 3\n",
      "2318\n",
      "sample -  1864 835\n",
      "2319\n",
      "sample -  6 4\n",
      "2320\n",
      "sample -  1 1\n",
      "2321\n",
      "sample -  1 1\n",
      "2322\n",
      "sample -  1 1\n",
      "2323\n",
      "2323\n",
      "sample -  1 1\n",
      "2324\n",
      "sample -  1 1\n",
      "2325\n",
      "sample -  2 1\n",
      "2326\n",
      "sample -  1 1\n",
      "2327\n",
      "sample -  1 1\n",
      "2328\n",
      "sample -  1 1\n",
      "2329\n",
      "sample -  12372 653\n",
      "2330\n",
      "sample -  5535 7618\n",
      "2331\n",
      "2331\n",
      "sample -  1 1\n",
      "2332\n",
      "sample -  15443 1607\n",
      "2333\n",
      "sample -  23678 5427\n",
      "2334\n",
      "sample -  1 1\n",
      "2335\n",
      "sample -  1 1\n",
      "2336\n",
      "sample -  24198 7772\n",
      "2337\n",
      "sample -  8118 5038\n",
      "2338\n",
      "sample -  12 2\n",
      "2339\n",
      "sample -  126 17494\n",
      "2340\n",
      "sample -  2 2\n",
      "2341\n",
      "sample -  5 2\n",
      "2342\n",
      "sample -  2 1\n",
      "2343\n",
      "sample -  5987 3453\n",
      "2344\n",
      "sample -  1 1\n",
      "2345\n",
      "sample -  1 1\n",
      "2346\n",
      "sample -  14666 22406\n",
      "2347\n",
      "sample -  1 1\n",
      "2348\n",
      "sample -  1 1\n",
      "2349\n",
      "sample -  28584 17234\n",
      "2350\n",
      "sample -  1 1\n",
      "2351\n",
      "sample -  737 4122\n",
      "2352\n",
      "sample -  1 1\n",
      "2353\n",
      "sample -  2805 940\n",
      "2354\n",
      "sample -  1 1\n",
      "2355\n",
      "sample -  1 1\n",
      "2356\n",
      "sample -  2 2\n",
      "2357\n",
      "sample -  1 1\n",
      "2358\n",
      "2358\n",
      "sample -  1 1\n",
      "2359\n",
      "sample -  3217 850\n",
      "2360\n",
      "sample -  1 1\n",
      "2361\n",
      "sample -  25 67\n",
      "2362\n",
      "sample -  15427 3065\n",
      "2363\n",
      "sample -  1 2\n",
      "2364\n",
      "sample -  3 1\n",
      "2365\n",
      "sample -  2837 280\n",
      "2366\n",
      "sample -  1 1\n",
      "2367\n",
      "sample -  11 2\n",
      "2368\n",
      "sample -  37 27\n",
      "2369\n",
      "sample -  6 11\n",
      "2370\n",
      "sample -  1 1\n",
      "2371\n",
      "sample -  47 3\n",
      "2372\n",
      "sample -  4521 406\n",
      "2373\n",
      "sample -  2 3\n",
      "2374\n",
      "sample -  22 8\n",
      "2375\n",
      "2375\n",
      "sample -  20997 20353\n",
      "2376\n",
      "sample -  1 1\n",
      "2377\n",
      "sample -  57 13\n",
      "2378\n",
      "sample -  1435 213\n",
      "2379\n",
      "sample -  11207 10464\n",
      "2380\n",
      "2380\n",
      "sample -  24032 2630\n",
      "2381\n",
      "sample -  60 454\n",
      "2382\n",
      "sample -  2 1\n",
      "2383\n",
      "sample -  1 1\n",
      "2384\n",
      "sample -  15 4\n",
      "2385\n",
      "sample -  3 2\n",
      "2386\n",
      "sample -  1806 1049\n",
      "2387\n",
      "2387\n",
      "sample -  1 5\n",
      "2388\n",
      "sample -  5 72\n",
      "2389\n",
      "sample -  14 3\n",
      "2390\n",
      "sample -  36 13\n",
      "2391\n",
      "sample -  1212 2144\n",
      "2392\n",
      "sample -  9 2\n",
      "2393\n",
      "sample -  1 1\n",
      "2394\n",
      "2394\n",
      "sample -  3 1\n",
      "2395\n",
      "sample -  1267 2464\n",
      "2396\n",
      "sample -  12 27\n",
      "2397\n",
      "2397\n",
      "sample -  7 6\n",
      "2398\n",
      "sample -  1196 6386\n",
      "2399\n",
      "sample -  6 20\n",
      "2400\n",
      "sample -  2 2\n",
      "2401\n",
      "sample -  1 1\n",
      "2402\n",
      "sample -  2 2\n",
      "2403\n",
      "sample -  1342 466\n",
      "2404\n",
      "sample -  128 157\n",
      "2405\n",
      "sample -  1 1\n",
      "2406\n",
      "sample -  26297 21542\n",
      "2407\n",
      "2407\n",
      "sample -  1 7\n",
      "2408\n",
      "2408\n",
      "sample -  231 395\n",
      "2409\n",
      "sample -  1 1\n",
      "2410\n",
      "sample -  5914 702\n",
      "2411\n",
      "sample -  61 11\n",
      "2412\n",
      "sample -  5 6\n",
      "2413\n",
      "sample -  2360 741\n",
      "2414\n",
      "sample -  1 1\n",
      "2415\n",
      "sample -  35664 19603\n",
      "2416\n",
      "2416\n",
      "sample -  6802 2036\n",
      "2417\n",
      "sample -  10990 1894\n",
      "2418\n",
      "sample -  11 7\n",
      "2419\n",
      "sample -  4848 10019\n",
      "2420\n",
      "sample -  1 1\n",
      "2421\n",
      "sample -  1 1\n",
      "2422\n",
      "sample -  1 1\n",
      "2423\n",
      "2423\n",
      "sample -  7485 7371\n",
      "2424\n",
      "sample -  1 1\n",
      "2425\n",
      "sample -  4 2\n",
      "2426\n",
      "sample -  25 6\n",
      "2427\n",
      "2427\n",
      "sample -  1 1\n",
      "2428\n",
      "sample -  1 1\n",
      "2429\n",
      "sample -  1 1\n",
      "2430\n",
      "2430\n",
      "sample -  10 2\n",
      "2431\n",
      "sample -  25 2\n",
      "2432\n",
      "sample -  2 2\n",
      "2433\n",
      "sample -  1 2\n",
      "2434\n",
      "sample -  247 235\n",
      "2435\n",
      "sample -  4 9\n",
      "2436\n",
      "sample -  1 1\n",
      "2437\n",
      "sample -  5 1\n",
      "2438\n",
      "sample -  1 1\n",
      "2439\n",
      "sample -  3496 1778\n",
      "2440\n",
      "sample -  15 2\n",
      "2441\n",
      "sample -  1962 1012\n",
      "2442\n",
      "sample -  16324 2859\n",
      "2443\n",
      "2443\n",
      "sample -  2 1\n",
      "2444\n",
      "sample -  2424 518\n",
      "2445\n",
      "sample -  1 1\n",
      "2446\n",
      "sample -  1 1\n",
      "2447\n",
      "sample -  2 1\n",
      "2448\n",
      "sample -  4505 7339\n",
      "2449\n",
      "sample -  1 1\n",
      "2450\n",
      "sample -  1 1\n",
      "2451\n",
      "sample -  2 2\n",
      "2452\n",
      "sample -  1 1\n",
      "2453\n",
      "sample -  848 329\n",
      "2454\n",
      "sample -  16 148\n",
      "2455\n",
      "sample -  27224 6260\n",
      "2456\n",
      "sample -  4514 11827\n",
      "2457\n",
      "2457\n",
      "sample -  32 20\n",
      "2458\n",
      "sample -  1 1\n",
      "2459\n",
      "sample -  5712 191\n",
      "2460\n",
      "sample -  1 1\n",
      "2461\n",
      "sample -  30911 8121\n",
      "2462\n",
      "sample -  915 278\n",
      "2463\n",
      "sample -  1 1\n",
      "2464\n",
      "sample -  4 27\n",
      "2465\n",
      "sample -  17585 11533\n",
      "2466\n",
      "sample -  6 1\n",
      "2467\n",
      "sample -  3 1\n",
      "2468\n",
      "sample -  1 1\n",
      "2469\n",
      "sample -  86 13\n",
      "2470\n",
      "sample -  409 130\n",
      "2471\n",
      "sample -  6 5\n",
      "2472\n",
      "sample -  5 2\n",
      "2473\n",
      "sample -  1 1\n",
      "2474\n",
      "sample -  7 8\n",
      "2475\n",
      "2475\n",
      "sample -  1 1\n",
      "2476\n",
      "sample -  114 282\n",
      "2477\n",
      "sample -  4 3\n",
      "2478\n",
      "sample -  1 1\n",
      "2479\n",
      "sample -  1 1\n",
      "2480\n",
      "sample -  1 1\n",
      "2481\n",
      "sample -  1 1\n",
      "2482\n",
      "sample -  63 9\n",
      "2483\n",
      "sample -  1 1\n",
      "2484\n",
      "sample -  1 1\n",
      "2485\n",
      "sample -  2 1\n",
      "2486\n",
      "sample -  72 21\n",
      "2487\n",
      "sample -  25268 5803\n",
      "2488\n",
      "sample -  26 11\n",
      "2489\n",
      "2489\n",
      "sample -  15855 1736\n",
      "2490\n",
      "sample -  2 1\n",
      "2491\n",
      "sample -  1 2\n",
      "2492\n",
      "sample -  1 1\n",
      "2493\n",
      "sample -  2 2\n",
      "2494\n",
      "2494\n",
      "sample -  1 1\n",
      "2495\n",
      "2495\n",
      "sample -  1 1\n",
      "2496\n",
      "sample -  93 35\n",
      "2497\n",
      "sample -  1 1\n",
      "2498\n",
      "sample -  2 1\n",
      "2499\n",
      "sample -  4 1\n",
      "2500\n",
      "sample -  20 19\n",
      "2501\n",
      "sample -  1 1\n",
      "2502\n",
      "sample -  5240 571\n",
      "2503\n",
      "sample -  18805 14406\n",
      "2504\n",
      "sample -  1 1\n",
      "2505\n",
      "sample -  1 1\n",
      "2506\n",
      "sample -  32031 21689\n",
      "2507\n",
      "sample -  23251 2237\n",
      "2508\n",
      "sample -  71 24\n",
      "2509\n",
      "sample -  1 1\n",
      "2510\n",
      "sample -  12 8\n",
      "2511\n",
      "sample -  1 1\n",
      "2512\n",
      "sample -  1 1\n",
      "2513\n",
      "sample -  8 332\n",
      "2514\n",
      "sample -  9 101\n",
      "2515\n",
      "sample -  3 5\n",
      "2516\n",
      "sample -  2 3\n",
      "2517\n",
      "sample -  1 1\n",
      "2518\n",
      "2518\n",
      "2518\n",
      "sample -  805 81\n",
      "2519\n",
      "sample -  1 1\n",
      "2520\n",
      "sample -  2 1\n",
      "2521\n",
      "sample -  1 1\n",
      "2522\n",
      "sample -  1 1\n",
      "2523\n",
      "sample -  40752 38996\n",
      "2524\n",
      "sample -  15796 263\n",
      "2525\n",
      "sample -  951 457\n",
      "2526\n",
      "sample -  1 1\n",
      "2527\n",
      "sample -  1 1\n",
      "2528\n",
      "sample -  1 2\n",
      "2529\n",
      "2529\n",
      "sample -  1 1\n",
      "2530\n",
      "sample -  1 2\n",
      "2531\n",
      "sample -  5 3\n",
      "2532\n",
      "sample -  1 1\n",
      "2533\n",
      "sample -  300 103\n",
      "2534\n",
      "2534\n",
      "sample -  4 1\n",
      "2535\n",
      "sample -  4441 305\n",
      "2536\n",
      "sample -  42 7\n",
      "2537\n",
      "sample -  64 10\n",
      "2538\n",
      "sample -  63 5\n",
      "2539\n",
      "sample -  2 5\n",
      "2540\n",
      "sample -  1 1\n",
      "2541\n",
      "sample -  1 1\n",
      "2542\n",
      "sample -  5 3\n",
      "2543\n",
      "sample -  1 1\n",
      "2544\n",
      "sample -  1 1\n",
      "2545\n",
      "sample -  1 1\n",
      "2546\n",
      "sample -  56 84\n",
      "2547\n",
      "sample -  1 2\n",
      "2548\n",
      "sample -  2 1\n",
      "2549\n",
      "sample -  1 1\n",
      "2550\n",
      "sample -  1306 117\n",
      "2551\n",
      "sample -  391 144\n",
      "2552\n",
      "sample -  1 1\n",
      "2553\n",
      "sample -  5 6\n",
      "2554\n",
      "sample -  3 4\n",
      "2555\n",
      "sample -  1372 94\n",
      "2556\n",
      "sample -  1 1\n",
      "2557\n",
      "sample -  1 1\n",
      "2558\n",
      "sample -  1 2\n",
      "2559\n",
      "sample -  10270 7456\n",
      "2560\n",
      "sample -  27 3\n",
      "2561\n",
      "sample -  99 50\n",
      "2562\n",
      "sample -  17962 15744\n",
      "2563\n",
      "sample -  1 1\n",
      "2564\n",
      "sample -  7 60\n",
      "2565\n",
      "sample -  1 1\n",
      "2566\n",
      "sample -  6631 1831\n",
      "2567\n",
      "sample -  6 3\n",
      "2568\n",
      "sample -  1 2\n",
      "2569\n",
      "sample -  6770 620\n",
      "2570\n",
      "sample -  2 1\n",
      "2571\n",
      "sample -  1 1\n",
      "2572\n",
      "sample -  87 764\n",
      "2573\n",
      "sample -  1 1\n",
      "2574\n",
      "sample -  5075 2933\n",
      "2575\n",
      "sample -  11 2\n",
      "2576\n",
      "sample -  1 3\n",
      "2577\n",
      "sample -  445 513\n",
      "2578\n",
      "sample -  1 1\n",
      "2579\n",
      "sample -  1 1\n",
      "2580\n",
      "sample -  42 133\n",
      "2581\n",
      "sample -  689 668\n",
      "2582\n",
      "sample -  6 3\n",
      "2583\n",
      "sample -  134 15\n",
      "2584\n",
      "sample -  2 1\n",
      "2585\n",
      "sample -  2 5\n",
      "2586\n",
      "sample -  4585 3941\n",
      "2587\n",
      "sample -  902 287\n",
      "2588\n",
      "sample -  1 1\n",
      "2589\n",
      "sample -  9951 2187\n",
      "2590\n",
      "sample -  6 3\n",
      "2591\n",
      "sample -  2416 256\n",
      "2592\n",
      "sample -  393 33\n",
      "2593\n",
      "sample -  71 383\n",
      "2594\n",
      "sample -  14830 20496\n",
      "2595\n",
      "sample -  49 71\n",
      "2596\n",
      "sample -  28313 37847\n",
      "2597\n",
      "sample -  3 1\n",
      "2598\n",
      "sample -  9 10\n",
      "2599\n",
      "sample -  1 1\n",
      "2600\n",
      "2600\n",
      "sample -  1 1\n",
      "2601\n",
      "sample -  1 1\n",
      "2602\n",
      "sample -  1 1\n",
      "2603\n",
      "sample -  1 1\n",
      "2604\n",
      "sample -  4 1\n",
      "2605\n",
      "sample -  12 3\n",
      "2606\n",
      "sample -  40649 40706\n",
      "2607\n",
      "sample -  2 1\n",
      "2608\n",
      "sample -  4 5\n",
      "2609\n",
      "sample -  29770 31793\n",
      "2610\n",
      "sample -  2 2\n",
      "2611\n",
      "2611\n",
      "sample -  1878 197\n",
      "2612\n",
      "sample -  1 1\n",
      "2613\n",
      "sample -  3 1\n",
      "2614\n",
      "sample -  7 47\n",
      "2615\n",
      "sample -  9882 13563\n",
      "2616\n",
      "sample -  13 24\n",
      "2617\n",
      "sample -  17455 27187\n",
      "2618\n",
      "sample -  24514 19588\n",
      "2619\n",
      "sample -  22211 6553\n",
      "2620\n",
      "sample -  26077 31499\n",
      "2621\n",
      "sample -  67 9\n",
      "2622\n",
      "sample -  1 1\n",
      "2623\n",
      "sample -  1 1\n",
      "2624\n",
      "sample -  29703 11678\n",
      "2625\n",
      "sample -  1 1\n",
      "2626\n",
      "sample -  21 4\n",
      "2627\n",
      "sample -  1 1\n",
      "2628\n",
      "sample -  1 1\n",
      "2629\n",
      "sample -  1 3\n",
      "2630\n",
      "sample -  31167 6354\n",
      "2631\n",
      "sample -  1 1\n",
      "2632\n",
      "2632\n",
      "sample -  2 2\n",
      "2633\n",
      "sample -  10429 1095\n",
      "2634\n",
      "sample -  16 171\n",
      "2635\n",
      "sample -  4 7\n",
      "2636\n",
      "sample -  767 21194\n",
      "2637\n",
      "sample -  1 1\n",
      "2638\n",
      "sample -  31391 26607\n",
      "2639\n",
      "sample -  12005 4046\n",
      "2640\n",
      "sample -  33 43\n",
      "2641\n",
      "sample -  3659 2733\n",
      "2642\n",
      "sample -  1 1\n",
      "2643\n",
      "sample -  1 2\n",
      "2644\n",
      "sample -  1 2\n",
      "2645\n",
      "sample -  128 49\n",
      "2646\n",
      "sample -  38 2\n",
      "2647\n",
      "sample -  2305 55\n",
      "2648\n",
      "sample -  9484 17256\n",
      "2649\n",
      "sample -  1 1\n",
      "2650\n",
      "sample -  2648 484\n",
      "2651\n",
      "sample -  1 1\n",
      "2652\n",
      "sample -  8457 3165\n",
      "2653\n",
      "2653\n",
      "sample -  1 1\n",
      "2654\n",
      "sample -  1 1\n",
      "2655\n",
      "sample -  706 82\n",
      "2656\n",
      "sample -  1 1\n",
      "2657\n",
      "sample -  1 1\n",
      "2658\n",
      "sample -  119 29\n",
      "2659\n",
      "sample -  486 142\n",
      "2660\n",
      "sample -  756 389\n",
      "2661\n",
      "sample -  18693 8502\n",
      "2662\n",
      "sample -  2 1\n",
      "2663\n",
      "sample -  1 1\n",
      "2664\n",
      "sample -  1 1\n",
      "2665\n",
      "sample -  27870 16711\n",
      "2666\n",
      "sample -  1 1\n",
      "2667\n",
      "sample -  55 13\n",
      "2668\n",
      "sample -  1298 91\n",
      "2669\n",
      "sample -  241 50\n",
      "2670\n",
      "sample -  1207 272\n",
      "2671\n",
      "sample -  9 3\n",
      "2672\n",
      "sample -  82 15\n",
      "2673\n",
      "sample -  1 1\n",
      "2674\n",
      "sample -  1 1\n",
      "2675\n",
      "sample -  1 1\n",
      "2676\n",
      "sample -  1 1\n",
      "2677\n",
      "sample -  18 285\n",
      "2678\n",
      "sample -  1 1\n",
      "2679\n",
      "sample -  688 10\n",
      "2680\n",
      "sample -  2 3\n",
      "2681\n",
      "sample -  1 1\n",
      "2682\n",
      "sample -  1 1\n",
      "2683\n",
      "sample -  2 1\n",
      "2684\n",
      "sample -  1 1\n",
      "2685\n",
      "sample -  1 1\n",
      "2686\n",
      "sample -  58 844\n",
      "2687\n",
      "sample -  5541 1700\n",
      "2688\n",
      "sample -  2 1\n",
      "2689\n",
      "sample -  1 1\n",
      "2690\n",
      "sample -  2 1\n",
      "2691\n",
      "sample -  589 930\n",
      "2692\n",
      "sample -  1 1\n",
      "2693\n",
      "sample -  285 34\n",
      "2694\n",
      "sample -  6 1\n",
      "2695\n",
      "sample -  6 3\n",
      "2696\n",
      "sample -  2 1\n",
      "2697\n",
      "sample -  284 72\n",
      "2698\n",
      "sample -  1 1\n",
      "2699\n",
      "sample -  945 4539\n",
      "2700\n",
      "sample -  539 162\n",
      "2701\n",
      "sample -  4 7\n",
      "2702\n",
      "sample -  1248 590\n",
      "2703\n",
      "sample -  485 106\n",
      "2704\n",
      "sample -  55 634\n",
      "2705\n",
      "sample -  1632 278\n",
      "2706\n",
      "sample -  37965 20096\n",
      "2707\n",
      "sample -  79 29\n",
      "2708\n",
      "2708\n",
      "sample -  4 11\n",
      "2709\n",
      "sample -  12 113\n",
      "2710\n",
      "sample -  5308 801\n",
      "2711\n",
      "sample -  1 1\n",
      "2712\n",
      "sample -  11 10\n",
      "2713\n",
      "sample -  3 24\n",
      "2714\n",
      "sample -  8 6\n",
      "2715\n",
      "sample -  93 1\n",
      "2716\n",
      "sample -  1 1\n",
      "2717\n",
      "sample -  1 2\n",
      "2718\n",
      "sample -  1 1\n",
      "2719\n",
      "sample -  584 303\n",
      "2720\n",
      "sample -  20937 19769\n",
      "2721\n",
      "sample -  210 54\n",
      "2722\n",
      "2722\n",
      "sample -  5571 3304\n",
      "2723\n",
      "sample -  198 55\n",
      "2724\n",
      "sample -  2 4\n",
      "2725\n",
      "sample -  143 5\n",
      "2726\n",
      "sample -  1 1\n",
      "2727\n",
      "sample -  493 44\n",
      "2728\n",
      "sample -  38 4086\n",
      "2729\n",
      "sample -  1 1\n",
      "2730\n",
      "2730\n",
      "sample -  16444 18607\n",
      "2731\n",
      "sample -  2 1\n",
      "2732\n",
      "sample -  1 1\n",
      "2733\n",
      "sample -  226 3\n",
      "2734\n",
      "sample -  1 1\n",
      "2735\n",
      "sample -  10 77\n",
      "2736\n",
      "sample -  154 514\n",
      "2737\n",
      "sample -  6 26\n",
      "2738\n",
      "sample -  1 1\n",
      "2739\n",
      "sample -  406 65\n",
      "2740\n",
      "sample -  5 6\n",
      "2741\n",
      "sample -  1 1\n",
      "2742\n",
      "sample -  3723 10383\n",
      "2743\n",
      "sample -  52 3\n",
      "2744\n",
      "sample -  1 2\n",
      "2745\n",
      "sample -  9238 2179\n",
      "2746\n",
      "sample -  27 106\n",
      "2747\n",
      "sample -  1 1\n",
      "2748\n",
      "sample -  1 1\n",
      "2749\n",
      "sample -  19583 27022\n",
      "2750\n",
      "sample -  1 1\n",
      "2751\n",
      "sample -  1 1\n",
      "2752\n",
      "sample -  42 7\n",
      "2753\n",
      "sample -  1 1\n",
      "2754\n",
      "sample -  1 1\n",
      "2755\n",
      "sample -  1 1\n",
      "2756\n",
      "sample -  707 1922\n",
      "2757\n",
      "sample -  1 1\n",
      "2758\n",
      "sample -  28 54\n",
      "2759\n",
      "sample -  6 31\n",
      "2760\n",
      "sample -  2334 464\n",
      "2761\n",
      "sample -  6023 180\n",
      "2762\n",
      "sample -  17 4\n",
      "2763\n",
      "sample -  14 93\n",
      "2764\n",
      "2764\n",
      "sample -  19221 19378\n",
      "2765\n",
      "sample -  6451 6512\n",
      "2766\n",
      "sample -  6114 797\n",
      "2767\n",
      "sample -  27808 7942\n",
      "2768\n",
      "sample -  1566 163\n",
      "2769\n",
      "sample -  1 1\n",
      "2770\n",
      "sample -  2 2\n",
      "2771\n",
      "sample -  554 101\n",
      "2772\n",
      "sample -  1 1\n",
      "2773\n",
      "sample -  343 64\n",
      "2774\n",
      "sample -  1 1\n",
      "2775\n",
      "sample -  1 1\n",
      "2776\n",
      "sample -  145 772\n",
      "2777\n",
      "sample -  1 1\n",
      "2778\n",
      "sample -  1 16\n",
      "2779\n",
      "sample -  1 2\n",
      "2780\n",
      "sample -  336 1495\n",
      "2781\n",
      "sample -  1149 89\n",
      "2782\n",
      "sample -  705 144\n",
      "2783\n",
      "sample -  1 1\n",
      "2784\n",
      "sample -  1 1\n",
      "2785\n",
      "2785\n",
      "sample -  1 1\n",
      "2786\n",
      "sample -  3263 1772\n",
      "2787\n",
      "sample -  1 2\n",
      "2788\n",
      "sample -  43 102\n",
      "2789\n",
      "sample -  19 80\n",
      "2790\n",
      "sample -  3 5\n",
      "2791\n",
      "sample -  9369 9629\n",
      "2792\n",
      "sample -  1 1\n",
      "2793\n",
      "sample -  1807 248\n",
      "2794\n",
      "sample -  2 1\n",
      "2795\n",
      "sample -  10135 3696\n",
      "2796\n",
      "sample -  292 2440\n",
      "2797\n",
      "sample -  1 1\n",
      "2798\n",
      "sample -  191 27\n",
      "2799\n",
      "sample -  1 2\n",
      "2800\n",
      "sample -  67 655\n",
      "2801\n",
      "sample -  1 1\n",
      "2802\n",
      "2802\n",
      "sample -  2 2\n",
      "2803\n",
      "sample -  1 1\n",
      "2804\n",
      "sample -  70 2\n",
      "2805\n",
      "sample -  1 1\n",
      "2806\n",
      "sample -  3112 7397\n",
      "2807\n",
      "sample -  1 4\n",
      "2808\n",
      "sample -  1 1\n",
      "2809\n",
      "sample -  22 135\n",
      "2810\n",
      "sample -  195 35\n",
      "2811\n",
      "sample -  38796 22010\n",
      "2812\n",
      "2812\n",
      "sample -  16027 2847\n",
      "2813\n",
      "sample -  2 1\n",
      "2814\n",
      "sample -  7 38\n",
      "2815\n",
      "sample -  4 1\n",
      "2816\n",
      "sample -  1 1\n",
      "2817\n",
      "sample -  1 1\n",
      "2818\n",
      "sample -  1 1\n",
      "2819\n",
      "sample -  2 1\n",
      "2820\n",
      "sample -  1 1\n",
      "2821\n",
      "sample -  265 300\n",
      "2822\n",
      "sample -  16183 13589\n",
      "2823\n",
      "sample -  23 5\n",
      "2824\n",
      "sample -  1 1\n",
      "2825\n",
      "sample -  305 44\n",
      "2826\n",
      "sample -  7 4\n",
      "2827\n",
      "sample -  1 1\n",
      "2828\n",
      "sample -  5 13\n",
      "2829\n",
      "sample -  54 528\n",
      "2830\n",
      "sample -  22408 14292\n",
      "2831\n",
      "sample -  2 1\n",
      "2832\n",
      "2832\n",
      "sample -  424 803\n",
      "2833\n",
      "sample -  3 4\n",
      "2834\n",
      "sample -  1 1\n",
      "2835\n",
      "sample -  2 3\n",
      "2836\n",
      "sample -  11624 1520\n",
      "2837\n",
      "sample -  2 1\n",
      "2838\n",
      "sample -  12 1221\n",
      "2839\n",
      "sample -  2414 52\n",
      "2840\n",
      "sample -  3 1\n",
      "2841\n",
      "2841\n",
      "sample -  4234 465\n",
      "2842\n",
      "sample -  16 11\n",
      "2843\n",
      "sample -  1 1\n",
      "2844\n",
      "sample -  48 27\n",
      "2845\n",
      "sample -  34500 27021\n",
      "2846\n",
      "sample -  33 34\n",
      "2847\n",
      "sample -  54 182\n",
      "2848\n",
      "sample -  2714 1320\n",
      "2849\n",
      "sample -  8 4\n",
      "2850\n",
      "sample -  6276 4139\n",
      "2851\n",
      "sample -  1 1\n",
      "2852\n",
      "sample -  9 1\n",
      "2853\n",
      "2853\n",
      "sample -  9208 3163\n",
      "2854\n",
      "sample -  1 1\n",
      "2855\n",
      "sample -  3424 121\n",
      "2856\n",
      "sample -  1405 3537\n",
      "2857\n",
      "sample -  2730 2261\n",
      "2858\n",
      "sample -  4747 23730\n",
      "2859\n",
      "sample -  3 4\n",
      "2860\n",
      "sample -  12 2\n",
      "2861\n",
      "sample -  1 1\n",
      "2862\n",
      "sample -  1 1\n",
      "2863\n",
      "sample -  28822 8257\n",
      "2864\n",
      "sample -  77 48\n",
      "2865\n",
      "sample -  1 1\n",
      "2866\n",
      "sample -  15210 9050\n",
      "2867\n",
      "sample -  1 1\n",
      "2868\n",
      "sample -  1 1\n",
      "2869\n",
      "sample -  172 24\n",
      "2870\n",
      "sample -  12 3\n",
      "2871\n",
      "sample -  17 41\n",
      "2872\n",
      "sample -  1 3\n",
      "2873\n",
      "sample -  2 2\n",
      "2874\n",
      "sample -  1646 378\n",
      "2875\n",
      "sample -  2260 1036\n",
      "2876\n",
      "sample -  1 3\n",
      "2877\n",
      "sample -  17 2\n",
      "2878\n",
      "sample -  13 5\n",
      "2879\n",
      "sample -  22959 1373\n",
      "2880\n",
      "sample -  1 1\n",
      "2881\n",
      "sample -  1 1\n",
      "2882\n",
      "sample -  5 1\n",
      "2883\n",
      "sample -  1 1\n",
      "2884\n",
      "sample -  13 4\n",
      "2885\n",
      "sample -  1 1\n",
      "2886\n",
      "2886\n",
      "sample -  1 1\n",
      "2887\n",
      "sample -  1 2\n",
      "2888\n",
      "sample -  1 1\n",
      "2889\n",
      "sample -  6 5\n",
      "2890\n",
      "sample -  8328 794\n",
      "2891\n",
      "sample -  17 4\n",
      "2892\n",
      "sample -  38792 12663\n",
      "2893\n",
      "2893\n",
      "sample -  1021 263\n",
      "2894\n",
      "sample -  5 5\n",
      "2895\n",
      "sample -  1 2\n",
      "2896\n",
      "sample -  6 1\n",
      "2897\n",
      "sample -  3 2\n",
      "2898\n",
      "sample -  1 1\n",
      "2899\n",
      "sample -  1 1\n",
      "2900\n",
      "sample -  3 2\n",
      "2901\n",
      "sample -  5 1\n",
      "2902\n",
      "sample -  6 3\n",
      "2903\n",
      "2903\n",
      "sample -  1 1\n",
      "2904\n",
      "sample -  5238 7128\n",
      "2905\n",
      "sample -  1 1\n",
      "2906\n",
      "2906\n",
      "sample -  1 2\n",
      "2907\n",
      "sample -  11 2\n",
      "2908\n",
      "sample -  9 3\n",
      "2909\n",
      "sample -  1 1\n",
      "2910\n",
      "sample -  29205 15410\n",
      "2911\n",
      "sample -  1 1\n",
      "2912\n",
      "2912\n",
      "sample -  8 3\n",
      "2913\n",
      "sample -  297 37\n",
      "2914\n",
      "sample -  1 1\n",
      "2915\n",
      "2915\n",
      "sample -  168 23\n",
      "2916\n",
      "sample -  81 630\n",
      "2917\n",
      "sample -  2 2\n",
      "2918\n",
      "sample -  1 1\n",
      "2919\n",
      "sample -  3 8\n",
      "2920\n",
      "sample -  1 1\n",
      "2921\n",
      "sample -  1 1\n",
      "2922\n",
      "sample -  30861 23315\n",
      "2923\n",
      "sample -  212 20\n",
      "here 2924\n",
      "\n",
      "Current iteration time 1646.8551361560822\n",
      "Stats for replacing head are -> \n",
      "Current iteration Hits@100 are 0.6925444596443229\n",
      "Current iteration Hits@10 are 0.5530095759233926\n",
      "Current iteration Hits@3 are 0.46443228454172364\n",
      "Current iteration Hits@1 are 0.3512311901504788\n",
      "Current iteration Mean rank 2430.063611491108\n",
      "Current iteration Mean Reciprocal Rank 0.4232347076047195\n",
      "\n",
      "Stats for replacing tail are -> \n",
      "Current iteration Hits@100 are 0.7270861833105335\n",
      "Current iteration Hits@10 are 0.5872093023255814\n",
      "Current iteration Hits@3 are 0.4887140902872777\n",
      "Current iteration Hits@1 are 0.3512311901504788\n",
      "Current iteration Mean rank 1517.1350889192886\n",
      "Current iteration Mean Reciprocal Rank 0.4368837445879888\n",
      "\n",
      "Averaged stats for replacing head are -> \n",
      "Hits@100 are 0.6925444596443229\n",
      "Hits@10 are 0.5530095759233926\n",
      "Hits@3 are 0.46443228454172364\n",
      "Hits@1 are 0.3512311901504788\n",
      "Mean rank 2430.063611491108\n",
      "Mean Reciprocal Rank 0.4232347076047195\n",
      "\n",
      "Averaged stats for replacing tail are -> \n",
      "Hits@100 are 0.7270861833105335\n",
      "Hits@10 are 0.5872093023255814\n",
      "Hits@3 are 0.4887140902872777\n",
      "Hits@1 are 0.3512311901504788\n",
      "Mean rank 1517.1350889192886\n",
      "Mean Reciprocal Rank 0.4368837445879888\n",
      "\n",
      "Cumulative stats are -> \n",
      "Hits@100 are 0.7098153214774282\n",
      "Hits@10 are 0.570109439124487\n",
      "Hits@3 are 0.47657318741450067\n",
      "Hits@1 are 0.3512311901504788\n",
      "Mean rank 1973.5993502051983\n",
      "Mean Reciprocal Rank 0.43005922609635416\n"
     ]
    }
   ],
   "source": [
    "def evaluate_conv(args, unique_entities):\n",
    "    model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "    model_conv.load_state_dict(torch.load(\n",
    "        '{0}/trained_{1}.pth'.format(\"/content/conv\", args.epochs_conv - 1)), strict=False)\n",
    "\n",
    "    model_conv.cuda()\n",
    "    model_conv.eval()\n",
    "    with torch.no_grad():\n",
    "        Corpus_.get_validation_pred(args, model_conv, unique_entities)\n",
    "\n",
    "\n",
    "evaluate_conv(args, Corpus_.unique_entities_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "leN-knzDWsGY"
   },
   "source": [
    "# Model Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F277Iwwnbc-c"
   },
   "source": [
    "## Model : SpKBGATModified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o6hcflQnXSvt"
   },
   "outputs": [],
   "source": [
    "\n",
    "model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "\n",
    "model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                              args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                              args.nheads_GAT, args.out_channels)\n",
    "\n",
    "if CUDA:\n",
    "    model_gat.cuda()\n",
    "    model_conv.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "x_QVGk3-XW7O",
    "outputId": "c6e99b44-b26a-4c6d-e1d9-b505a27fc8d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of unique_entities  40559\n"
     ]
    }
   ],
   "source": [
    "current_batch_2hop_indices = torch.tensor([])\n",
    "if(args.use_2hop):\n",
    "    current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\n",
    "                                                                      Corpus_.unique_entities_train, node_neighbors_2hop)\n",
    "\n",
    "if CUDA:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices)).cuda()\n",
    "else:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices))\n",
    "\n",
    "train_indices, train_values = Corpus_.get_iteration_batch(0)\n",
    "\n",
    "if CUDA:\n",
    "    train_indices = Variable(\n",
    "        torch.LongTensor(train_indices)).cuda()\n",
    "    train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "entity_embed, relation_embed = model_gat(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "11phU2ofaNjw",
    "outputId": "cba2c4ba-cd28-4f20-f9f0-df0cca059c11"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpKBGATModified(\n",
       "  (sparse_gat_1): SpGAT(\n",
       "    (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "    (attention_0): SpGraphAttentionLayer (50 -> 100)\n",
       "    (attention_1): SpGraphAttentionLayer (50 -> 100)\n",
       "    (out_att): SpGraphAttentionLayer (200 -> 200)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_gat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "y-x6Q4gpaycu",
    "outputId": "54abf8d7-562c-4c6b-9c97-c9c46a9eb47e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_entity_embeddings : torch.Size([40943, 200])\n",
      "final_relation_embeddings : torch.Size([11, 200])\n",
      "entity_embeddings : torch.Size([40943, 50])\n",
      "relation_embeddings : torch.Size([11, 50])\n",
      "W_entities : torch.Size([50, 200])\n",
      "sparse_gat_1.W : torch.Size([50, 200])\n",
      "sparse_gat_1.attention_0.a : torch.Size([100, 150])\n",
      "sparse_gat_1.attention_0.a_2 : torch.Size([1, 100])\n",
      "sparse_gat_1.attention_1.a : torch.Size([100, 150])\n",
      "sparse_gat_1.attention_1.a_2 : torch.Size([1, 100])\n",
      "sparse_gat_1.out_att.a : torch.Size([200, 600])\n",
      "sparse_gat_1.out_att.a_2 : torch.Size([1, 200])\n"
     ]
    }
   ],
   "source": [
    "dict_gat = model_gat.state_dict()\n",
    "\n",
    "for para_name in dict_gat:\n",
    "  print(\"{0} : {1}\".format(para_name, dict_gat[para_name].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5jDkOunYbibt"
   },
   "source": [
    "## Model : SpKBGATConvOnly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoU8rzbcRbYM"
   },
   "outputs": [],
   "source": [
    "model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "Corpus_.batch_size = args.batch_size_conv\n",
    "Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jw5SOvAJd2cS"
   },
   "outputs": [],
   "source": [
    "model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "Corpus_.batch_size = args.batch_size_conv\n",
    "Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "epoch_losses = []   # losses of all epochs\n",
    "\n",
    "for epoch in range(1):\n",
    "    random.shuffle(Corpus_.train_triples)\n",
    "    Corpus_.train_indices = np.array(\n",
    "        list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "    model_conv.train()  # getting in training mode\n",
    "    epoch_loss = []\n",
    "\n",
    "\n",
    "    for iters in range(1):\n",
    "        train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "        if CUDA:\n",
    "            train_indices = Variable(\n",
    "                torch.LongTensor(train_indices)).cuda()\n",
    "            train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "        else:\n",
    "            train_indices = Variable(torch.LongTensor(train_indices))\n",
    "            train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "        preds = model_conv(\n",
    "            Corpus_, Corpus_.train_adj_matrix, train_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "ZCDezP-bWSoY",
    "outputId": "d7564d80-6194-41c9-eaf8-e80a2037c01d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpKBGATConvOnly(\n",
       "  (convKB): ConvKB(\n",
       "    (conv_layer): Conv2d(1, 500, kernel_size=(1, 3), stride=(1, 1))\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (non_linearity): ReLU()\n",
       "    (fc_layer): Linear(in_features=100000, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "jKpvTGZBbbEF",
    "outputId": "aa6224de-5c91-43cd-adf3-0444a5b74c19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_entity_embeddings : torch.Size([40943, 200])\n",
      "final_relation_embeddings : torch.Size([11, 200])\n",
      "convKB.conv_layer.weight : torch.Size([500, 1, 1, 3])\n",
      "convKB.conv_layer.bias : torch.Size([500])\n",
      "convKB.fc_layer.weight : torch.Size([1, 100000])\n",
      "convKB.fc_layer.bias : torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "dict_conv = model_conv.state_dict()\n",
    "\n",
    "for para_name in dict_conv:\n",
    "  print(\"{0} : {1}\".format(para_name, dict_conv[para_name].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IZNOu8A6ca-5"
   },
   "source": [
    "## Model : SpGAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fkjYcTl5WvDL"
   },
   "outputs": [],
   "source": [
    "num_nodes = 40943\n",
    "entity_in_dim = 200\n",
    "entity_out_dim_1 = 200\n",
    "relation_dim = 200\n",
    "drop_GAT = 0.3\n",
    "alpha = 0.2\n",
    "nheads_GAT_1 = 2\n",
    "\n",
    "\n",
    "sparse_gat_1 = SpGAT(num_nodes, entity_in_dim, entity_out_dim_1, relation_dim,\n",
    "                                  args.drop_GAT, alpha, nheads_GAT_1)\n",
    "# # Rename input \n",
    "# adj = Corpus_.train_adj_matrix\n",
    "# batch_inputs = train_indices\n",
    "# train_indices_nhop = current_batch_2hop_indices\n",
    "# ###################################\n",
    "# edge_list = adj[0] # head_id and tail_id\n",
    "# edge_type = adj[1] # relation_id\n",
    "\n",
    "# edge_list_nhop = torch.cat(\n",
    "#     (train_indices_nhop[:, 3].unsqueeze(-1), train_indices_nhop[:, 0].unsqueeze(-1)), dim=1).t()\n",
    "# edge_type_nhop = torch.cat(\n",
    "#     [train_indices_nhop[:, 1].unsqueeze(-1), train_indices_nhop[:, 2].unsqueeze(-1)], dim=1)\n",
    "\n",
    "# if(CUDA):\n",
    "#     edge_list = edge_list.cuda()\n",
    "#     edge_type = edge_type.cuda()\n",
    "#     edge_list_nhop = edge_list_nhop.cuda()\n",
    "#     edge_type_nhop = edge_type_nhop.cuda()\n",
    "\n",
    "# edge_embed = relation_embeddings[edge_type]\n",
    "# entity_embeddings.data = F.normalize(\n",
    "#     entity_embeddings.data, p=2, dim=1).detach()\n",
    "\n",
    "# out_entity_1, out_relation_1 = sparse_gat_1(\n",
    "#     Corpus_, batch_inputs, entity_embeddings, relation_embeddings,\n",
    "#     edge_list, edge_type, edge_embed, edge_list_nhop, edge_type_nhop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "SzmEzomkcV4r",
    "outputId": "66671110-17e2-4224-f6ed-8e0c40d473da"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpGAT(\n",
       "  (dropout_layer): Dropout(p=0.3, inplace=False)\n",
       "  (attention_0): SpGraphAttentionLayer (200 -> 200)\n",
       "  (attention_1): SpGraphAttentionLayer (200 -> 200)\n",
       "  (out_att): SpGraphAttentionLayer (400 -> 400)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparse_gat_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "FM44eUBDclr3",
    "outputId": "b1102d1c-715d-476c-8656-aaea91a846d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W : torch.Size([200, 400])\n",
      "attention_0.a : torch.Size([200, 600])\n",
      "attention_0.a_2 : torch.Size([1, 200])\n",
      "attention_1.a : torch.Size([200, 600])\n",
      "attention_1.a_2 : torch.Size([1, 200])\n",
      "out_att.a : torch.Size([400, 1200])\n",
      "out_att.a_2 : torch.Size([1, 400])\n"
     ]
    }
   ],
   "source": [
    "dict_sp = sparse_gat_1.state_dict()\n",
    "\n",
    "for para_name in dict_sp:\n",
    "  print(\"{0} : {1}\".format(para_name, dict_sp[para_name].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hBknvawxd7ie"
   },
   "source": [
    "# Model Visualizing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEBxQs9m4O9v"
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter('/content/tensorboard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STZvSD6w4dN3"
   },
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(train_indices, batch_size=4,\n",
    "                                        shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "OVCSXJkn1dSd",
    "outputId": "f70f3e2e-b07d-4ff0-fb04-c717c2dbb962"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'Tuple[int, int, int, int, float, float, int]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced\n",
      "Error occurs, No graph saved\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-62-eb5f393691b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m writer.add_graph(sparse_gat_1, (num_nodes, entity_in_dim, entity_out_dim_1, relation_dim,\n\u001b[0;32m----> 2\u001b[0;31m                                   args.drop_GAT, alpha, nheads_GAT_1))\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/writer.py\u001b[0m in \u001b[0;36madd_graph\u001b[0;34m(self, model, input_to_model, verbose)\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m             \u001b[0;31m# A valid PyTorch model should have a 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_to_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    709\u001b[0m             \u001b[0;31m# Caffe2 models do not have the 'forward' method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    289\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Error occurs, No graph saved'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/tensorboard/_pytorch_graph.py\u001b[0m in \u001b[0;36mgraph\u001b[0;34m(model, args, verbose)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# TODO: move outside of torch.onnx?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m             \u001b[0mtrace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m             \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_pass_inline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace\u001b[0;34m(func, example_inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m    873\u001b[0m         return trace_module(func, {'forward': example_inputs}, None,\n\u001b[1;32m    874\u001b[0m                             \u001b[0mcheck_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrap_check_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m                             check_tolerance, _force_outplace, _module_class)\n\u001b[0m\u001b[1;32m    876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m     if (hasattr(func, '__self__') and isinstance(func.__self__, torch.nn.Module) and\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/jit/__init__.py\u001b[0m in \u001b[0;36mtrace_module\u001b[0;34m(mod, inputs, optimize, check_trace, check_inputs, check_tolerance, _force_outplace, _module_class, _compilation_unit)\u001b[0m\n\u001b[1;32m   1025\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmethod_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"forward\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0mexample_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_method_from_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_lookup_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_force_outplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mcheck_trace_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1029\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Type 'Tuple[int, int, int, int, float, float, int]' cannot be traced. Only Tensors and (possibly nested) Lists, Dicts, and Tuples of Tensors can be traced"
     ]
    }
   ],
   "source": [
    "writer.add_graph(sparse_gat_1, (num_nodes, entity_in_dim, entity_out_dim_1, relation_dim,\n",
    "                                  args.drop_GAT, alpha, nheads_GAT_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jWT0L27O3UL7",
    "outputId": "1655b77f-4526-42b2-ed69-8586b69c5f9b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5248, 3])"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RbJTiTg1Ckj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zUeedpyeYjz0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TrkCqFO8WvLi"
   },
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R1XZ30kxWvTs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "0hMcLCGiar_-",
    "outputId": "e6dd9cc5-8ad1-441d-8b4d-d1681a335025"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining model\n",
      "\n",
      "Model type -> GAT layer with 2 heads used , Initital Embeddings training\n",
      "length of unique_entities  40559\n",
      "Number of epochs 3600\n"
     ]
    }
   ],
   "source": [
    "# Creating the gat model here.\n",
    "####################################\n",
    "\n",
    "print(\"Defining model\")\n",
    "\n",
    "print(\n",
    "    \"\\nModel type -> GAT layer with {} heads used , Initital Embeddings training\".format(args.nheads_GAT[0]))\n",
    "model_gat = SpKBGATModified(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                            args.drop_GAT, args.alpha, args.nheads_GAT)\n",
    "\n",
    "if CUDA:\n",
    "    model_gat.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=500, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "gat_loss_func = nn.MarginRankingLoss(margin=args.margin)\n",
    "\n",
    "current_batch_2hop_indices = torch.tensor([])\n",
    "if(args.use_2hop):\n",
    "    current_batch_2hop_indices = Corpus_.get_batch_nhop_neighbors_all(args,\n",
    "                                                                      Corpus_.unique_entities_train, node_neighbors_2hop)\n",
    "\n",
    "if CUDA:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices)).cuda()\n",
    "else:\n",
    "    current_batch_2hop_indices = Variable(\n",
    "        torch.LongTensor(current_batch_2hop_indices))\n",
    "\n",
    "epoch_losses = []   # losses of all epochs\n",
    "print(\"Number of epochs {}\".format(args.epochs_gat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "et7l0bIAa37y",
    "outputId": "9fe559dd-5570-4229-a99b-9faf0a36d593"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch->  0\n",
      "Iteration-> 0  , Iteration_time-> 1.6560 , Iteration_loss 9.7971\n",
      "Epoch 0 , average loss 9.797070503234863 , epoch_time 1.6561825275421143\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  1\n",
      "Iteration-> 0  , Iteration_time-> 1.2222 , Iteration_loss 9.4253\n",
      "Epoch 1 , average loss 9.425337791442871 , epoch_time 1.2224769592285156\n",
      "\n",
      "epoch->  2\n",
      "Iteration-> 0  , Iteration_time-> 1.2487 , Iteration_loss 9.0414\n",
      "Epoch 2 , average loss 9.041407585144043 , epoch_time 1.2488822937011719\n",
      "\n",
      "epoch->  3\n",
      "Iteration-> 0  , Iteration_time-> 1.2173 , Iteration_loss 8.6528\n",
      "Epoch 3 , average loss 8.652764320373535 , epoch_time 1.2175490856170654\n",
      "\n",
      "epoch->  4\n",
      "Iteration-> 0  , Iteration_time-> 1.2242 , Iteration_loss 8.2719\n",
      "Epoch 4 , average loss 8.271947860717773 , epoch_time 1.224367380142212\n",
      "\n",
      "epoch->  5\n",
      "Iteration-> 0  , Iteration_time-> 1.2199 , Iteration_loss 7.9355\n",
      "Epoch 5 , average loss 7.935461521148682 , epoch_time 1.220139980316162\n",
      "\n",
      "epoch->  6\n",
      "Iteration-> 0  , Iteration_time-> 1.2123 , Iteration_loss 7.6180\n",
      "Epoch 6 , average loss 7.618046283721924 , epoch_time 1.2125353813171387\n",
      "\n",
      "epoch->  7\n",
      "Iteration-> 0  , Iteration_time-> 1.2232 , Iteration_loss 7.3481\n",
      "Epoch 7 , average loss 7.348085403442383 , epoch_time 1.2234573364257812\n",
      "\n",
      "epoch->  8\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 7.1338\n",
      "Epoch 8 , average loss 7.133815765380859 , epoch_time 1.2175836563110352\n",
      "\n",
      "epoch->  9\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 6.9518\n",
      "Epoch 9 , average loss 6.951755523681641 , epoch_time 1.207761287689209\n",
      "\n",
      "epoch->  10\n",
      "Iteration-> 0  , Iteration_time-> 1.2060 , Iteration_loss 6.7867\n",
      "Epoch 10 , average loss 6.78674840927124 , epoch_time 1.2062084674835205\n",
      "\n",
      "epoch->  11\n",
      "Iteration-> 0  , Iteration_time-> 1.2038 , Iteration_loss 6.6151\n",
      "Epoch 11 , average loss 6.6151227951049805 , epoch_time 1.2041025161743164\n",
      "\n",
      "epoch->  12\n",
      "Iteration-> 0  , Iteration_time-> 1.2129 , Iteration_loss 6.4675\n",
      "Epoch 12 , average loss 6.467486381530762 , epoch_time 1.213120937347412\n",
      "\n",
      "epoch->  13\n",
      "Iteration-> 0  , Iteration_time-> 1.2066 , Iteration_loss 6.3430\n",
      "Epoch 13 , average loss 6.342957019805908 , epoch_time 1.2068390846252441\n",
      "\n",
      "epoch->  14\n",
      "Iteration-> 0  , Iteration_time-> 1.2086 , Iteration_loss 6.2003\n",
      "Epoch 14 , average loss 6.200253486633301 , epoch_time 1.208909273147583\n",
      "\n",
      "epoch->  15\n",
      "Iteration-> 0  , Iteration_time-> 1.2105 , Iteration_loss 6.0757\n",
      "Epoch 15 , average loss 6.07565450668335 , epoch_time 1.2107152938842773\n",
      "\n",
      "epoch->  16\n",
      "Iteration-> 0  , Iteration_time-> 1.2245 , Iteration_loss 5.9690\n",
      "Epoch 16 , average loss 5.968954563140869 , epoch_time 1.2247836589813232\n",
      "\n",
      "epoch->  17\n",
      "Iteration-> 0  , Iteration_time-> 1.2222 , Iteration_loss 5.8324\n",
      "Epoch 17 , average loss 5.832404136657715 , epoch_time 1.222463607788086\n",
      "\n",
      "epoch->  18\n",
      "Iteration-> 0  , Iteration_time-> 1.2156 , Iteration_loss 5.7211\n",
      "Epoch 18 , average loss 5.721078872680664 , epoch_time 1.2158286571502686\n",
      "\n",
      "epoch->  19\n",
      "Iteration-> 0  , Iteration_time-> 1.2083 , Iteration_loss 5.5980\n",
      "Epoch 19 , average loss 5.59796142578125 , epoch_time 1.2085638046264648\n",
      "\n",
      "epoch->  20\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 5.5105\n",
      "Epoch 20 , average loss 5.510478973388672 , epoch_time 1.210944652557373\n",
      "\n",
      "epoch->  21\n",
      "Iteration-> 0  , Iteration_time-> 1.2059 , Iteration_loss 5.4060\n",
      "Epoch 21 , average loss 5.406010627746582 , epoch_time 1.2060964107513428\n",
      "\n",
      "epoch->  22\n",
      "Iteration-> 0  , Iteration_time-> 1.2136 , Iteration_loss 5.3171\n",
      "Epoch 22 , average loss 5.317124843597412 , epoch_time 1.2137658596038818\n",
      "\n",
      "epoch->  23\n",
      "Iteration-> 0  , Iteration_time-> 1.2182 , Iteration_loss 5.2119\n",
      "Epoch 23 , average loss 5.211884021759033 , epoch_time 1.218358039855957\n",
      "\n",
      "epoch->  24\n",
      "Iteration-> 0  , Iteration_time-> 1.2265 , Iteration_loss 5.1300\n",
      "Epoch 24 , average loss 5.1300458908081055 , epoch_time 1.2266554832458496\n",
      "\n",
      "epoch->  25\n",
      "Iteration-> 0  , Iteration_time-> 1.2117 , Iteration_loss 5.0325\n",
      "Epoch 25 , average loss 5.032534122467041 , epoch_time 1.21189284324646\n",
      "\n",
      "epoch->  26\n",
      "Iteration-> 0  , Iteration_time-> 1.2205 , Iteration_loss 4.9477\n",
      "Epoch 26 , average loss 4.947713375091553 , epoch_time 1.220729112625122\n",
      "\n",
      "epoch->  27\n",
      "Iteration-> 0  , Iteration_time-> 1.2241 , Iteration_loss 4.8909\n",
      "Epoch 27 , average loss 4.890949249267578 , epoch_time 1.2243413925170898\n",
      "\n",
      "epoch->  28\n",
      "Iteration-> 0  , Iteration_time-> 1.2339 , Iteration_loss 4.7954\n",
      "Epoch 28 , average loss 4.795355319976807 , epoch_time 1.2341184616088867\n",
      "\n",
      "epoch->  29\n",
      "Iteration-> 0  , Iteration_time-> 1.2097 , Iteration_loss 4.7301\n",
      "Epoch 29 , average loss 4.730057239532471 , epoch_time 1.2099313735961914\n",
      "\n",
      "epoch->  30\n",
      "Iteration-> 0  , Iteration_time-> 1.2259 , Iteration_loss 4.6670\n",
      "Epoch 30 , average loss 4.667011737823486 , epoch_time 1.2261316776275635\n",
      "\n",
      "epoch->  31\n",
      "Iteration-> 0  , Iteration_time-> 1.2261 , Iteration_loss 4.5948\n",
      "Epoch 31 , average loss 4.594799995422363 , epoch_time 1.22636079788208\n",
      "\n",
      "epoch->  32\n",
      "Iteration-> 0  , Iteration_time-> 1.2040 , Iteration_loss 4.5441\n",
      "Epoch 32 , average loss 4.544144630432129 , epoch_time 1.204211711883545\n",
      "\n",
      "epoch->  33\n",
      "Iteration-> 0  , Iteration_time-> 1.2089 , Iteration_loss 4.4778\n",
      "Epoch 33 , average loss 4.477794170379639 , epoch_time 1.209123134613037\n",
      "\n",
      "epoch->  34\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 4.4322\n",
      "Epoch 34 , average loss 4.432223320007324 , epoch_time 1.2107949256896973\n",
      "\n",
      "epoch->  35\n",
      "Iteration-> 0  , Iteration_time-> 1.2082 , Iteration_loss 4.3687\n",
      "Epoch 35 , average loss 4.368653297424316 , epoch_time 1.2084050178527832\n",
      "\n",
      "epoch->  36\n",
      "Iteration-> 0  , Iteration_time-> 1.2239 , Iteration_loss 4.3125\n",
      "Epoch 36 , average loss 4.312453746795654 , epoch_time 1.2241413593292236\n",
      "\n",
      "epoch->  37\n",
      "Iteration-> 0  , Iteration_time-> 1.2061 , Iteration_loss 4.2689\n",
      "Epoch 37 , average loss 4.268913745880127 , epoch_time 1.206317663192749\n",
      "\n",
      "epoch->  38\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 4.2164\n",
      "Epoch 38 , average loss 4.21642541885376 , epoch_time 1.2108874320983887\n",
      "\n",
      "epoch->  39\n",
      "Iteration-> 0  , Iteration_time-> 1.2306 , Iteration_loss 4.1547\n",
      "Epoch 39 , average loss 4.154722213745117 , epoch_time 1.230872631072998\n",
      "\n",
      "epoch->  40\n",
      "Iteration-> 0  , Iteration_time-> 1.2142 , Iteration_loss 4.1273\n",
      "Epoch 40 , average loss 4.127281665802002 , epoch_time 1.214385747909546\n",
      "\n",
      "epoch->  41\n",
      "Iteration-> 0  , Iteration_time-> 1.2043 , Iteration_loss 4.0782\n",
      "Epoch 41 , average loss 4.078161716461182 , epoch_time 1.2044410705566406\n",
      "\n",
      "epoch->  42\n",
      "Iteration-> 0  , Iteration_time-> 1.2204 , Iteration_loss 4.0336\n",
      "Epoch 42 , average loss 4.033588409423828 , epoch_time 1.2206602096557617\n",
      "\n",
      "epoch->  43\n",
      "Iteration-> 0  , Iteration_time-> 1.2165 , Iteration_loss 3.9988\n",
      "Epoch 43 , average loss 3.9988362789154053 , epoch_time 1.2167508602142334\n",
      "\n",
      "epoch->  44\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 3.9576\n",
      "Epoch 44 , average loss 3.9576401710510254 , epoch_time 1.2109284400939941\n",
      "\n",
      "epoch->  45\n",
      "Iteration-> 0  , Iteration_time-> 1.2186 , Iteration_loss 3.9084\n",
      "Epoch 45 , average loss 3.9083991050720215 , epoch_time 1.2188286781311035\n",
      "\n",
      "epoch->  46\n",
      "Iteration-> 0  , Iteration_time-> 1.2067 , Iteration_loss 3.8793\n",
      "Epoch 46 , average loss 3.8793036937713623 , epoch_time 1.2068772315979004\n",
      "\n",
      "epoch->  47\n",
      "Iteration-> 0  , Iteration_time-> 1.2081 , Iteration_loss 3.8525\n",
      "Epoch 47 , average loss 3.852468729019165 , epoch_time 1.2083179950714111\n",
      "\n",
      "epoch->  48\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 3.8028\n",
      "Epoch 48 , average loss 3.8028225898742676 , epoch_time 1.2106733322143555\n",
      "\n",
      "epoch->  49\n",
      "Iteration-> 0  , Iteration_time-> 1.2243 , Iteration_loss 3.7708\n",
      "Epoch 49 , average loss 3.7708399295806885 , epoch_time 1.2245523929595947\n",
      "\n",
      "epoch->  50\n",
      "Iteration-> 0  , Iteration_time-> 1.2100 , Iteration_loss 3.7305\n",
      "Epoch 50 , average loss 3.7304835319519043 , epoch_time 1.2102196216583252\n",
      "\n",
      "epoch->  51\n",
      "Iteration-> 0  , Iteration_time-> 1.2057 , Iteration_loss 3.7063\n",
      "Epoch 51 , average loss 3.706298589706421 , epoch_time 1.2059485912322998\n",
      "\n",
      "epoch->  52\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 3.6805\n",
      "Epoch 52 , average loss 3.680532693862915 , epoch_time 1.2074966430664062\n",
      "\n",
      "epoch->  53\n",
      "Iteration-> 0  , Iteration_time-> 1.2259 , Iteration_loss 3.6530\n",
      "Epoch 53 , average loss 3.6530497074127197 , epoch_time 1.2261419296264648\n",
      "\n",
      "epoch->  54\n",
      "Iteration-> 0  , Iteration_time-> 1.2237 , Iteration_loss 3.6248\n",
      "Epoch 54 , average loss 3.6248178482055664 , epoch_time 1.223853588104248\n",
      "\n",
      "epoch->  55\n",
      "Iteration-> 0  , Iteration_time-> 1.2034 , Iteration_loss 3.5819\n",
      "Epoch 55 , average loss 3.581871509552002 , epoch_time 1.203596591949463\n",
      "\n",
      "epoch->  56\n",
      "Iteration-> 0  , Iteration_time-> 1.2112 , Iteration_loss 3.5559\n",
      "Epoch 56 , average loss 3.5558581352233887 , epoch_time 1.2114453315734863\n",
      "\n",
      "epoch->  57\n",
      "Iteration-> 0  , Iteration_time-> 1.2081 , Iteration_loss 3.5355\n",
      "Epoch 57 , average loss 3.535529375076294 , epoch_time 1.2082910537719727\n",
      "\n",
      "epoch->  58\n",
      "Iteration-> 0  , Iteration_time-> 1.2160 , Iteration_loss 3.5023\n",
      "Epoch 58 , average loss 3.5022521018981934 , epoch_time 1.2161877155303955\n",
      "\n",
      "epoch->  59\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 3.4731\n",
      "Epoch 59 , average loss 3.473106622695923 , epoch_time 1.2076964378356934\n",
      "\n",
      "epoch->  60\n",
      "Iteration-> 0  , Iteration_time-> 1.2115 , Iteration_loss 3.4530\n",
      "Epoch 60 , average loss 3.4530341625213623 , epoch_time 1.2117702960968018\n",
      "\n",
      "epoch->  61\n",
      "Iteration-> 0  , Iteration_time-> 1.2119 , Iteration_loss 3.4321\n",
      "Epoch 61 , average loss 3.4321136474609375 , epoch_time 1.2121303081512451\n",
      "\n",
      "epoch->  62\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 3.3960\n",
      "Epoch 62 , average loss 3.3960068225860596 , epoch_time 1.2169229984283447\n",
      "\n",
      "epoch->  63\n",
      "Iteration-> 0  , Iteration_time-> 1.2282 , Iteration_loss 3.3581\n",
      "Epoch 63 , average loss 3.358102321624756 , epoch_time 1.2284231185913086\n",
      "\n",
      "epoch->  64\n",
      "Iteration-> 0  , Iteration_time-> 1.2207 , Iteration_loss 3.3386\n",
      "Epoch 64 , average loss 3.3385989665985107 , epoch_time 1.220914363861084\n",
      "\n",
      "epoch->  65\n",
      "Iteration-> 0  , Iteration_time-> 1.2255 , Iteration_loss 3.3049\n",
      "Epoch 65 , average loss 3.3048784732818604 , epoch_time 1.2257328033447266\n",
      "\n",
      "epoch->  66\n",
      "Iteration-> 0  , Iteration_time-> 1.2212 , Iteration_loss 3.2797\n",
      "Epoch 66 , average loss 3.279653310775757 , epoch_time 1.221428632736206\n",
      "\n",
      "epoch->  67\n",
      "Iteration-> 0  , Iteration_time-> 1.2153 , Iteration_loss 3.2582\n",
      "Epoch 67 , average loss 3.258209466934204 , epoch_time 1.2155060768127441\n",
      "\n",
      "epoch->  68\n",
      "Iteration-> 0  , Iteration_time-> 1.2127 , Iteration_loss 3.2365\n",
      "Epoch 68 , average loss 3.2365009784698486 , epoch_time 1.2129154205322266\n",
      "\n",
      "epoch->  69\n",
      "Iteration-> 0  , Iteration_time-> 1.2104 , Iteration_loss 3.2024\n",
      "Epoch 69 , average loss 3.2023630142211914 , epoch_time 1.210662603378296\n",
      "\n",
      "epoch->  70\n",
      "Iteration-> 0  , Iteration_time-> 1.2282 , Iteration_loss 3.1789\n",
      "Epoch 70 , average loss 3.178863048553467 , epoch_time 1.2284460067749023\n",
      "\n",
      "epoch->  71\n",
      "Iteration-> 0  , Iteration_time-> 1.2308 , Iteration_loss 3.1498\n",
      "Epoch 71 , average loss 3.149822235107422 , epoch_time 1.2310664653778076\n",
      "\n",
      "epoch->  72\n",
      "Iteration-> 0  , Iteration_time-> 1.2209 , Iteration_loss 3.1268\n",
      "Epoch 72 , average loss 3.1268486976623535 , epoch_time 1.2210874557495117\n",
      "\n",
      "epoch->  73\n",
      "Iteration-> 0  , Iteration_time-> 1.2185 , Iteration_loss 3.1101\n",
      "Epoch 73 , average loss 3.1100540161132812 , epoch_time 1.21870756149292\n",
      "\n",
      "epoch->  74\n",
      "Iteration-> 0  , Iteration_time-> 1.2243 , Iteration_loss 3.0785\n",
      "Epoch 74 , average loss 3.078479051589966 , epoch_time 1.2245204448699951\n",
      "\n",
      "epoch->  75\n",
      "Iteration-> 0  , Iteration_time-> 1.2092 , Iteration_loss 3.0568\n",
      "Epoch 75 , average loss 3.0568161010742188 , epoch_time 1.209428310394287\n",
      "\n",
      "epoch->  76\n",
      "Iteration-> 0  , Iteration_time-> 1.2169 , Iteration_loss 3.0385\n",
      "Epoch 76 , average loss 3.038536787033081 , epoch_time 1.217099905014038\n",
      "\n",
      "epoch->  77\n",
      "Iteration-> 0  , Iteration_time-> 1.2227 , Iteration_loss 3.0066\n",
      "Epoch 77 , average loss 3.006619691848755 , epoch_time 1.2228844165802002\n",
      "\n",
      "epoch->  78\n",
      "Iteration-> 0  , Iteration_time-> 1.2222 , Iteration_loss 2.9875\n",
      "Epoch 78 , average loss 2.9874908924102783 , epoch_time 1.2223868370056152\n",
      "\n",
      "epoch->  79\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 2.9612\n",
      "Epoch 79 , average loss 2.961238384246826 , epoch_time 1.2169396877288818\n",
      "\n",
      "epoch->  80\n",
      "Iteration-> 0  , Iteration_time-> 1.2177 , Iteration_loss 2.9443\n",
      "Epoch 80 , average loss 2.9443280696868896 , epoch_time 1.2179481983184814\n",
      "\n",
      "epoch->  81\n",
      "Iteration-> 0  , Iteration_time-> 1.2074 , Iteration_loss 2.9202\n",
      "Epoch 81 , average loss 2.9202146530151367 , epoch_time 1.207592487335205\n",
      "\n",
      "epoch->  82\n",
      "Iteration-> 0  , Iteration_time-> 1.2131 , Iteration_loss 2.9071\n",
      "Epoch 82 , average loss 2.907106399536133 , epoch_time 1.2132997512817383\n",
      "\n",
      "epoch->  83\n",
      "Iteration-> 0  , Iteration_time-> 1.2221 , Iteration_loss 2.8762\n",
      "Epoch 83 , average loss 2.876174211502075 , epoch_time 1.2223434448242188\n",
      "\n",
      "epoch->  84\n",
      "Iteration-> 0  , Iteration_time-> 1.2230 , Iteration_loss 2.8587\n",
      "Epoch 84 , average loss 2.8587050437927246 , epoch_time 1.2231948375701904\n",
      "\n",
      "epoch->  85\n",
      "Iteration-> 0  , Iteration_time-> 1.2221 , Iteration_loss 2.8540\n",
      "Epoch 85 , average loss 2.854005813598633 , epoch_time 1.222259283065796\n",
      "\n",
      "epoch->  86\n",
      "Iteration-> 0  , Iteration_time-> 1.2150 , Iteration_loss 2.8289\n",
      "Epoch 86 , average loss 2.828937530517578 , epoch_time 1.2151832580566406\n",
      "\n",
      "epoch->  87\n",
      "Iteration-> 0  , Iteration_time-> 1.2223 , Iteration_loss 2.8213\n",
      "Epoch 87 , average loss 2.821279525756836 , epoch_time 1.2225656509399414\n",
      "\n",
      "epoch->  88\n",
      "Iteration-> 0  , Iteration_time-> 1.2200 , Iteration_loss 2.7981\n",
      "Epoch 88 , average loss 2.798144578933716 , epoch_time 1.2201805114746094\n",
      "\n",
      "epoch->  89\n",
      "Iteration-> 0  , Iteration_time-> 1.2192 , Iteration_loss 2.7892\n",
      "Epoch 89 , average loss 2.7892110347747803 , epoch_time 1.2193970680236816\n",
      "\n",
      "epoch->  90\n",
      "Iteration-> 0  , Iteration_time-> 1.2096 , Iteration_loss 2.7588\n",
      "Epoch 90 , average loss 2.7587509155273438 , epoch_time 1.2098150253295898\n",
      "\n",
      "epoch->  91\n",
      "Iteration-> 0  , Iteration_time-> 1.2103 , Iteration_loss 2.7380\n",
      "Epoch 91 , average loss 2.7379589080810547 , epoch_time 1.2104980945587158\n",
      "\n",
      "epoch->  92\n",
      "Iteration-> 0  , Iteration_time-> 1.2095 , Iteration_loss 2.7283\n",
      "Epoch 92 , average loss 2.7282845973968506 , epoch_time 1.2096941471099854\n",
      "\n",
      "epoch->  93\n",
      "Iteration-> 0  , Iteration_time-> 1.2442 , Iteration_loss 2.7159\n",
      "Epoch 93 , average loss 2.7158613204956055 , epoch_time 1.244431972503662\n",
      "\n",
      "epoch->  94\n",
      "Iteration-> 0  , Iteration_time-> 1.2229 , Iteration_loss 2.7005\n",
      "Epoch 94 , average loss 2.7005133628845215 , epoch_time 1.223142385482788\n",
      "\n",
      "epoch->  95\n",
      "Iteration-> 0  , Iteration_time-> 1.2078 , Iteration_loss 2.6852\n",
      "Epoch 95 , average loss 2.685185194015503 , epoch_time 1.2080368995666504\n",
      "\n",
      "epoch->  96\n",
      "Iteration-> 0  , Iteration_time-> 1.2828 , Iteration_loss 2.6642\n",
      "Epoch 96 , average loss 2.6642439365386963 , epoch_time 1.2830281257629395\n",
      "\n",
      "epoch->  97\n",
      "Iteration-> 0  , Iteration_time-> 1.2183 , Iteration_loss 2.6620\n",
      "Epoch 97 , average loss 2.6620121002197266 , epoch_time 1.2185652256011963\n",
      "\n",
      "epoch->  98\n",
      "Iteration-> 0  , Iteration_time-> 1.2205 , Iteration_loss 2.6379\n",
      "Epoch 98 , average loss 2.6379220485687256 , epoch_time 1.2207412719726562\n",
      "\n",
      "epoch->  99\n",
      "Iteration-> 0  , Iteration_time-> 1.2291 , Iteration_loss 2.6326\n",
      "Epoch 99 , average loss 2.6325623989105225 , epoch_time 1.2293736934661865\n",
      "\n",
      "epoch->  100\n",
      "Iteration-> 0  , Iteration_time-> 1.2385 , Iteration_loss 2.6103\n",
      "Epoch 100 , average loss 2.61028790473938 , epoch_time 1.2388296127319336\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  101\n",
      "Iteration-> 0  , Iteration_time-> 1.2306 , Iteration_loss 2.6050\n",
      "Epoch 101 , average loss 2.6050281524658203 , epoch_time 1.230818271636963\n",
      "\n",
      "epoch->  102\n",
      "Iteration-> 0  , Iteration_time-> 1.2336 , Iteration_loss 2.5945\n",
      "Epoch 102 , average loss 2.5945029258728027 , epoch_time 1.2338545322418213\n",
      "\n",
      "epoch->  103\n",
      "Iteration-> 0  , Iteration_time-> 1.2293 , Iteration_loss 2.5743\n",
      "Epoch 103 , average loss 2.5743443965911865 , epoch_time 1.2295198440551758\n",
      "\n",
      "epoch->  104\n",
      "Iteration-> 0  , Iteration_time-> 1.2235 , Iteration_loss 2.5642\n",
      "Epoch 104 , average loss 2.564239025115967 , epoch_time 1.2236878871917725\n",
      "\n",
      "epoch->  105\n",
      "Iteration-> 0  , Iteration_time-> 1.2152 , Iteration_loss 2.5546\n",
      "Epoch 105 , average loss 2.5546092987060547 , epoch_time 1.2154302597045898\n",
      "\n",
      "epoch->  106\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 2.5320\n",
      "Epoch 106 , average loss 2.531999349594116 , epoch_time 1.2118263244628906\n",
      "\n",
      "epoch->  107\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 2.5242\n",
      "Epoch 107 , average loss 2.524198532104492 , epoch_time 1.217695951461792\n",
      "\n",
      "epoch->  108\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 2.5203\n",
      "Epoch 108 , average loss 2.5203404426574707 , epoch_time 1.217611312866211\n",
      "\n",
      "epoch->  109\n",
      "Iteration-> 0  , Iteration_time-> 1.2055 , Iteration_loss 2.5012\n",
      "Epoch 109 , average loss 2.501164197921753 , epoch_time 1.2057201862335205\n",
      "\n",
      "epoch->  110\n",
      "Iteration-> 0  , Iteration_time-> 1.2117 , Iteration_loss 2.4915\n",
      "Epoch 110 , average loss 2.49153995513916 , epoch_time 1.2119359970092773\n",
      "\n",
      "epoch->  111\n",
      "Iteration-> 0  , Iteration_time-> 1.2099 , Iteration_loss 2.4856\n",
      "Epoch 111 , average loss 2.485605478286743 , epoch_time 1.210118293762207\n",
      "\n",
      "epoch->  112\n",
      "Iteration-> 0  , Iteration_time-> 1.2152 , Iteration_loss 2.4690\n",
      "Epoch 112 , average loss 2.4689528942108154 , epoch_time 1.2154178619384766\n",
      "\n",
      "epoch->  113\n",
      "Iteration-> 0  , Iteration_time-> 1.2146 , Iteration_loss 2.4566\n",
      "Epoch 113 , average loss 2.456618070602417 , epoch_time 1.2148017883300781\n",
      "\n",
      "epoch->  114\n",
      "Iteration-> 0  , Iteration_time-> 1.2371 , Iteration_loss 2.4502\n",
      "Epoch 114 , average loss 2.450197219848633 , epoch_time 1.2373261451721191\n",
      "\n",
      "epoch->  115\n",
      "Iteration-> 0  , Iteration_time-> 1.2497 , Iteration_loss 2.4419\n",
      "Epoch 115 , average loss 2.4419102668762207 , epoch_time 1.2498764991760254\n",
      "\n",
      "epoch->  116\n",
      "Iteration-> 0  , Iteration_time-> 1.2160 , Iteration_loss 2.4245\n",
      "Epoch 116 , average loss 2.424459934234619 , epoch_time 1.2162106037139893\n",
      "\n",
      "epoch->  117\n",
      "Iteration-> 0  , Iteration_time-> 1.2212 , Iteration_loss 2.4209\n",
      "Epoch 117 , average loss 2.4208505153656006 , epoch_time 1.2213952541351318\n",
      "\n",
      "epoch->  118\n",
      "Iteration-> 0  , Iteration_time-> 1.2172 , Iteration_loss 2.4045\n",
      "Epoch 118 , average loss 2.4044675827026367 , epoch_time 1.2173678874969482\n",
      "\n",
      "epoch->  119\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 2.4010\n",
      "Epoch 119 , average loss 2.401026487350464 , epoch_time 1.2155845165252686\n",
      "\n",
      "epoch->  120\n",
      "Iteration-> 0  , Iteration_time-> 1.2098 , Iteration_loss 2.3931\n",
      "Epoch 120 , average loss 2.3930835723876953 , epoch_time 1.2100181579589844\n",
      "\n",
      "epoch->  121\n",
      "Iteration-> 0  , Iteration_time-> 1.2110 , Iteration_loss 2.3813\n",
      "Epoch 121 , average loss 2.3813014030456543 , epoch_time 1.211216926574707\n",
      "\n",
      "epoch->  122\n",
      "Iteration-> 0  , Iteration_time-> 1.2394 , Iteration_loss 2.3643\n",
      "Epoch 122 , average loss 2.3642702102661133 , epoch_time 1.239609718322754\n",
      "\n",
      "epoch->  123\n",
      "Iteration-> 0  , Iteration_time-> 1.2282 , Iteration_loss 2.3531\n",
      "Epoch 123 , average loss 2.3530502319335938 , epoch_time 1.228440523147583\n",
      "\n",
      "epoch->  124\n",
      "Iteration-> 0  , Iteration_time-> 1.2292 , Iteration_loss 2.3551\n",
      "Epoch 124 , average loss 2.3551175594329834 , epoch_time 1.229475498199463\n",
      "\n",
      "epoch->  125\n",
      "Iteration-> 0  , Iteration_time-> 1.2294 , Iteration_loss 2.3529\n",
      "Epoch 125 , average loss 2.352851390838623 , epoch_time 1.229576587677002\n",
      "\n",
      "epoch->  126\n",
      "Iteration-> 0  , Iteration_time-> 1.2437 , Iteration_loss 2.3367\n",
      "Epoch 126 , average loss 2.336731195449829 , epoch_time 1.243966817855835\n",
      "\n",
      "epoch->  127\n",
      "Iteration-> 0  , Iteration_time-> 1.2223 , Iteration_loss 2.3296\n",
      "Epoch 127 , average loss 2.329554557800293 , epoch_time 1.2225286960601807\n",
      "\n",
      "epoch->  128\n",
      "Iteration-> 0  , Iteration_time-> 1.2298 , Iteration_loss 2.3240\n",
      "Epoch 128 , average loss 2.3240435123443604 , epoch_time 1.2300465106964111\n",
      "\n",
      "epoch->  129\n",
      "Iteration-> 0  , Iteration_time-> 1.2163 , Iteration_loss 2.3116\n",
      "Epoch 129 , average loss 2.311612367630005 , epoch_time 1.2165277004241943\n",
      "\n",
      "epoch->  130\n",
      "Iteration-> 0  , Iteration_time-> 1.2072 , Iteration_loss 2.3066\n",
      "Epoch 130 , average loss 2.306567668914795 , epoch_time 1.207364797592163\n",
      "\n",
      "epoch->  131\n",
      "Iteration-> 0  , Iteration_time-> 1.2166 , Iteration_loss 2.2984\n",
      "Epoch 131 , average loss 2.2984421253204346 , epoch_time 1.2167754173278809\n",
      "\n",
      "epoch->  132\n",
      "Iteration-> 0  , Iteration_time-> 1.2149 , Iteration_loss 2.2790\n",
      "Epoch 132 , average loss 2.2790439128875732 , epoch_time 1.2154748439788818\n",
      "\n",
      "epoch->  133\n",
      "Iteration-> 0  , Iteration_time-> 1.2121 , Iteration_loss 2.2809\n",
      "Epoch 133 , average loss 2.2808799743652344 , epoch_time 1.2122783660888672\n",
      "\n",
      "epoch->  134\n",
      "Iteration-> 0  , Iteration_time-> 1.2137 , Iteration_loss 2.2760\n",
      "Epoch 134 , average loss 2.276029348373413 , epoch_time 1.2138879299163818\n",
      "\n",
      "epoch->  135\n",
      "Iteration-> 0  , Iteration_time-> 1.2136 , Iteration_loss 2.2580\n",
      "Epoch 135 , average loss 2.257982015609741 , epoch_time 1.2138400077819824\n",
      "\n",
      "epoch->  136\n",
      "Iteration-> 0  , Iteration_time-> 1.2254 , Iteration_loss 2.2588\n",
      "Epoch 136 , average loss 2.25877046585083 , epoch_time 1.2256660461425781\n",
      "\n",
      "epoch->  137\n",
      "Iteration-> 0  , Iteration_time-> 1.2324 , Iteration_loss 2.2554\n",
      "Epoch 137 , average loss 2.2554197311401367 , epoch_time 1.2325923442840576\n",
      "\n",
      "epoch->  138\n",
      "Iteration-> 0  , Iteration_time-> 1.2640 , Iteration_loss 2.2475\n",
      "Epoch 138 , average loss 2.2475149631500244 , epoch_time 1.2642457485198975\n",
      "\n",
      "epoch->  139\n",
      "Iteration-> 0  , Iteration_time-> 1.2214 , Iteration_loss 2.2312\n",
      "Epoch 139 , average loss 2.2312467098236084 , epoch_time 1.2215754985809326\n",
      "\n",
      "epoch->  140\n",
      "Iteration-> 0  , Iteration_time-> 1.2129 , Iteration_loss 2.2327\n",
      "Epoch 140 , average loss 2.23266863822937 , epoch_time 1.2131116390228271\n",
      "\n",
      "epoch->  141\n",
      "Iteration-> 0  , Iteration_time-> 1.2211 , Iteration_loss 2.2287\n",
      "Epoch 141 , average loss 2.2286951541900635 , epoch_time 1.2212998867034912\n",
      "\n",
      "epoch->  142\n",
      "Iteration-> 0  , Iteration_time-> 1.2219 , Iteration_loss 2.2199\n",
      "Epoch 142 , average loss 2.219851016998291 , epoch_time 1.2220754623413086\n",
      "\n",
      "epoch->  143\n",
      "Iteration-> 0  , Iteration_time-> 1.2161 , Iteration_loss 2.2076\n",
      "Epoch 143 , average loss 2.207552671432495 , epoch_time 1.2163102626800537\n",
      "\n",
      "epoch->  144\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 2.1926\n",
      "Epoch 144 , average loss 2.192625045776367 , epoch_time 1.2169029712677002\n",
      "\n",
      "epoch->  145\n",
      "Iteration-> 0  , Iteration_time-> 1.2138 , Iteration_loss 2.2017\n",
      "Epoch 145 , average loss 2.2017343044281006 , epoch_time 1.2139732837677002\n",
      "\n",
      "epoch->  146\n",
      "Iteration-> 0  , Iteration_time-> 1.2094 , Iteration_loss 2.1829\n",
      "Epoch 146 , average loss 2.1828815937042236 , epoch_time 1.2095866203308105\n",
      "\n",
      "epoch->  147\n",
      "Iteration-> 0  , Iteration_time-> 1.2082 , Iteration_loss 2.1866\n",
      "Epoch 147 , average loss 2.1866390705108643 , epoch_time 1.208409070968628\n",
      "\n",
      "epoch->  148\n",
      "Iteration-> 0  , Iteration_time-> 1.2210 , Iteration_loss 2.1730\n",
      "Epoch 148 , average loss 2.173022508621216 , epoch_time 1.2212269306182861\n",
      "\n",
      "epoch->  149\n",
      "Iteration-> 0  , Iteration_time-> 1.2119 , Iteration_loss 2.1817\n",
      "Epoch 149 , average loss 2.1816701889038086 , epoch_time 1.2121505737304688\n",
      "\n",
      "epoch->  150\n",
      "Iteration-> 0  , Iteration_time-> 1.2119 , Iteration_loss 2.1676\n",
      "Epoch 150 , average loss 2.1675944328308105 , epoch_time 1.2121672630310059\n",
      "\n",
      "epoch->  151\n",
      "Iteration-> 0  , Iteration_time-> 1.2176 , Iteration_loss 2.1637\n",
      "Epoch 151 , average loss 2.163651943206787 , epoch_time 1.2178034782409668\n",
      "\n",
      "epoch->  152\n",
      "Iteration-> 0  , Iteration_time-> 1.2225 , Iteration_loss 2.1612\n",
      "Epoch 152 , average loss 2.161186456680298 , epoch_time 1.222722053527832\n",
      "\n",
      "epoch->  153\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 2.1545\n",
      "Epoch 153 , average loss 2.1545186042785645 , epoch_time 1.2155802249908447\n",
      "\n",
      "epoch->  154\n",
      "Iteration-> 0  , Iteration_time-> 1.2068 , Iteration_loss 2.1451\n",
      "Epoch 154 , average loss 2.145054817199707 , epoch_time 1.2070438861846924\n",
      "\n",
      "epoch->  155\n",
      "Iteration-> 0  , Iteration_time-> 1.2097 , Iteration_loss 2.1456\n",
      "Epoch 155 , average loss 2.145634889602661 , epoch_time 1.2099800109863281\n",
      "\n",
      "epoch->  156\n",
      "Iteration-> 0  , Iteration_time-> 1.2108 , Iteration_loss 2.1364\n",
      "Epoch 156 , average loss 2.1364047527313232 , epoch_time 1.2110309600830078\n",
      "\n",
      "epoch->  157\n",
      "Iteration-> 0  , Iteration_time-> 1.2099 , Iteration_loss 2.1268\n",
      "Epoch 157 , average loss 2.1268296241760254 , epoch_time 1.2101311683654785\n",
      "\n",
      "epoch->  158\n",
      "Iteration-> 0  , Iteration_time-> 1.2051 , Iteration_loss 2.1262\n",
      "Epoch 158 , average loss 2.126166820526123 , epoch_time 1.2053287029266357\n",
      "\n",
      "epoch->  159\n",
      "Iteration-> 0  , Iteration_time-> 1.2142 , Iteration_loss 2.1220\n",
      "Epoch 159 , average loss 2.1219866275787354 , epoch_time 1.2144136428833008\n",
      "\n",
      "epoch->  160\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 2.1143\n",
      "Epoch 160 , average loss 2.1143417358398438 , epoch_time 1.211812973022461\n",
      "\n",
      "epoch->  161\n",
      "Iteration-> 0  , Iteration_time-> 1.2091 , Iteration_loss 2.1155\n",
      "Epoch 161 , average loss 2.115527629852295 , epoch_time 1.2093255519866943\n",
      "\n",
      "epoch->  162\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 2.1003\n",
      "Epoch 162 , average loss 2.10025691986084 , epoch_time 1.2075371742248535\n",
      "\n",
      "epoch->  163\n",
      "Iteration-> 0  , Iteration_time-> 1.2121 , Iteration_loss 2.0938\n",
      "Epoch 163 , average loss 2.093848705291748 , epoch_time 1.2123425006866455\n",
      "\n",
      "epoch->  164\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 2.0945\n",
      "Epoch 164 , average loss 2.0944929122924805 , epoch_time 1.211827278137207\n",
      "\n",
      "epoch->  165\n",
      "Iteration-> 0  , Iteration_time-> 1.2111 , Iteration_loss 2.0909\n",
      "Epoch 165 , average loss 2.090879440307617 , epoch_time 1.2113163471221924\n",
      "\n",
      "epoch->  166\n",
      "Iteration-> 0  , Iteration_time-> 1.2124 , Iteration_loss 2.0771\n",
      "Epoch 166 , average loss 2.0771453380584717 , epoch_time 1.212597370147705\n",
      "\n",
      "epoch->  167\n",
      "Iteration-> 0  , Iteration_time-> 1.2012 , Iteration_loss 2.0797\n",
      "Epoch 167 , average loss 2.079737424850464 , epoch_time 1.2014532089233398\n",
      "\n",
      "epoch->  168\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 2.0677\n",
      "Epoch 168 , average loss 2.0677196979522705 , epoch_time 1.2077374458312988\n",
      "\n",
      "epoch->  169\n",
      "Iteration-> 0  , Iteration_time-> 1.2416 , Iteration_loss 2.0697\n",
      "Epoch 169 , average loss 2.069669246673584 , epoch_time 1.2418558597564697\n",
      "\n",
      "epoch->  170\n",
      "Iteration-> 0  , Iteration_time-> 1.2345 , Iteration_loss 2.0662\n",
      "Epoch 170 , average loss 2.0661604404449463 , epoch_time 1.2347455024719238\n",
      "\n",
      "epoch->  171\n",
      "Iteration-> 0  , Iteration_time-> 1.2316 , Iteration_loss 2.0689\n",
      "Epoch 171 , average loss 2.068859338760376 , epoch_time 1.231769323348999\n",
      "\n",
      "epoch->  172\n",
      "Iteration-> 0  , Iteration_time-> 1.2321 , Iteration_loss 2.0616\n",
      "Epoch 172 , average loss 2.061624765396118 , epoch_time 1.2323169708251953\n",
      "\n",
      "epoch->  173\n",
      "Iteration-> 0  , Iteration_time-> 1.2235 , Iteration_loss 2.0528\n",
      "Epoch 173 , average loss 2.0528080463409424 , epoch_time 1.223750352859497\n",
      "\n",
      "epoch->  174\n",
      "Iteration-> 0  , Iteration_time-> 1.2351 , Iteration_loss 2.0564\n",
      "Epoch 174 , average loss 2.056428909301758 , epoch_time 1.2352547645568848\n",
      "\n",
      "epoch->  175\n",
      "Iteration-> 0  , Iteration_time-> 1.2141 , Iteration_loss 2.0537\n",
      "Epoch 175 , average loss 2.053687572479248 , epoch_time 1.2142822742462158\n",
      "\n",
      "epoch->  176\n",
      "Iteration-> 0  , Iteration_time-> 1.2210 , Iteration_loss 2.0411\n",
      "Epoch 176 , average loss 2.0411362648010254 , epoch_time 1.221261739730835\n",
      "\n",
      "epoch->  177\n",
      "Iteration-> 0  , Iteration_time-> 1.2095 , Iteration_loss 2.0310\n",
      "Epoch 177 , average loss 2.0309979915618896 , epoch_time 1.2097389698028564\n",
      "\n",
      "epoch->  178\n",
      "Iteration-> 0  , Iteration_time-> 1.2381 , Iteration_loss 2.0330\n",
      "Epoch 178 , average loss 2.03300142288208 , epoch_time 1.2383170127868652\n",
      "\n",
      "epoch->  179\n",
      "Iteration-> 0  , Iteration_time-> 1.2259 , Iteration_loss 2.0330\n",
      "Epoch 179 , average loss 2.0330443382263184 , epoch_time 1.226142406463623\n",
      "\n",
      "epoch->  180\n",
      "Iteration-> 0  , Iteration_time-> 1.2323 , Iteration_loss 2.0284\n",
      "Epoch 180 , average loss 2.0283663272857666 , epoch_time 1.2333729267120361\n",
      "\n",
      "epoch->  181\n",
      "Iteration-> 0  , Iteration_time-> 1.2146 , Iteration_loss 2.0226\n",
      "Epoch 181 , average loss 2.022571086883545 , epoch_time 1.2148568630218506\n",
      "\n",
      "epoch->  182\n",
      "Iteration-> 0  , Iteration_time-> 1.2118 , Iteration_loss 2.0144\n",
      "Epoch 182 , average loss 2.0144412517547607 , epoch_time 1.2121169567108154\n",
      "\n",
      "epoch->  183\n",
      "Iteration-> 0  , Iteration_time-> 1.2378 , Iteration_loss 2.0207\n",
      "Epoch 183 , average loss 2.0206587314605713 , epoch_time 1.238037109375\n",
      "\n",
      "epoch->  184\n",
      "Iteration-> 0  , Iteration_time-> 1.2286 , Iteration_loss 2.0103\n",
      "Epoch 184 , average loss 2.010310173034668 , epoch_time 1.228832483291626\n",
      "\n",
      "epoch->  185\n",
      "Iteration-> 0  , Iteration_time-> 1.2388 , Iteration_loss 2.0073\n",
      "Epoch 185 , average loss 2.0072784423828125 , epoch_time 1.239046573638916\n",
      "\n",
      "epoch->  186\n",
      "Iteration-> 0  , Iteration_time-> 1.2263 , Iteration_loss 2.0093\n",
      "Epoch 186 , average loss 2.009276866912842 , epoch_time 1.2266027927398682\n",
      "\n",
      "epoch->  187\n",
      "Iteration-> 0  , Iteration_time-> 1.2326 , Iteration_loss 2.0034\n",
      "Epoch 187 , average loss 2.003354072570801 , epoch_time 1.232846975326538\n",
      "\n",
      "epoch->  188\n",
      "Iteration-> 0  , Iteration_time-> 1.2247 , Iteration_loss 2.0004\n",
      "Epoch 188 , average loss 2.0003697872161865 , epoch_time 1.224905252456665\n",
      "\n",
      "epoch->  189\n",
      "Iteration-> 0  , Iteration_time-> 1.2378 , Iteration_loss 1.9934\n",
      "Epoch 189 , average loss 1.9934498071670532 , epoch_time 1.237999439239502\n",
      "\n",
      "epoch->  190\n",
      "Iteration-> 0  , Iteration_time-> 1.2179 , Iteration_loss 1.9877\n",
      "Epoch 190 , average loss 1.9876950979232788 , epoch_time 1.218200445175171\n",
      "\n",
      "epoch->  191\n",
      "Iteration-> 0  , Iteration_time-> 1.2205 , Iteration_loss 1.9847\n",
      "Epoch 191 , average loss 1.9846519231796265 , epoch_time 1.2207696437835693\n",
      "\n",
      "epoch->  192\n",
      "Iteration-> 0  , Iteration_time-> 1.2112 , Iteration_loss 1.9834\n",
      "Epoch 192 , average loss 1.9833742380142212 , epoch_time 1.2114160060882568\n",
      "\n",
      "epoch->  193\n",
      "Iteration-> 0  , Iteration_time-> 1.2298 , Iteration_loss 1.9685\n",
      "Epoch 193 , average loss 1.9685293436050415 , epoch_time 1.229954719543457\n",
      "\n",
      "epoch->  194\n",
      "Iteration-> 0  , Iteration_time-> 1.2168 , Iteration_loss 1.9722\n",
      "Epoch 194 , average loss 1.9722496271133423 , epoch_time 1.216982126235962\n",
      "\n",
      "epoch->  195\n",
      "Iteration-> 0  , Iteration_time-> 1.2168 , Iteration_loss 1.9714\n",
      "Epoch 195 , average loss 1.9714244604110718 , epoch_time 1.216975212097168\n",
      "\n",
      "epoch->  196\n",
      "Iteration-> 0  , Iteration_time-> 1.2414 , Iteration_loss 1.9723\n",
      "Epoch 196 , average loss 1.9722626209259033 , epoch_time 1.24159836769104\n",
      "\n",
      "epoch->  197\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.9731\n",
      "Epoch 197 , average loss 1.9730772972106934 , epoch_time 1.2049264907836914\n",
      "\n",
      "epoch->  198\n",
      "Iteration-> 0  , Iteration_time-> 1.2151 , Iteration_loss 1.9644\n",
      "Epoch 198 , average loss 1.9644172191619873 , epoch_time 1.21527099609375\n",
      "\n",
      "epoch->  199\n",
      "Iteration-> 0  , Iteration_time-> 1.2127 , Iteration_loss 1.9637\n",
      "Epoch 199 , average loss 1.9637272357940674 , epoch_time 1.2129089832305908\n",
      "\n",
      "epoch->  200\n",
      "Iteration-> 0  , Iteration_time-> 1.2074 , Iteration_loss 1.9512\n",
      "Epoch 200 , average loss 1.9512380361557007 , epoch_time 1.2076163291931152\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  201\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 1.9537\n",
      "Epoch 201 , average loss 1.9537088871002197 , epoch_time 1.2118184566497803\n",
      "\n",
      "epoch->  202\n",
      "Iteration-> 0  , Iteration_time-> 1.2104 , Iteration_loss 1.9468\n",
      "Epoch 202 , average loss 1.9467679262161255 , epoch_time 1.2106034755706787\n",
      "\n",
      "epoch->  203\n",
      "Iteration-> 0  , Iteration_time-> 1.2095 , Iteration_loss 1.9385\n",
      "Epoch 203 , average loss 1.9384618997573853 , epoch_time 1.2097322940826416\n",
      "\n",
      "epoch->  204\n",
      "Iteration-> 0  , Iteration_time-> 1.2135 , Iteration_loss 1.9459\n",
      "Epoch 204 , average loss 1.9459148645401 , epoch_time 1.2136752605438232\n",
      "\n",
      "epoch->  205\n",
      "Iteration-> 0  , Iteration_time-> 1.2048 , Iteration_loss 1.9449\n",
      "Epoch 205 , average loss 1.9448708295822144 , epoch_time 1.2049942016601562\n",
      "\n",
      "epoch->  206\n",
      "Iteration-> 0  , Iteration_time-> 1.2202 , Iteration_loss 1.9408\n",
      "Epoch 206 , average loss 1.9407950639724731 , epoch_time 1.2203984260559082\n",
      "\n",
      "epoch->  207\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.9417\n",
      "Epoch 207 , average loss 1.9416526556015015 , epoch_time 1.211625099182129\n",
      "\n",
      "epoch->  208\n",
      "Iteration-> 0  , Iteration_time-> 1.2215 , Iteration_loss 1.9326\n",
      "Epoch 208 , average loss 1.9325833320617676 , epoch_time 1.2217390537261963\n",
      "\n",
      "epoch->  209\n",
      "Iteration-> 0  , Iteration_time-> 1.2161 , Iteration_loss 1.9323\n",
      "Epoch 209 , average loss 1.9323289394378662 , epoch_time 1.2163431644439697\n",
      "\n",
      "epoch->  210\n",
      "Iteration-> 0  , Iteration_time-> 1.2261 , Iteration_loss 1.9247\n",
      "Epoch 210 , average loss 1.9246978759765625 , epoch_time 1.226306676864624\n",
      "\n",
      "epoch->  211\n",
      "Iteration-> 0  , Iteration_time-> 1.2157 , Iteration_loss 1.9231\n",
      "Epoch 211 , average loss 1.9231419563293457 , epoch_time 1.215888500213623\n",
      "\n",
      "epoch->  212\n",
      "Iteration-> 0  , Iteration_time-> 1.2254 , Iteration_loss 1.9266\n",
      "Epoch 212 , average loss 1.9266126155853271 , epoch_time 1.225581407546997\n",
      "\n",
      "epoch->  213\n",
      "Iteration-> 0  , Iteration_time-> 1.2183 , Iteration_loss 1.9166\n",
      "Epoch 213 , average loss 1.916593074798584 , epoch_time 1.2185347080230713\n",
      "\n",
      "epoch->  214\n",
      "Iteration-> 0  , Iteration_time-> 1.2131 , Iteration_loss 1.9146\n",
      "Epoch 214 , average loss 1.9145519733428955 , epoch_time 1.2133164405822754\n",
      "\n",
      "epoch->  215\n",
      "Iteration-> 0  , Iteration_time-> 1.2139 , Iteration_loss 1.9138\n",
      "Epoch 215 , average loss 1.91383695602417 , epoch_time 1.2141499519348145\n",
      "\n",
      "epoch->  216\n",
      "Iteration-> 0  , Iteration_time-> 1.2212 , Iteration_loss 1.9108\n",
      "Epoch 216 , average loss 1.910778522491455 , epoch_time 1.2213943004608154\n",
      "\n",
      "epoch->  217\n",
      "Iteration-> 0  , Iteration_time-> 1.2183 , Iteration_loss 1.9166\n",
      "Epoch 217 , average loss 1.916623830795288 , epoch_time 1.2184770107269287\n",
      "\n",
      "epoch->  218\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.9094\n",
      "Epoch 218 , average loss 1.9093942642211914 , epoch_time 1.2049109935760498\n",
      "\n",
      "epoch->  219\n",
      "Iteration-> 0  , Iteration_time-> 1.2100 , Iteration_loss 1.9012\n",
      "Epoch 219 , average loss 1.9012075662612915 , epoch_time 1.210235834121704\n",
      "\n",
      "epoch->  220\n",
      "Iteration-> 0  , Iteration_time-> 1.2080 , Iteration_loss 1.8978\n",
      "Epoch 220 , average loss 1.897786259651184 , epoch_time 1.2082412242889404\n",
      "\n",
      "epoch->  221\n",
      "Iteration-> 0  , Iteration_time-> 1.2092 , Iteration_loss 1.8951\n",
      "Epoch 221 , average loss 1.895054578781128 , epoch_time 1.2093501091003418\n",
      "\n",
      "epoch->  222\n",
      "Iteration-> 0  , Iteration_time-> 1.2241 , Iteration_loss 1.8904\n",
      "Epoch 222 , average loss 1.890353798866272 , epoch_time 1.2243471145629883\n",
      "\n",
      "epoch->  223\n",
      "Iteration-> 0  , Iteration_time-> 1.2100 , Iteration_loss 1.8941\n",
      "Epoch 223 , average loss 1.8941174745559692 , epoch_time 1.210216999053955\n",
      "\n",
      "epoch->  224\n",
      "Iteration-> 0  , Iteration_time-> 1.2053 , Iteration_loss 1.8851\n",
      "Epoch 224 , average loss 1.8851197957992554 , epoch_time 1.2054708003997803\n",
      "\n",
      "epoch->  225\n",
      "Iteration-> 0  , Iteration_time-> 1.2363 , Iteration_loss 1.8908\n",
      "Epoch 225 , average loss 1.8908436298370361 , epoch_time 1.2365386486053467\n",
      "\n",
      "epoch->  226\n",
      "Iteration-> 0  , Iteration_time-> 1.2123 , Iteration_loss 1.8785\n",
      "Epoch 226 , average loss 1.8785083293914795 , epoch_time 1.2125215530395508\n",
      "\n",
      "epoch->  227\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 1.8801\n",
      "Epoch 227 , average loss 1.8801145553588867 , epoch_time 1.2075285911560059\n",
      "\n",
      "epoch->  228\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 1.8741\n",
      "Epoch 228 , average loss 1.8740766048431396 , epoch_time 1.2108206748962402\n",
      "\n",
      "epoch->  229\n",
      "Iteration-> 0  , Iteration_time-> 1.2318 , Iteration_loss 1.8723\n",
      "Epoch 229 , average loss 1.8723481893539429 , epoch_time 1.232018232345581\n",
      "\n",
      "epoch->  230\n",
      "Iteration-> 0  , Iteration_time-> 1.2188 , Iteration_loss 1.8704\n",
      "Epoch 230 , average loss 1.8704311847686768 , epoch_time 1.218984603881836\n",
      "\n",
      "epoch->  231\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 1.8661\n",
      "Epoch 231 , average loss 1.8660880327224731 , epoch_time 1.2175631523132324\n",
      "\n",
      "epoch->  232\n",
      "Iteration-> 0  , Iteration_time-> 1.2211 , Iteration_loss 1.8650\n",
      "Epoch 232 , average loss 1.8649543523788452 , epoch_time 1.2213187217712402\n",
      "\n",
      "epoch->  233\n",
      "Iteration-> 0  , Iteration_time-> 1.2040 , Iteration_loss 1.8561\n",
      "Epoch 233 , average loss 1.8560765981674194 , epoch_time 1.2042157649993896\n",
      "\n",
      "epoch->  234\n",
      "Iteration-> 0  , Iteration_time-> 1.2063 , Iteration_loss 1.8681\n",
      "Epoch 234 , average loss 1.868112325668335 , epoch_time 1.2065215110778809\n",
      "\n",
      "epoch->  235\n",
      "Iteration-> 0  , Iteration_time-> 1.2098 , Iteration_loss 1.8520\n",
      "Epoch 235 , average loss 1.8519681692123413 , epoch_time 1.2099952697753906\n",
      "\n",
      "epoch->  236\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 1.8529\n",
      "Epoch 236 , average loss 1.8529497385025024 , epoch_time 1.2077527046203613\n",
      "\n",
      "epoch->  237\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.8522\n",
      "Epoch 237 , average loss 1.852221131324768 , epoch_time 1.2048859596252441\n",
      "\n",
      "epoch->  238\n",
      "Iteration-> 0  , Iteration_time-> 1.2132 , Iteration_loss 1.8521\n",
      "Epoch 238 , average loss 1.852052927017212 , epoch_time 1.2133841514587402\n",
      "\n",
      "epoch->  239\n",
      "Iteration-> 0  , Iteration_time-> 1.2156 , Iteration_loss 1.8479\n",
      "Epoch 239 , average loss 1.8478895425796509 , epoch_time 1.2157702445983887\n",
      "\n",
      "epoch->  240\n",
      "Iteration-> 0  , Iteration_time-> 1.2044 , Iteration_loss 1.8449\n",
      "Epoch 240 , average loss 1.8448950052261353 , epoch_time 1.2045996189117432\n",
      "\n",
      "epoch->  241\n",
      "Iteration-> 0  , Iteration_time-> 1.2156 , Iteration_loss 1.8397\n",
      "Epoch 241 , average loss 1.8396823406219482 , epoch_time 1.2158689498901367\n",
      "\n",
      "epoch->  242\n",
      "Iteration-> 0  , Iteration_time-> 1.2129 , Iteration_loss 1.8400\n",
      "Epoch 242 , average loss 1.840009093284607 , epoch_time 1.2133634090423584\n",
      "\n",
      "epoch->  243\n",
      "Iteration-> 0  , Iteration_time-> 1.2184 , Iteration_loss 1.8372\n",
      "Epoch 243 , average loss 1.8372241258621216 , epoch_time 1.2186393737792969\n",
      "\n",
      "epoch->  244\n",
      "Iteration-> 0  , Iteration_time-> 1.2103 , Iteration_loss 1.8438\n",
      "Epoch 244 , average loss 1.8438050746917725 , epoch_time 1.2105138301849365\n",
      "\n",
      "epoch->  245\n",
      "Iteration-> 0  , Iteration_time-> 1.2181 , Iteration_loss 1.8301\n",
      "Epoch 245 , average loss 1.8300681114196777 , epoch_time 1.218273401260376\n",
      "\n",
      "epoch->  246\n",
      "Iteration-> 0  , Iteration_time-> 1.2201 , Iteration_loss 1.8294\n",
      "Epoch 246 , average loss 1.8294495344161987 , epoch_time 1.2202684879302979\n",
      "\n",
      "epoch->  247\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 1.8251\n",
      "Epoch 247 , average loss 1.8251005411148071 , epoch_time 1.2155625820159912\n",
      "\n",
      "epoch->  248\n",
      "Iteration-> 0  , Iteration_time-> 1.2220 , Iteration_loss 1.8266\n",
      "Epoch 248 , average loss 1.8266199827194214 , epoch_time 1.2222237586975098\n",
      "\n",
      "epoch->  249\n",
      "Iteration-> 0  , Iteration_time-> 1.2162 , Iteration_loss 1.8188\n",
      "Epoch 249 , average loss 1.8187510967254639 , epoch_time 1.21641206741333\n",
      "\n",
      "epoch->  250\n",
      "Iteration-> 0  , Iteration_time-> 1.2126 , Iteration_loss 1.8226\n",
      "Epoch 250 , average loss 1.822636365890503 , epoch_time 1.2128486633300781\n",
      "\n",
      "epoch->  251\n",
      "Iteration-> 0  , Iteration_time-> 1.2145 , Iteration_loss 1.8269\n",
      "Epoch 251 , average loss 1.8268706798553467 , epoch_time 1.2146849632263184\n",
      "\n",
      "epoch->  252\n",
      "Iteration-> 0  , Iteration_time-> 1.2049 , Iteration_loss 1.8125\n",
      "Epoch 252 , average loss 1.8124624490737915 , epoch_time 1.205547571182251\n",
      "\n",
      "epoch->  253\n",
      "Iteration-> 0  , Iteration_time-> 1.2052 , Iteration_loss 1.8168\n",
      "Epoch 253 , average loss 1.8168174028396606 , epoch_time 1.205397367477417\n",
      "\n",
      "epoch->  254\n",
      "Iteration-> 0  , Iteration_time-> 1.2137 , Iteration_loss 1.8088\n",
      "Epoch 254 , average loss 1.8088300228118896 , epoch_time 1.2139246463775635\n",
      "\n",
      "epoch->  255\n",
      "Iteration-> 0  , Iteration_time-> 1.2145 , Iteration_loss 1.8079\n",
      "Epoch 255 , average loss 1.8079065084457397 , epoch_time 1.2147433757781982\n",
      "\n",
      "epoch->  256\n",
      "Iteration-> 0  , Iteration_time-> 1.2094 , Iteration_loss 1.8018\n",
      "Epoch 256 , average loss 1.8017765283584595 , epoch_time 1.2095625400543213\n",
      "\n",
      "epoch->  257\n",
      "Iteration-> 0  , Iteration_time-> 1.2154 , Iteration_loss 1.8064\n",
      "Epoch 257 , average loss 1.8063865900039673 , epoch_time 1.2155802249908447\n",
      "\n",
      "epoch->  258\n",
      "Iteration-> 0  , Iteration_time-> 1.2174 , Iteration_loss 1.8052\n",
      "Epoch 258 , average loss 1.8051800727844238 , epoch_time 1.2176165580749512\n",
      "\n",
      "epoch->  259\n",
      "Iteration-> 0  , Iteration_time-> 1.2055 , Iteration_loss 1.8004\n",
      "Epoch 259 , average loss 1.8004488945007324 , epoch_time 1.2056560516357422\n",
      "\n",
      "epoch->  260\n",
      "Iteration-> 0  , Iteration_time-> 1.2171 , Iteration_loss 1.7867\n",
      "Epoch 260 , average loss 1.7867215871810913 , epoch_time 1.2172894477844238\n",
      "\n",
      "epoch->  261\n",
      "Iteration-> 0  , Iteration_time-> 1.2066 , Iteration_loss 1.7955\n",
      "Epoch 261 , average loss 1.7954986095428467 , epoch_time 1.2068164348602295\n",
      "\n",
      "epoch->  262\n",
      "Iteration-> 0  , Iteration_time-> 1.2187 , Iteration_loss 1.7909\n",
      "Epoch 262 , average loss 1.7909480333328247 , epoch_time 1.2189357280731201\n",
      "\n",
      "epoch->  263\n",
      "Iteration-> 0  , Iteration_time-> 1.2133 , Iteration_loss 1.7839\n",
      "Epoch 263 , average loss 1.7839341163635254 , epoch_time 1.2135131359100342\n",
      "\n",
      "epoch->  264\n",
      "Iteration-> 0  , Iteration_time-> 1.2101 , Iteration_loss 1.7873\n",
      "Epoch 264 , average loss 1.7872707843780518 , epoch_time 1.2102556228637695\n",
      "\n",
      "epoch->  265\n",
      "Iteration-> 0  , Iteration_time-> 1.2170 , Iteration_loss 1.7914\n",
      "Epoch 265 , average loss 1.791441559791565 , epoch_time 1.2172777652740479\n",
      "\n",
      "epoch->  266\n",
      "Iteration-> 0  , Iteration_time-> 1.2189 , Iteration_loss 1.7850\n",
      "Epoch 266 , average loss 1.784998893737793 , epoch_time 1.2191245555877686\n",
      "\n",
      "epoch->  267\n",
      "Iteration-> 0  , Iteration_time-> 1.2096 , Iteration_loss 1.7797\n",
      "Epoch 267 , average loss 1.779678225517273 , epoch_time 1.2098262310028076\n",
      "\n",
      "epoch->  268\n",
      "Iteration-> 0  , Iteration_time-> 1.2150 , Iteration_loss 1.7785\n",
      "Epoch 268 , average loss 1.77845299243927 , epoch_time 1.2152307033538818\n",
      "\n",
      "epoch->  269\n",
      "Iteration-> 0  , Iteration_time-> 1.2116 , Iteration_loss 1.7820\n",
      "Epoch 269 , average loss 1.7819722890853882 , epoch_time 1.211871862411499\n",
      "\n",
      "epoch->  270\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 1.7632\n",
      "Epoch 270 , average loss 1.76324462890625 , epoch_time 1.2168653011322021\n",
      "\n",
      "epoch->  271\n",
      "Iteration-> 0  , Iteration_time-> 1.2137 , Iteration_loss 1.7736\n",
      "Epoch 271 , average loss 1.773577094078064 , epoch_time 1.2138855457305908\n",
      "\n",
      "epoch->  272\n",
      "Iteration-> 0  , Iteration_time-> 1.2191 , Iteration_loss 1.7680\n",
      "Epoch 272 , average loss 1.7679654359817505 , epoch_time 1.2193853855133057\n",
      "\n",
      "epoch->  273\n",
      "Iteration-> 0  , Iteration_time-> 1.2180 , Iteration_loss 1.7686\n",
      "Epoch 273 , average loss 1.7685803174972534 , epoch_time 1.2182223796844482\n",
      "\n",
      "epoch->  274\n",
      "Iteration-> 0  , Iteration_time-> 1.2193 , Iteration_loss 1.7662\n",
      "Epoch 274 , average loss 1.7661901712417603 , epoch_time 1.2195179462432861\n",
      "\n",
      "epoch->  275\n",
      "Iteration-> 0  , Iteration_time-> 1.2216 , Iteration_loss 1.7577\n",
      "Epoch 275 , average loss 1.7577003240585327 , epoch_time 1.221834421157837\n",
      "\n",
      "epoch->  276\n",
      "Iteration-> 0  , Iteration_time-> 1.2201 , Iteration_loss 1.7547\n",
      "Epoch 276 , average loss 1.7547115087509155 , epoch_time 1.220346450805664\n",
      "\n",
      "epoch->  277\n",
      "Iteration-> 0  , Iteration_time-> 1.2051 , Iteration_loss 1.7570\n",
      "Epoch 277 , average loss 1.7570048570632935 , epoch_time 1.2053122520446777\n",
      "\n",
      "epoch->  278\n",
      "Iteration-> 0  , Iteration_time-> 1.2185 , Iteration_loss 1.7563\n",
      "Epoch 278 , average loss 1.7563050985336304 , epoch_time 1.2187035083770752\n",
      "\n",
      "epoch->  279\n",
      "Iteration-> 0  , Iteration_time-> 1.2138 , Iteration_loss 1.7453\n",
      "Epoch 279 , average loss 1.7452564239501953 , epoch_time 1.2140398025512695\n",
      "\n",
      "epoch->  280\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.7483\n",
      "Epoch 280 , average loss 1.7482718229293823 , epoch_time 1.2049081325531006\n",
      "\n",
      "epoch->  281\n",
      "Iteration-> 0  , Iteration_time-> 1.2184 , Iteration_loss 1.7475\n",
      "Epoch 281 , average loss 1.7474614381790161 , epoch_time 1.2186672687530518\n",
      "\n",
      "epoch->  282\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.7475\n",
      "Epoch 282 , average loss 1.7475420236587524 , epoch_time 1.2103965282440186\n",
      "\n",
      "epoch->  283\n",
      "Iteration-> 0  , Iteration_time-> 1.2044 , Iteration_loss 1.7486\n",
      "Epoch 283 , average loss 1.748584270477295 , epoch_time 1.2046236991882324\n",
      "\n",
      "epoch->  284\n",
      "Iteration-> 0  , Iteration_time-> 1.2041 , Iteration_loss 1.7414\n",
      "Epoch 284 , average loss 1.741400122642517 , epoch_time 1.2043256759643555\n",
      "\n",
      "epoch->  285\n",
      "Iteration-> 0  , Iteration_time-> 1.2096 , Iteration_loss 1.7441\n",
      "Epoch 285 , average loss 1.7440890073776245 , epoch_time 1.2098002433776855\n",
      "\n",
      "epoch->  286\n",
      "Iteration-> 0  , Iteration_time-> 1.2036 , Iteration_loss 1.7378\n",
      "Epoch 286 , average loss 1.7377902269363403 , epoch_time 1.2038016319274902\n",
      "\n",
      "epoch->  287\n",
      "Iteration-> 0  , Iteration_time-> 1.2069 , Iteration_loss 1.7387\n",
      "Epoch 287 , average loss 1.7386870384216309 , epoch_time 1.2071452140808105\n",
      "\n",
      "epoch->  288\n",
      "Iteration-> 0  , Iteration_time-> 1.2159 , Iteration_loss 1.7300\n",
      "Epoch 288 , average loss 1.7299944162368774 , epoch_time 1.216076135635376\n",
      "\n",
      "epoch->  289\n",
      "Iteration-> 0  , Iteration_time-> 1.2059 , Iteration_loss 1.7350\n",
      "Epoch 289 , average loss 1.7350093126296997 , epoch_time 1.2060577869415283\n",
      "\n",
      "epoch->  290\n",
      "Iteration-> 0  , Iteration_time-> 1.2115 , Iteration_loss 1.7365\n",
      "Epoch 290 , average loss 1.736507773399353 , epoch_time 1.2116882801055908\n",
      "\n",
      "epoch->  291\n",
      "Iteration-> 0  , Iteration_time-> 1.2120 , Iteration_loss 1.7239\n",
      "Epoch 291 , average loss 1.723869800567627 , epoch_time 1.2121822834014893\n",
      "\n",
      "epoch->  292\n",
      "Iteration-> 0  , Iteration_time-> 1.2109 , Iteration_loss 1.7213\n",
      "Epoch 292 , average loss 1.7212570905685425 , epoch_time 1.2111396789550781\n",
      "\n",
      "epoch->  293\n",
      "Iteration-> 0  , Iteration_time-> 1.2079 , Iteration_loss 1.7235\n",
      "Epoch 293 , average loss 1.7234967947006226 , epoch_time 1.2081520557403564\n",
      "\n",
      "epoch->  294\n",
      "Iteration-> 0  , Iteration_time-> 1.2147 , Iteration_loss 1.7216\n",
      "Epoch 294 , average loss 1.7216473817825317 , epoch_time 1.2148945331573486\n",
      "\n",
      "epoch->  295\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.7064\n",
      "Epoch 295 , average loss 1.7063572406768799 , epoch_time 1.211594820022583\n",
      "\n",
      "epoch->  296\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.7065\n",
      "Epoch 296 , average loss 1.7065116167068481 , epoch_time 1.210383415222168\n",
      "\n",
      "epoch->  297\n",
      "Iteration-> 0  , Iteration_time-> 1.2036 , Iteration_loss 1.7109\n",
      "Epoch 297 , average loss 1.7109143733978271 , epoch_time 1.2037675380706787\n",
      "\n",
      "epoch->  298\n",
      "Iteration-> 0  , Iteration_time-> 1.2104 , Iteration_loss 1.7129\n",
      "Epoch 298 , average loss 1.7128961086273193 , epoch_time 1.2105612754821777\n",
      "\n",
      "epoch->  299\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.7019\n",
      "Epoch 299 , average loss 1.7018747329711914 , epoch_time 1.2115528583526611\n",
      "\n",
      "epoch->  300\n",
      "Iteration-> 0  , Iteration_time-> 1.2093 , Iteration_loss 1.7023\n",
      "Epoch 300 , average loss 1.7023006677627563 , epoch_time 1.2095379829406738\n",
      "Saving Model\n",
      "Done saving Model\n",
      "\n",
      "epoch->  301\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.7000\n",
      "Epoch 301 , average loss 1.7000164985656738 , epoch_time 1.2103502750396729\n",
      "\n",
      "epoch->  302\n",
      "Iteration-> 0  , Iteration_time-> 1.2164 , Iteration_loss 1.6963\n",
      "Epoch 302 , average loss 1.6963437795639038 , epoch_time 1.2165956497192383\n",
      "\n",
      "epoch->  303\n",
      "Iteration-> 0  , Iteration_time-> 1.2034 , Iteration_loss 1.7048\n",
      "Epoch 303 , average loss 1.7048425674438477 , epoch_time 1.2038750648498535\n",
      "\n",
      "epoch->  304\n",
      "Iteration-> 0  , Iteration_time-> 1.2107 , Iteration_loss 1.6918\n",
      "Epoch 304 , average loss 1.6918352842330933 , epoch_time 1.2108688354492188\n",
      "\n",
      "epoch->  305\n",
      "Iteration-> 0  , Iteration_time-> 1.2144 , Iteration_loss 1.6952\n",
      "Epoch 305 , average loss 1.695172905921936 , epoch_time 1.2146148681640625\n",
      "\n",
      "epoch->  306\n",
      "Iteration-> 0  , Iteration_time-> 1.2123 , Iteration_loss 1.6908\n",
      "Epoch 306 , average loss 1.6907801628112793 , epoch_time 1.212533950805664\n",
      "\n",
      "epoch->  307\n",
      "Iteration-> 0  , Iteration_time-> 1.2155 , Iteration_loss 1.6874\n",
      "Epoch 307 , average loss 1.6873618364334106 , epoch_time 1.2156586647033691\n",
      "\n",
      "epoch->  308\n",
      "Iteration-> 0  , Iteration_time-> 1.2112 , Iteration_loss 1.6771\n",
      "Epoch 308 , average loss 1.6771472692489624 , epoch_time 1.2113840579986572\n",
      "\n",
      "epoch->  309\n",
      "Iteration-> 0  , Iteration_time-> 1.2087 , Iteration_loss 1.6851\n",
      "Epoch 309 , average loss 1.6851152181625366 , epoch_time 1.2089533805847168\n",
      "\n",
      "epoch->  310\n",
      "Iteration-> 0  , Iteration_time-> 1.2164 , Iteration_loss 1.6753\n",
      "Epoch 310 , average loss 1.67534601688385 , epoch_time 1.216588020324707\n",
      "\n",
      "epoch->  311\n",
      "Iteration-> 0  , Iteration_time-> 1.2306 , Iteration_loss 1.6721\n",
      "Epoch 311 , average loss 1.6721091270446777 , epoch_time 1.2308835983276367\n",
      "\n",
      "epoch->  312\n",
      "Iteration-> 0  , Iteration_time-> 1.2126 , Iteration_loss 1.6732\n",
      "Epoch 312 , average loss 1.6731617450714111 , epoch_time 1.2127957344055176\n",
      "\n",
      "epoch->  313\n",
      "Iteration-> 0  , Iteration_time-> 1.2346 , Iteration_loss 1.6742\n",
      "Epoch 313 , average loss 1.6742061376571655 , epoch_time 1.2347767353057861\n",
      "\n",
      "epoch->  314\n",
      "Iteration-> 0  , Iteration_time-> 1.2146 , Iteration_loss 1.6732\n",
      "Epoch 314 , average loss 1.6732341051101685 , epoch_time 1.2151579856872559\n",
      "\n",
      "epoch->  315\n",
      "Iteration-> 0  , Iteration_time-> 1.2143 , Iteration_loss 1.6701\n",
      "Epoch 315 , average loss 1.6700859069824219 , epoch_time 1.214526653289795\n",
      "\n",
      "epoch->  316\n",
      "Iteration-> 0  , Iteration_time-> 1.2172 , Iteration_loss 1.6596\n",
      "Epoch 316 , average loss 1.6595771312713623 , epoch_time 1.2174592018127441\n",
      "\n",
      "epoch->  317\n",
      "Iteration-> 0  , Iteration_time-> 1.2056 , Iteration_loss 1.6700\n",
      "Epoch 317 , average loss 1.6699531078338623 , epoch_time 1.2058544158935547\n",
      "\n",
      "epoch->  318\n",
      "Iteration-> 0  , Iteration_time-> 1.2098 , Iteration_loss 1.6602\n",
      "Epoch 318 , average loss 1.6601837873458862 , epoch_time 1.2100088596343994\n",
      "\n",
      "epoch->  319\n",
      "Iteration-> 0  , Iteration_time-> 1.2101 , Iteration_loss 1.6573\n",
      "Epoch 319 , average loss 1.6572809219360352 , epoch_time 1.2102758884429932\n",
      "\n",
      "epoch->  320\n",
      "Iteration-> 0  , Iteration_time-> 1.2315 , Iteration_loss 1.6547\n",
      "Epoch 320 , average loss 1.6546777486801147 , epoch_time 1.2317025661468506\n",
      "\n",
      "epoch->  321\n",
      "Iteration-> 0  , Iteration_time-> 1.2114 , Iteration_loss 1.6517\n",
      "Epoch 321 , average loss 1.6516716480255127 , epoch_time 1.2115592956542969\n",
      "\n",
      "epoch->  322\n",
      "Iteration-> 0  , Iteration_time-> 1.2003 , Iteration_loss 1.6493\n",
      "Epoch 322 , average loss 1.64930260181427 , epoch_time 1.2005460262298584\n",
      "\n",
      "epoch->  323\n",
      "Iteration-> 0  , Iteration_time-> 1.1981 , Iteration_loss 1.6494\n",
      "Epoch 323 , average loss 1.6493613719940186 , epoch_time 1.198324203491211\n",
      "\n",
      "epoch->  324\n",
      "Iteration-> 0  , Iteration_time-> 1.2029 , Iteration_loss 1.6454\n",
      "Epoch 324 , average loss 1.6453561782836914 , epoch_time 1.203141212463379\n",
      "\n",
      "epoch->  325\n",
      "Iteration-> 0  , Iteration_time-> 1.1996 , Iteration_loss 1.6379\n",
      "Epoch 325 , average loss 1.6379098892211914 , epoch_time 1.1998467445373535\n",
      "\n",
      "epoch->  326\n",
      "Iteration-> 0  , Iteration_time-> 1.2029 , Iteration_loss 1.6429\n",
      "Epoch 326 , average loss 1.6428667306900024 , epoch_time 1.2030932903289795\n",
      "\n",
      "epoch->  327\n",
      "Iteration-> 0  , Iteration_time-> 1.2031 , Iteration_loss 1.6355\n",
      "Epoch 327 , average loss 1.6354994773864746 , epoch_time 1.20328950881958\n",
      "\n",
      "epoch->  328\n",
      "Iteration-> 0  , Iteration_time-> 1.2102 , Iteration_loss 1.6291\n",
      "Epoch 328 , average loss 1.6291208267211914 , epoch_time 1.2104685306549072\n",
      "\n",
      "epoch->  329\n",
      "Iteration-> 0  , Iteration_time-> 1.2036 , Iteration_loss 1.6290\n",
      "Epoch 329 , average loss 1.628955364227295 , epoch_time 1.2037591934204102\n",
      "\n",
      "epoch->  330\n",
      "Iteration-> 0  , Iteration_time-> 1.2043 , Iteration_loss 1.6252\n",
      "Epoch 330 , average loss 1.625213384628296 , epoch_time 1.2044670581817627\n",
      "\n",
      "epoch->  331\n",
      "Iteration-> 0  , Iteration_time-> 1.2200 , Iteration_loss 1.6172\n",
      "Epoch 331 , average loss 1.6172163486480713 , epoch_time 1.2202098369598389\n",
      "\n",
      "epoch->  332\n",
      "Iteration-> 0  , Iteration_time-> 1.2057 , Iteration_loss 1.6157\n",
      "Epoch 332 , average loss 1.6156518459320068 , epoch_time 1.2059361934661865\n",
      "\n",
      "epoch->  333\n",
      "Iteration-> 0  , Iteration_time-> 1.2186 , Iteration_loss 1.6137\n",
      "Epoch 333 , average loss 1.6136997938156128 , epoch_time 1.218926191329956\n",
      "\n",
      "epoch->  334\n",
      "Iteration-> 0  , Iteration_time-> 1.2223 , Iteration_loss 1.6116\n",
      "Epoch 334 , average loss 1.611610770225525 , epoch_time 1.2225017547607422\n",
      "\n",
      "epoch->  335\n",
      "Iteration-> 0  , Iteration_time-> 1.2117 , Iteration_loss 1.6154\n",
      "Epoch 335 , average loss 1.615362286567688 , epoch_time 1.2118830680847168\n",
      "\n",
      "epoch->  336\n",
      "Iteration-> 0  , Iteration_time-> 1.2230 , Iteration_loss 1.6056\n",
      "Epoch 336 , average loss 1.605606198310852 , epoch_time 1.2232484817504883\n",
      "\n",
      "epoch->  337\n",
      "Iteration-> 0  , Iteration_time-> 1.2149 , Iteration_loss 1.6064\n",
      "Epoch 337 , average loss 1.6064008474349976 , epoch_time 1.215196132659912\n",
      "\n",
      "epoch->  338\n",
      "Iteration-> 0  , Iteration_time-> 1.2167 , Iteration_loss 1.6019\n",
      "Epoch 338 , average loss 1.6018836498260498 , epoch_time 1.2168738842010498\n",
      "\n",
      "epoch->  339\n",
      "Iteration-> 0  , Iteration_time-> 1.2366 , Iteration_loss 1.5981\n",
      "Epoch 339 , average loss 1.5981214046478271 , epoch_time 1.2368214130401611\n",
      "\n",
      "epoch->  340\n",
      "Iteration-> 0  , Iteration_time-> 1.2073 , Iteration_loss 1.5977\n",
      "Epoch 340 , average loss 1.5977317094802856 , epoch_time 1.2075159549713135\n",
      "\n",
      "epoch->  341\n",
      "Iteration-> 0  , Iteration_time-> 1.2057 , Iteration_loss 1.5961\n",
      "Epoch 341 , average loss 1.5960676670074463 , epoch_time 1.205885887145996\n",
      "\n",
      "epoch->  342\n",
      "Iteration-> 0  , Iteration_time-> 1.2069 , Iteration_loss 1.5890\n",
      "Epoch 342 , average loss 1.5890392065048218 , epoch_time 1.2070579528808594\n",
      "\n",
      "epoch->  343\n",
      "Iteration-> 0  , Iteration_time-> 1.2142 , Iteration_loss 1.5933\n",
      "Epoch 343 , average loss 1.5933250188827515 , epoch_time 1.2144203186035156\n",
      "\n",
      "epoch->  344\n",
      "Iteration-> 0  , Iteration_time-> 1.2039 , Iteration_loss 1.5879\n",
      "Epoch 344 , average loss 1.5878580808639526 , epoch_time 1.2041232585906982\n",
      "\n",
      "epoch->  345\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 1.5901\n",
      "Epoch 345 , average loss 1.5901433229446411 , epoch_time 1.2107815742492676\n",
      "\n",
      "epoch->  346\n",
      "Iteration-> 0  , Iteration_time-> 1.2062 , Iteration_loss 1.5841\n",
      "Epoch 346 , average loss 1.5840797424316406 , epoch_time 1.2063970565795898\n",
      "\n",
      "epoch->  347\n",
      "Iteration-> 0  , Iteration_time-> 1.2238 , Iteration_loss 1.5786\n",
      "Epoch 347 , average loss 1.578605055809021 , epoch_time 1.224045753479004\n",
      "\n",
      "epoch->  348\n",
      "Iteration-> 0  , Iteration_time-> 1.2166 , Iteration_loss 1.5776\n",
      "Epoch 348 , average loss 1.5776472091674805 , epoch_time 1.216813087463379\n",
      "\n",
      "epoch->  349\n",
      "Iteration-> 0  , Iteration_time-> 1.2032 , Iteration_loss 1.5822\n",
      "Epoch 349 , average loss 1.5821540355682373 , epoch_time 1.2034249305725098\n",
      "\n",
      "epoch->  350\n",
      "Iteration-> 0  , Iteration_time-> 1.2165 , Iteration_loss 1.5603\n",
      "Epoch 350 , average loss 1.5603429079055786 , epoch_time 1.2167682647705078\n",
      "\n",
      "epoch->  351\n",
      "Iteration-> 0  , Iteration_time-> 1.1992 , Iteration_loss 1.5613\n",
      "Epoch 351 , average loss 1.5613102912902832 , epoch_time 1.199418067932129\n",
      "\n",
      "epoch->  352\n",
      "Iteration-> 0  , Iteration_time-> 1.2056 , Iteration_loss 1.5579\n",
      "Epoch 352 , average loss 1.5579286813735962 , epoch_time 1.2058210372924805\n",
      "\n",
      "epoch->  353\n",
      "Iteration-> 0  , Iteration_time-> 1.2031 , Iteration_loss 1.5625\n",
      "Epoch 353 , average loss 1.5624678134918213 , epoch_time 1.2033300399780273\n",
      "\n",
      "epoch->  354\n",
      "Iteration-> 0  , Iteration_time-> 1.2051 , Iteration_loss 1.5575\n",
      "Epoch 354 , average loss 1.557465672492981 , epoch_time 1.2053484916687012\n",
      "\n",
      "epoch->  355\n",
      "Iteration-> 0  , Iteration_time-> 1.1993 , Iteration_loss 1.5492\n",
      "Epoch 355 , average loss 1.549163818359375 , epoch_time 1.1994953155517578\n",
      "\n",
      "epoch->  356\n",
      "Iteration-> 0  , Iteration_time-> 1.2193 , Iteration_loss 1.5490\n",
      "Epoch 356 , average loss 1.5489740371704102 , epoch_time 1.2195489406585693\n",
      "\n",
      "epoch->  357\n",
      "Iteration-> 0  , Iteration_time-> 1.2042 , Iteration_loss 1.5470\n",
      "Epoch 357 , average loss 1.547032117843628 , epoch_time 1.2043876647949219\n",
      "\n",
      "epoch->  358\n",
      "Iteration-> 0  , Iteration_time-> 1.2094 , Iteration_loss 1.5470\n",
      "Epoch 358 , average loss 1.5470354557037354 , epoch_time 1.2095634937286377\n",
      "\n",
      "epoch->  359\n",
      "Iteration-> 0  , Iteration_time-> 1.2071 , Iteration_loss 1.5501\n",
      "Epoch 359 , average loss 1.5501482486724854 , epoch_time 1.2072784900665283\n",
      "\n",
      "epoch->  360\n",
      "Iteration-> 0  , Iteration_time-> 1.2053 , Iteration_loss 1.5355\n",
      "Epoch 360 , average loss 1.5355253219604492 , epoch_time 1.2054908275604248\n",
      "\n",
      "epoch->  361\n",
      "Iteration-> 0  , Iteration_time-> 1.2000 , Iteration_loss 1.5343\n",
      "Epoch 361 , average loss 1.5343085527420044 , epoch_time 1.2002136707305908\n",
      "\n",
      "epoch->  362\n",
      "Iteration-> 0  , Iteration_time-> 1.2178 , Iteration_loss 1.5305\n",
      "Epoch 362 , average loss 1.5305209159851074 , epoch_time 1.2179696559906006\n",
      "\n",
      "epoch->  363\n",
      "Iteration-> 0  , Iteration_time-> 1.2026 , Iteration_loss 1.5317\n",
      "Epoch 363 , average loss 1.5317426919937134 , epoch_time 1.2027919292449951\n",
      "\n",
      "epoch->  364\n",
      "Iteration-> 0  , Iteration_time-> 1.2131 , Iteration_loss 1.5280\n",
      "Epoch 364 , average loss 1.5280450582504272 , epoch_time 1.2132596969604492\n",
      "\n",
      "epoch->  365\n",
      "Iteration-> 0  , Iteration_time-> 1.2113 , Iteration_loss 1.5235\n",
      "Epoch 365 , average loss 1.5234898328781128 , epoch_time 1.2115592956542969\n",
      "\n",
      "epoch->  366\n",
      "Iteration-> 0  , Iteration_time-> 1.2202 , Iteration_loss 1.5212\n",
      "Epoch 366 , average loss 1.521173119544983 , epoch_time 1.2204203605651855\n",
      "\n",
      "epoch->  367\n",
      "Iteration-> 0  , Iteration_time-> 1.2139 , Iteration_loss 1.5139\n",
      "Epoch 367 , average loss 1.513898491859436 , epoch_time 1.214045763015747\n",
      "\n",
      "epoch->  368\n",
      "Iteration-> 0  , Iteration_time-> 1.2093 , Iteration_loss 1.5141\n",
      "Epoch 368 , average loss 1.5140879154205322 , epoch_time 1.2095026969909668\n",
      "\n",
      "epoch->  369\n",
      "Iteration-> 0  , Iteration_time-> 1.2035 , Iteration_loss 1.5036\n",
      "Epoch 369 , average loss 1.5036070346832275 , epoch_time 1.20363187789917\n",
      "\n",
      "epoch->  370\n",
      "Iteration-> 0  , Iteration_time-> 1.2162 , Iteration_loss 1.5060\n",
      "Epoch 370 , average loss 1.505972146987915 , epoch_time 1.2163655757904053\n",
      "\n",
      "epoch->  371\n",
      "Iteration-> 0  , Iteration_time-> 1.2108 , Iteration_loss 1.5072\n",
      "Epoch 371 , average loss 1.507236123085022 , epoch_time 1.2110531330108643\n",
      "\n",
      "epoch->  372\n",
      "Iteration-> 0  , Iteration_time-> 1.2111 , Iteration_loss 1.4995\n",
      "Epoch 372 , average loss 1.499458909034729 , epoch_time 1.2113244533538818\n",
      "\n",
      "epoch->  373\n",
      "Iteration-> 0  , Iteration_time-> 1.2086 , Iteration_loss 1.4928\n",
      "Epoch 373 , average loss 1.4928362369537354 , epoch_time 1.2087864875793457\n",
      "\n",
      "epoch->  374\n",
      "Iteration-> 0  , Iteration_time-> 1.2106 , Iteration_loss 1.4937\n",
      "Epoch 374 , average loss 1.49374520778656 , epoch_time 1.210822343826294\n",
      "\n",
      "epoch->  375\n",
      "Iteration-> 0  , Iteration_time-> 1.2042 , Iteration_loss 1.4931\n",
      "Epoch 375 , average loss 1.493117094039917 , epoch_time 1.2043850421905518\n",
      "\n",
      "epoch->  376\n",
      "Iteration-> 0  , Iteration_time-> 1.2069 , Iteration_loss 1.4845\n",
      "Epoch 376 , average loss 1.4845491647720337 , epoch_time 1.2071287631988525\n",
      "\n",
      "epoch->  377\n",
      "Iteration-> 0  , Iteration_time-> 1.2038 , Iteration_loss 1.4873\n",
      "Epoch 377 , average loss 1.4872822761535645 , epoch_time 1.203974962234497\n",
      "\n",
      "epoch->  378\n",
      "Iteration-> 0  , Iteration_time-> 1.2141 , Iteration_loss 1.4835\n",
      "Epoch 378 , average loss 1.4835373163223267 , epoch_time 1.2143440246582031\n",
      "\n",
      "epoch->  379\n",
      "Iteration-> 0  , Iteration_time-> 1.2011 , Iteration_loss 1.4770\n",
      "Epoch 379 , average loss 1.4769562482833862 , epoch_time 1.2013144493103027\n",
      "\n",
      "epoch->  380\n",
      "Iteration-> 0  , Iteration_time-> 1.2148 , Iteration_loss 1.4798\n",
      "Epoch 380 , average loss 1.4798409938812256 , epoch_time 1.2149944305419922\n",
      "\n",
      "epoch->  381\n",
      "Iteration-> 0  , Iteration_time-> 1.2047 , Iteration_loss 1.4702\n",
      "Epoch 381 , average loss 1.4701939821243286 , epoch_time 1.2048718929290771\n",
      "\n",
      "epoch->  382\n",
      "Iteration-> 0  , Iteration_time-> 1.2078 , Iteration_loss 1.4671\n",
      "Epoch 382 , average loss 1.4671119451522827 , epoch_time 1.20798921585083\n",
      "\n",
      "epoch->  383\n",
      "Iteration-> 0  , Iteration_time-> 1.2010 , Iteration_loss 1.4590\n",
      "Epoch 383 , average loss 1.458957552909851 , epoch_time 1.2012255191802979\n",
      "\n",
      "epoch->  384\n",
      "Iteration-> 0  , Iteration_time-> 1.2059 , Iteration_loss 1.4629\n",
      "Epoch 384 , average loss 1.4628510475158691 , epoch_time 1.2060985565185547\n",
      "\n",
      "epoch->  385\n",
      "Iteration-> 0  , Iteration_time-> 1.2130 , Iteration_loss 1.4524\n",
      "Epoch 385 , average loss 1.4524025917053223 , epoch_time 1.2131593227386475\n",
      "\n",
      "epoch->  386\n",
      "Iteration-> 0  , Iteration_time-> 1.1996 , Iteration_loss 1.4538\n",
      "Epoch 386 , average loss 1.453837513923645 , epoch_time 1.1998183727264404\n",
      "\n",
      "epoch->  387\n",
      "Iteration-> 0  , Iteration_time-> 1.2109 , Iteration_loss 1.4454\n",
      "Epoch 387 , average loss 1.4453933238983154 , epoch_time 1.2111167907714844\n",
      "\n",
      "epoch->  388\n",
      "Iteration-> 0  , Iteration_time-> 1.2149 , Iteration_loss 1.4453\n",
      "Epoch 388 , average loss 1.445340871810913 , epoch_time 1.2150940895080566\n",
      "\n",
      "epoch->  389\n",
      "Iteration-> 0  , Iteration_time-> 1.2075 , Iteration_loss 1.4439\n",
      "Epoch 389 , average loss 1.4438693523406982 , epoch_time 1.2076592445373535\n",
      "\n",
      "epoch->  390\n",
      "Iteration-> 0  , Iteration_time-> 1.2130 , Iteration_loss 1.4448\n",
      "Epoch 390 , average loss 1.4447673559188843 , epoch_time 1.2132141590118408\n",
      "\n",
      "epoch->  391\n",
      "Iteration-> 0  , Iteration_time-> 1.2011 , Iteration_loss 1.4360\n",
      "Epoch 391 , average loss 1.4359699487686157 , epoch_time 1.2013213634490967\n",
      "\n",
      "epoch->  392\n",
      "Iteration-> 0  , Iteration_time-> 1.2061 , Iteration_loss 1.4294\n",
      "Epoch 392 , average loss 1.4293609857559204 , epoch_time 1.2063260078430176\n",
      "\n",
      "epoch->  393\n",
      "Iteration-> 0  , Iteration_time-> 1.2101 , Iteration_loss 1.4318\n",
      "Epoch 393 , average loss 1.4317986965179443 , epoch_time 1.2102515697479248\n",
      "\n",
      "epoch->  394\n",
      "Iteration-> 0  , Iteration_time-> 1.2065 , Iteration_loss 1.4258\n",
      "Epoch 394 , average loss 1.4258432388305664 , epoch_time 1.2066740989685059\n",
      "\n",
      "epoch->  395\n",
      "Iteration-> 0  , Iteration_time-> 1.2011 , Iteration_loss 1.4227\n",
      "Epoch 395 , average loss 1.422664761543274 , epoch_time 1.2012584209442139\n",
      "\n",
      "epoch->  396\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs_gat):\n",
    "    print(\"\\nepoch-> \", epoch)\n",
    "    random.shuffle(Corpus_.train_triples)\n",
    "    Corpus_.train_indices = np.array(\n",
    "        list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "    model_gat.train()  # getting in training mode\n",
    "    start_time = time.time()\n",
    "    epoch_loss = []\n",
    "\n",
    "    if len(Corpus_.train_indices) % args.batch_size_gat == 0:\n",
    "        num_iters_per_epoch = len(\n",
    "            Corpus_.train_indices) // args.batch_size_gat\n",
    "    else:\n",
    "        num_iters_per_epoch = (\n",
    "            len(Corpus_.train_indices) // args.batch_size_gat) + 1\n",
    "\n",
    "    for iters in range(num_iters_per_epoch):\n",
    "        start_time_iter = time.time()\n",
    "        train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "        if CUDA:\n",
    "            train_indices = Variable(\n",
    "                torch.LongTensor(train_indices)).cuda()\n",
    "            train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "        else:\n",
    "            train_indices = Variable(torch.LongTensor(train_indices))\n",
    "            train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "        # forward pass\n",
    "        entity_embed, relation_embed = model_gat(\n",
    "            Corpus_, Corpus_.train_adj_matrix, train_indices, current_batch_2hop_indices)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = batch_gat_loss(\n",
    "            gat_loss_func, train_indices, entity_embed, relation_embed)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss.append(loss.data.item())\n",
    "\n",
    "        end_time_iter = time.time()\n",
    "\n",
    "        print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\n",
    "            iters, end_time_iter - start_time_iter, loss.data.item()))\n",
    "\n",
    "    scheduler.step()\n",
    "    print(\"Epoch {} , average loss {} , epoch_time {}\".format(\n",
    "        epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\n",
    "    epoch_losses.append(sum(epoch_loss) / len(epoch_loss))\n",
    "\n",
    "    if (epoch % 100 == 0):\n",
    "        save_model(model_gat, args.data, epoch,args.output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHN3vmlRbRRf"
   },
   "outputs": [],
   "source": [
    "torch.save(model_gat.state_dict(), \"gat.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RD9n-7Q-beBU",
    "outputId": "ffaaa4f1-e459-4cb3-b82e-135e873568b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RqMSuugCxTJU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "InVqT_jQF3Lq"
   },
   "outputs": [],
   "source": [
    "if CUDA:\n",
    "    model_conv.cuda()\n",
    "    model_gat.cuda()\n",
    "\n",
    "model_gat.load_state_dict(torch.load(\n",
    "    '{}/trained_{}.pth'.format(args.output_folder, args.epochs_gat - 1)), strict=False)\n",
    "model_conv.final_entity_embeddings = model_gat.final_entity_embeddings\n",
    "model_conv.final_relation_embeddings = model_gat.final_relation_embeddings\n",
    "\n",
    "Corpus_.batch_size = args.batch_size_conv\n",
    "Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=25, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "margin_loss = torch.nn.SoftMarginLoss()\n",
    "\n",
    "epoch_losses = []   # losses of all epochs\n",
    "print(\"Number of epochs {}\".format(args.epochs_conv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "colab_type": "code",
    "id": "1XnMK4kAzTqB",
    "outputId": "8f1ee640-af25-45e6-a74b-136c6c1c93f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch->  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-3acd0a34a9dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m             list(Corpus_.train_triples)).astype(np.int32)\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# getting in training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_conv' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs_conv):\n",
    "        print(\"\\nepoch-> \", epoch)\n",
    "        random.shuffle(Corpus_.train_triples)\n",
    "        Corpus_.train_indices = np.array(\n",
    "            list(Corpus_.train_triples)).astype(np.int32)\n",
    "\n",
    "        model_conv.train()  # getting in training mode\n",
    "        start_time = time.time()\n",
    "        epoch_loss = []\n",
    "\n",
    "        if len(Corpus_.train_indices) % args.batch_size_conv == 0:\n",
    "            num_iters_per_epoch = len(\n",
    "                Corpus_.train_indices) // args.batch_size_conv\n",
    "        else:\n",
    "            num_iters_per_epoch = (\n",
    "                len(Corpus_.train_indices) // args.batch_size_conv) + 1\n",
    "\n",
    "        for iters in range(num_iters_per_epoch):\n",
    "            start_time_iter = time.time()\n",
    "            train_indices, train_values = Corpus_.get_iteration_batch(iters)\n",
    "\n",
    "            if CUDA:\n",
    "                train_indices = Variable(\n",
    "                    torch.LongTensor(train_indices)).cuda()\n",
    "                train_values = Variable(torch.FloatTensor(train_values)).cuda()\n",
    "\n",
    "            else:\n",
    "                train_indices = Variable(torch.LongTensor(train_indices))\n",
    "                train_values = Variable(torch.FloatTensor(train_values))\n",
    "\n",
    "            preds = model_conv(\n",
    "                Corpus_, Corpus_.train_adj_matrix, train_indices)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = margin_loss(preds.view(-1), train_values.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss.append(loss.data.item())\n",
    "\n",
    "            end_time_iter = time.time()\n",
    "\n",
    "            print(\"Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\".format(\n",
    "                iters, end_time_iter - start_time_iter, loss.data.item()))\n",
    "\n",
    "        scheduler.step()\n",
    "        print(\"Epoch {} , average loss {} , epoch_time {}\".format(\n",
    "            epoch, sum(epoch_loss) / len(epoch_loss), time.time() - start_time))\n",
    "        epoch_losses.append(sum(epoch_loss) / len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_DrHllJzWze"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "yswke-BDzeV7",
    "outputId": "f0ced3c0-3115-4916-aabc-27ef25395609"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-cc5482ee7137>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                                  \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_GAT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malpha_conv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                                  args.nheads_GAT, args.out_channels)\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'gat.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel_conv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 584\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    586\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'gat.pth'"
     ]
    }
   ],
   "source": [
    "unique_entities = Corpus_.unique_entities_train\n",
    "model_conv = SpKBGATConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,\n",
    "                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,\n",
    "                                 args.nheads_GAT, args.out_channels)\n",
    "model_conv.load_state_dict(torch.load('gat.pth'), strict=False)\n",
    "\n",
    "model_conv.cuda()\n",
    "model_conv.eval()\n",
    "with torch.no_grad():\n",
    "    Corpus_.get_validation_pred(args, model_conv, unique_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AzfNiXGcF-5h"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3LsqGX8eRfje"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T1JaayJcRsTO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhmLex6-SVTE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svEWQ6mgRCl5"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2_TVFncS2w5j"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCVi58UB2xzx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "oUf0F7XsVXxi",
    "tlUuYNjCLSy4",
    "zBdtg0VELehu",
    "sjIwefuvL1kC",
    "BOprxDhkMGbQ",
    "tXAQD6XVMTLJ",
    "f0ZfxfQfMp21",
    "xy-rjzy9fluF"
   ],
   "machine_shape": "hm",
   "name": "KBAT_Embedded_Chính thức.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
