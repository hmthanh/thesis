{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cài đặt mã nguồn GAT\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-77803f9f9900>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \"\"\"\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Mở file entity2id ra và kết quả trả ra là một dictionary chứa key là entity, value là entity_id\n",
    "\"\"\" \n",
    "def read_entity_from_id(filename='./data/WN18RR/entity2id.txt'):\n",
    "    entity2id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                entity, entity_id = line.strip().split()[0].strip(), line.strip().split()[1].strip()\n",
    "                entity2id[entity] = int(entity_id)\n",
    "    return entity2id\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Tương tự như trên trả ra dictionary key, value\n",
    "\"\"\"\n",
    "def read_relation_from_id(filename='./data/WN18RR/relation2id.txt'):\n",
    "    relation2id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if len(line.strip().split()) > 1:\n",
    "                relation, relation_id = line.strip().split(\n",
    "                )[0].strip(), line.strip().split()[1].strip()\n",
    "                relation2id[relation] = int(relation_id)\n",
    "    return relation2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nKết quả trả ra là :\\nnp.array(entity_emb, dtype=np.float32) : \\nLà một mảng numpy kiểu float32 , giá trị là kết quả nhúng của thực thể được dòng từng dòng từ file\\n\\nnp.array(relation_emb, dtype=np.float32)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Đọc file entity và relation, sau đó đọc từng dòng và trả ra mảng numpy kiểu float 32 các entity hoặc relation nhúng\n",
    "\"\"\"\n",
    "def init_embeddings(entity_file, relation_file):\n",
    "    entity_emb, relation_emb = [], []\n",
    "\n",
    "    with open(entity_file) as f:\n",
    "        for line in f:\n",
    "            entity_emb.append([float(val) for val in line.strip().split()])\n",
    "\n",
    "    with open(relation_file) as f:\n",
    "        # Đọc từng dòng\n",
    "        for line in f:\n",
    "            relation_emb.append([float(val) for val in line.strip().split()])\n",
    "\n",
    "    return np.array(entity_emb, dtype=np.float32), np.array(relation_emb, dtype=np.float32)\n",
    "\"\"\"\n",
    "Kết quả trả ra là :\n",
    "np.array(entity_emb, dtype=np.float32) : \n",
    "Là một mảng numpy kiểu float32 , giá trị là kết quả nhúng của thực thể được dòng từng dòng từ file\n",
    "\n",
    "np.array(relation_emb, dtype=np.float32)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Đọc từng dòng bộ ba :\n",
    "<entity1> <realtion1> <entity1>\n",
    "<entity2> <realtion2> <entity2>\n",
    "...\n",
    "\"\"\"\n",
    "def parse_line(line):\n",
    "    line = line.strip().split()\n",
    "    e1, relation, e2 = line[0].strip(), line[1].strip(), line[2].strip()\n",
    "    return e1, relation, e2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntriples_data : là bộ ba số hóa của head entity relation và tail entity\\nunique_entities : là tập hợp set entity\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "rows, cols, data : là bộ ba gồm entity_head_id, entity_tail_id, relation\n",
    "[rows, cols] = data\n",
    "Vi dụ : head1_id relation1_id tail1_id\n",
    "rows = [head1_id,...]\n",
    "cols = [relation1_id,...]\n",
    "data = [tail1_id,...]\n",
    "\"\"\"\n",
    "def load_data(filename, entity2id, relation2id, is_unweigted=False, directed=True):\n",
    "    # Mở ra và đọc đến cuối\n",
    "    with open(filename) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # this is list for relation triples\n",
    "    triples_data = []\n",
    "\n",
    "    # for sparse tensor, rows list contains corresponding row of sparse tensor, cols list contains corresponding\n",
    "    # columnn of sparse tensor, data contains the type of relation\n",
    "    # Adjacecny matrix of entities is undirected, as the source and tail entities should know, the relation\n",
    "    # type they are connected with\n",
    "    rows, cols, data = [], [], []\n",
    "    unique_entities = set()\n",
    "    for line in lines:\n",
    "        # Tách ra thành mối quan hệ\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "        unique_entities.add(e1)\n",
    "        unique_entities.add(e2)\n",
    "        \n",
    "        # Nhúng kết quả số hóa\n",
    "        triples_data.append(\n",
    "            (entity2id[e1], relation2id[relation], entity2id[e2]))\n",
    "        if not directed:\n",
    "                # Connecting source and tail entity\n",
    "            rows.append(entity2id[e1])\n",
    "            cols.append(entity2id[e2])\n",
    "            if is_unweigted:\n",
    "                data.append(1)\n",
    "            else:\n",
    "                data.append(relation2id[relation])\n",
    "\n",
    "        # Connecting tail and source entity\n",
    "        rows.append(entity2id[e2])\n",
    "        cols.append(entity2id[e1])\n",
    "        if is_unweigted:\n",
    "            data.append(1)\n",
    "        else:\n",
    "            data.append(relation2id[relation])\n",
    "\n",
    "    print(\"number of unique_entities ->\", len(unique_entities))\n",
    "    return triples_data, (rows, cols, data), list(unique_entities)\n",
    "\n",
    "\"\"\"\n",
    "triples_data : là bộ ba số hóa của head entity relation và tail entity\n",
    "unique_entities : là tập hợp set entity\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_data(path='./data/WN18RR/', is_unweigted=False, directed=True):\n",
    "    entity2id = read_entity_from_id(path + 'entity2id.txt')\n",
    "    relation2id = read_relation_from_id(path + 'relation2id.txt')\n",
    "\n",
    "    # Adjacency matrix only required for training phase\n",
    "    # Currenlty creating as unweighted, undirected\n",
    "    train_triples, train_adjacency_mat, unique_entities_train = load_data(os.path.join(\n",
    "        path, 'train.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    \n",
    "    validation_triples, valid_adjacency_mat, unique_entities_validation = load_data(\n",
    "        os.path.join(path, 'valid.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    \n",
    "    test_triples, test_adjacency_mat, unique_entities_test = load_data(os.path.join(\n",
    "        path, 'test.txt'), entity2id, relation2id, is_unweigted, directed)\n",
    "    \n",
    "    \"\"\"\n",
    "    Đên đây đã load ra được bộ ba gồm có train_triples, train_adjacency_mat, và unique_entities_train của cả 3\n",
    "    \"\"\"\n",
    "\n",
    "    id2entity = {v: k for k, v in entity2id.items()}\n",
    "    id2relation = {v: k for k, v in relation2id.items()}\n",
    "    left_entity, right_entity = {}, {}\n",
    "    # Trả ra diction với mỗi cái là id to entity hoặc relation\n",
    "\n",
    "    with open(os.path.join(path, 'train.txt')) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Đọc với mỗi dòng\n",
    "    for line in lines:\n",
    "        e1, relation, e2 = parse_line(line)\n",
    "        \n",
    "        # Đếm số lượng quan hệ giữa (e1, relation) khi duyệt qua từng bộ ba\n",
    "        # Count number of occurences for each (e1, relation)\n",
    "        if relation2id[relation] not in left_entity:\n",
    "            left_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e1] not in left_entity[relation2id[relation]]:\n",
    "            left_entity[relation2id[relation]][entity2id[e1]] = 0\n",
    "        left_entity[relation2id[relation]][entity2id[e1]] += 1\n",
    "\n",
    "        # Count number of occurences for each (relation, e2)\n",
    "        if relation2id[relation] not in right_entity:\n",
    "            right_entity[relation2id[relation]] = {}\n",
    "        if entity2id[e2] not in right_entity[relation2id[relation]]:\n",
    "            right_entity[relation2id[relation]][entity2id[e2]] = 0\n",
    "        right_entity[relation2id[relation]][entity2id[e2]] += 1\n",
    "        \n",
    "    \"\"\"\n",
    "    Đến đây ta đã tính đươc tổng số hàng xóm liên kết dến head hoặc tail trong một node của head entity và tail entity\n",
    "    num_neighbour(head) = left_entity\n",
    "    num_neighbour(tail) = right_entity\n",
    "    \n",
    "    Trong đó, kết quả dữ liệu sẽ có dạng\n",
    "    left_entity[relation_id][head_id] = tổng số lượng hàng xóm của head\n",
    "    right_entity[relation_id][tail_id] = tổng số lượng hàng xóm của tail\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    left_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        left_entity_avg[i] = sum(\n",
    "            left_entity[i].values()) * 1.0 / len(left_entity[i])\n",
    "    \"\"\"Với mỗi quan hệ realtion_i (trong tất cả quan hệ có relation_i) : tính trung bình số hàng xóm của phần head\"\"\"\n",
    "        \n",
    "\n",
    "    right_entity_avg = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        right_entity_avg[i] = sum(\n",
    "            right_entity[i].values()) * 1.0 / len(right_entity[i])\n",
    "    \"\"\"Với mỗi quan hệ realtion : tính trung bình số hàng xóm của tail\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Duyệt tất cả trong tập realtions\n",
    "    headTailSelector = relation_i_th = 1000 * trung_bình_số_neibour_tail_i / (số node head + tail)\n",
    "    \n",
    "    => Đây là một hàm tính điểm số\n",
    "    \"\"\"\n",
    "    headTailSelector = {}\n",
    "    for i in range(len(relation2id)):\n",
    "        headTailSelector[i] = 1000 * right_entity_avg[i] / \\\n",
    "            (right_entity_avg[i] + left_entity_avg[i])\n",
    "\n",
    "    return (train_triples, train_adjacency_mat), (validation_triples, valid_adjacency_mat), (test_triples, test_adjacency_mat), \\\n",
    "        entity2id, relation2id, headTailSelector, unique_entities_train\n",
    "\n",
    "\"\"\"\n",
    "Kết quả trả ra là :\n",
    "(train_triples, train_adjacency_mat) : tập train\n",
    "(validation_triples, valid_adjacency_mat) : tập valid\n",
    "(test_triples, test_adjacency_mat) : tập test\n",
    "entity2id : entity_id\n",
    "relation2id : relation_id\n",
    "headTailSelector : độ đo số node của tail trên tổng số hàng xóm của 1 relation\n",
    "unique_entities_train : unique\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hàm thực thi quá trình trên\n",
    "train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(args):\n",
    "    \"\"\"\n",
    "    Tương tự quá trình trên\"\"\"\n",
    "    train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(\n",
    "        args.data, is_unweigted=False, directed=True)\n",
    "\n",
    "    if args.pretrained_emb:\n",
    "        entity_embeddings, relation_embeddings = init_embeddings(os.path.join(args.data, 'entity2vec.txt'),\n",
    "                                                                 os.path.join(args.data, 'relation2vec.txt'))\n",
    "        print(\"Initialised relations and entities from TransE\")\n",
    "\n",
    "    else:\n",
    "        \"\"\"\n",
    "        Khởi tạo entity_embeddings với kích thước len(entity2id) và kích thước nhúng\n",
    "        Tức số vector biểu diễn 1 embed trong không gian\n",
    "        \"\"\"\n",
    "        entity_embeddings = np.random.randn(\n",
    "            len(entity2id), args.embedding_size)\n",
    "        relation_embeddings = np.random.randn(\n",
    "            len(relation2id), args.embedding_size)\n",
    "        print(\"Initialised relations and entities randomly\")\n",
    "\n",
    "    corpus = Corpus(args, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector,\n",
    "                    args.batch_size_gat, args.valid_invalid_ratio_gat, unique_entities_train, args.get_2hop)\n",
    "\n",
    "    return corpus, torch.FloatTensor(entity_embeddings), torch.FloatTensor(relation_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph(self):\n",
    "        graph = {}\n",
    "        all_tiples = torch.cat([self.train_adj_matrix[0].transpose(0, 1), self.train_adj_matrix[1].unsqueeze(1)], dim=1)\n",
    "        \"\"\"\n",
    "        train_adj_matrix = [[tail_id, head_id], [relation]]\n",
    "        Chuyển từ dạng [[tail_id, head_id], [relation]] sang dạng (n dòng, 1 cột) [[tail_id], [head_id], [relation_id]]\n",
    "        \"\"\"\n",
    "\n",
    "        for data in all_tiples:\n",
    "            source = data[1].data.item() # head\n",
    "            target = data[0].data.item() # tail\n",
    "            value = data[2].data.item() # relation\n",
    "\n",
    "            if(source not in graph.keys()):\n",
    "                graph[source] = {}\n",
    "                graph[source][target] = value\n",
    "            else:\n",
    "                graph[source][target] = value\n",
    "        print(\"Graph created\")\n",
    "        return graph\n",
    "    \n",
    "\"\"\"\n",
    "Tạo ra kết quả bao gồm một dictionary gồm\n",
    "graph[head_id][tail_id] = relation_id\n",
    "\n",
    "=> Tức là lấy ra tất cả các cạnh trong đồ thị\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mục tiêu : Dùng thuật toán BFS để tìm những node xung quanh với 1 node source với độ sâu là 2\n",
    "Input : \n",
    "graph : chứa tất cả các cạnh có hướng trong đồ thị với cấu trúc : [[head_id], [tail_id], [relation_id]]\n",
    "source : node nguồn\n",
    "nbd_size : độ sâu của node\n",
    "\"\"\"\n",
    "def bfs(self, graph, source, nbd_size=2):\n",
    "        visit = {} # Tập hợp đã ghé thăm\n",
    "        distance = {} # Khoảng cách\n",
    "        parent = {} # Tập hợp node cha\n",
    "        distance_lengths = {} # Chiều dài khoảng cách\n",
    "\n",
    "        visit[source] = 1 # Ghé thăm node gốc\n",
    "        distance[source] = 0 # Khoảng cách đến node đang xét tới tất cả các node\n",
    "        parent[source] = (-1, -1) # Cha của node gốc là không có ai cả\n",
    "\n",
    "        q = queue.Queue() # Queue\n",
    "        q.put((source, -1)) # Đưa node gốc vào\n",
    "\n",
    "        while(not q.empty()): # Khi Queue vẫn còn rỗng\n",
    "            top = q.get() # Lấy phần tử đầu tiên ra\n",
    "            if top[0] in graph.keys(): # nếu node head đang duyệt nằm trong đồ thị\n",
    "                for target in graph[top[0]].keys(): # node tail nằm trong đồ thị\n",
    "                    if(target in visit.keys()): # nếu tail đã nằm trong danh sách duyệt rồi thì đi tiếp\n",
    "                        continue\n",
    "                    else:\n",
    "                        q.put((target, graph[top[0]][target]))\n",
    "\n",
    "                        distance[target] = distance[top[0]] + 1\n",
    "\n",
    "                        visit[target] = 1\n",
    "                        if distance[target] > 2:\n",
    "                            continue\n",
    "                        parent[target] = (top[0], graph[top[0]][target])\n",
    "\n",
    "                        if distance[target] not in distance_lengths.keys():\n",
    "                            distance_lengths[distance[target]] = 1\n",
    "\n",
    "        neighbors = {}\n",
    "        for target in visit.keys():\n",
    "            if(distance[target] != nbd_size):\n",
    "                continue\n",
    "            edges = [-1, parent[target][1]]\n",
    "            relations = []\n",
    "            entities = [target]\n",
    "            temp = target\n",
    "            while(parent[temp] != (-1, -1)):\n",
    "                relations.append(parent[temp][1])\n",
    "                entities.append(parent[temp][0])\n",
    "                temp = parent[temp][0]\n",
    "\n",
    "            if(distance[target] in neighbors.keys()):\n",
    "                neighbors[distance[target]].append(\n",
    "                    (tuple(relations), tuple(entities[:-1])))\n",
    "            else:\n",
    "                neighbors[distance[target]] = [\n",
    "                    (tuple(relations), tuple(entities[:-1]))]\n",
    "\n",
    "        return neighbors\n",
    "    \n",
    "\"\"\"\n",
    "Kết quả trả ra là một dictionary chứa\n",
    "neighbors[distance] = (relations_id, entities_id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-06ac2e8a9b1b>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-06ac2e8a9b1b>\"\u001b[1;36m, line \u001b[1;32m13\u001b[0m\n\u001b[1;33m    \"\" Trả ra những node hàng xóm với độ sâu là 2\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "nbd_size : kích thước độ sâu khi duyệt hàng xóm kế cận\n",
    "\"\"\"\n",
    "def get_further_neighbors(self, nbd_size=2):\n",
    "        neighbors = {}\n",
    "        start_time = time.time()\n",
    "        print(\"length of graph keys is \", len(self.graph.keys()))\n",
    "        \n",
    "        # Với mỗi head trong từng cặp head, relation, tail trong tập train\n",
    "        for source in self.graph.keys():\n",
    "            # st_time = time.time()\n",
    "            \n",
    "            \"\"\"\n",
    "            Trả ra những node hàng xóm với độ sâu là 2\n",
    "            temp_neighbors có định dạng :\n",
    "            temp_neighbors[distance] = (relations_id, entities_id)\n",
    "            \n",
    "            neighbors[source_id][distance] = (relations_id, entity_id)\n",
    "            \"\"\" \n",
    "            temp_neighbors = self.bfs(self.graph, source, nbd_size)\n",
    "            for distance in temp_neighbors.keys():\n",
    "                if(source in neighbors.keys()):\n",
    "                    if(distance in neighbors[source].keys()):\n",
    "                        neighbors[source][distance].append(\n",
    "                            temp_neighbors[distance])\n",
    "                    else:\n",
    "                        neighbors[source][distance] = temp_neighbors[distance]\n",
    "                else:\n",
    "                    neighbors[source] = {}\n",
    "                    neighbors[source][distance] = temp_neighbors[distance]\n",
    "\n",
    "        print(\"time taken \", time.time() - start_time)\n",
    "\n",
    "        print(\"length of neighbors dict is \", len(neighbors))\n",
    "        return neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus:\n",
    "    def __init__(self, args, train_data, validation_data, test_data, entity2id,\n",
    "                 relation2id, headTailSelector, batch_size, valid_to_invalid_samples_ratio, unique_entities_train, get_2hop=False):\n",
    "        self.train_triples = train_data[0]\n",
    "        \n",
    "\n",
    "        # Converting to sparse tensor\n",
    "        \"\"\"Lấy từ train_adjacency_mat\n",
    "        adj_indices sẽ có dạng là tuple (rows, cols, data)\n",
    "        row = [head1,..]\n",
    "        cols = [tail1,..]\n",
    "        data = [relation1,..]\n",
    "        \"\"\"\n",
    "        adj_indices = torch.LongTensor(\n",
    "            [train_data[1][0], train_data[1][1]])  # rows and columns\n",
    "        # data = train_data[1][2] với train_data[1] = (rows, cols, data)\n",
    "        adj_values = torch.LongTensor(train_data[1][2])\n",
    "        self.train_adj_matrix = (adj_indices, adj_values)\n",
    "        # train_adj_matrix : ([head_id, tail_id], [relation]\n",
    "\n",
    "        # adjacency matrix is needed for train_data only, as GAT is trained for\n",
    "        # training data\n",
    "        self.validation_triples = validation_data[0]\n",
    "        self.test_triples = test_data[0]\n",
    "\n",
    "        self.headTailSelector = headTailSelector  # for selecting random entities\n",
    "        # Với mỗi quan hệ, tính trung bình tổng số hàng xóm tail trên tổng số hàng xóm, có quan hệ\n",
    "        self.entity2id = entity2id\n",
    "        self.id2entity = {v: k for k, v in self.entity2id.items()}\n",
    "        self.relation2id = relation2id\n",
    "        self.id2relation = {v: k for k, v in self.relation2id.items()}\n",
    "        self.batch_size = batch_size\n",
    "        # ratio of valid to invalid samples per batch for training ConvKB Model\n",
    "        self.invalid_valid_ratio = int(valid_to_invalid_samples_ratio)\n",
    "\n",
    "        if(get_2hop):\n",
    "            self.graph = self.get_graph()\n",
    "            \"\"\"\n",
    "            Tạo ra kết quả bao gồm một dictionary gồm\n",
    "            self.graph[head_id][tail_id] = relation_id\n",
    "            \"\"\"\n",
    "            self.node_neighbors_2hop = self.get_further_neighbors()\n",
    "\n",
    "        self.unique_entities_train = [self.entity2id[i]\n",
    "                                      for i in unique_entities_train]\n",
    "        # unique_entities_train : danh sách entity_id trong tập train\n",
    "\n",
    "        # train_indices có shape là [[head_id, tail_id, relation_id]]\n",
    "        self.train_indices = np.array(\n",
    "            list(self.train_triples)).astype(np.int32)\n",
    "        \n",
    "        # These are valid triples, hence all have value 1\n",
    "        # Kích thước là : 1 x số cạnh trong đồ thị, với giá trị với cái là 1\n",
    "        self.train_values = np.array(\n",
    "            [[1]] * len(self.train_triples)).astype(np.float32)\n",
    "\n",
    "        self.validation_indices = np.array(\n",
    "            list(self.validation_triples)).astype(np.int32)\n",
    "        self.validation_values = np.array(\n",
    "            [[1]] * len(self.validation_triples)).astype(np.float32)\n",
    "\n",
    "        self.test_indices = np.array(list(self.test_triples)).astype(np.int32)\n",
    "        self.test_values = np.array(\n",
    "            [[1]] * len(self.test_triples)).astype(np.float32)\n",
    "        \n",
    "        \"\"\"\n",
    "        Tương tự với tập valid và tập test\n",
    "        \"\"\"\n",
    "\n",
    "        self.valid_triples_dict = {j: i for i, j in enumerate(\n",
    "            self.train_triples + self.validation_triples + self.test_triples)}\n",
    "        print(\"Total triples count {}, training triples {}, validation_triples {}, test_triples {}\".format(len(self.valid_triples_dict), len(self.train_indices),\n",
    "                                                                                                           len(self.validation_indices), len(self.test_indices)))\n",
    "\n",
    "        # For training purpose\n",
    "        \"\"\"\n",
    "        batch_indices : có kích thước là (batch_invalid_size, 3) : kiểu int32\n",
    "        Ý nghĩa : là tập để chọn ra danh sách những tập valid, phân biệt với tập invalid trong đồ thị\n",
    "        \n",
    "        batch_values có kích thước chỉ là : (batch_invalid_size, 3)\n",
    "        Dùng để biểu diễn giá trị\n",
    "        \"\"\"\n",
    "        self.batch_indices = np.empty(\n",
    "            (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "        self.batch_values = np.empty(\n",
    "            (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_iteration_batch(self, iter_num):\n",
    "        if (iter_num + 1) * self.batch_size <= len(self.train_indices):\n",
    "            self.batch_indices = np.empty(\n",
    "                (self.batch_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "            self.batch_values = np.empty(\n",
    "                (self.batch_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "            indices = range(self.batch_size * iter_num,\n",
    "                            self.batch_size * (iter_num + 1))\n",
    "\n",
    "            self.batch_indices[:self.batch_size,\n",
    "                               :] = self.train_indices[indices, :]\n",
    "            self.batch_values[:self.batch_size,\n",
    "                              :] = self.train_values[indices, :]\n",
    "\n",
    "            last_index = self.batch_size\n",
    "\n",
    "            if self.invalid_valid_ratio > 0:\n",
    "                random_entities = np.random.randint(\n",
    "                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "                # Precopying the same valid indices from 0 to batch_size to rest\n",
    "                # of the indices\n",
    "                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "                for i in range(last_index):\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\n",
    "                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           0] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = last_index * \\\n",
    "                            (self.invalid_valid_ratio // 2) + \\\n",
    "                            (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\n",
    "                               random_entities[current_index]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           2] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                return self.batch_indices, self.batch_values\n",
    "\n",
    "            return self.batch_indices, self.batch_values\n",
    "\n",
    "        else:\n",
    "            last_iter_size = len(self.train_indices) - \\\n",
    "                self.batch_size * iter_num\n",
    "            self.batch_indices = np.empty(\n",
    "                (last_iter_size * (self.invalid_valid_ratio + 1), 3)).astype(np.int32)\n",
    "            self.batch_values = np.empty(\n",
    "                (last_iter_size * (self.invalid_valid_ratio + 1), 1)).astype(np.float32)\n",
    "\n",
    "            indices = range(self.batch_size * iter_num,\n",
    "                            len(self.train_indices))\n",
    "            self.batch_indices[:last_iter_size,\n",
    "                               :] = self.train_indices[indices, :]\n",
    "            self.batch_values[:last_iter_size,\n",
    "                              :] = self.train_values[indices, :]\n",
    "\n",
    "            last_index = last_iter_size\n",
    "\n",
    "            if self.invalid_valid_ratio > 0:\n",
    "                random_entities = np.random.randint(\n",
    "                    0, len(self.entity2id), last_index * self.invalid_valid_ratio)\n",
    "\n",
    "                # Precopying the same valid indices from 0 to batch_size to rest\n",
    "                # of the indices\n",
    "                self.batch_indices[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_indices[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "                self.batch_values[last_index:(last_index * (self.invalid_valid_ratio + 1)), :] = np.tile(\n",
    "                    self.batch_values[:last_index, :], (self.invalid_valid_ratio, 1))\n",
    "\n",
    "                for i in range(last_index):\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = i * (self.invalid_valid_ratio // 2) + j\n",
    "\n",
    "                        while (random_entities[current_index], self.batch_indices[last_index + current_index, 1],\n",
    "                               self.batch_indices[last_index + current_index, 2]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           0] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                    for j in range(self.invalid_valid_ratio // 2):\n",
    "                        current_index = last_index * \\\n",
    "                            (self.invalid_valid_ratio // 2) + \\\n",
    "                            (i * (self.invalid_valid_ratio // 2) + j)\n",
    "\n",
    "                        while (self.batch_indices[last_index + current_index, 0], self.batch_indices[last_index + current_index, 1],\n",
    "                               random_entities[current_index]) in self.valid_triples_dict.keys():\n",
    "                            random_entities[current_index] = np.random.randint(\n",
    "                                0, len(self.entity2id))\n",
    "                        self.batch_indices[last_index + current_index,\n",
    "                                           2] = random_entities[current_index]\n",
    "                        self.batch_values[last_index + current_index, :] = [-1]\n",
    "\n",
    "                return self.batch_indices, self.batch_values\n",
    "\n",
    "            return self.batch_indices, self.batch_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
